{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Decision Trees\n",
        "\n",
        "Decision trees are a type of supervised learning algorithm used for both classification and regression tasks. Their aim is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features.\n",
        "\n",
        "### Characteristics of Decision Trees:\n",
        "\n",
        "- **Structure**: A decision tree consists of nodes, where each node represents a feature (or attribute), each link (or branch) represents a decision/rule, and each leaf represents an outcome.\n",
        "- **Ease of Interpretation**: They are easy to understand and interpret, as they can be visually represented.\n",
        "- **Versatility**: Can handle both numerical and categorical data.\n",
        "- **Non-parametric**: They do not assume any distribution of the data, making them suitable for analysis where the data does not meet certain assumptions.\n",
        "\n",
        "### Uses of Decision Trees:\n",
        "\n",
        "- **Classification and Regression**: Used for solving both classification and regression problems.\n",
        "- **Exploratory Data Analysis**: Help in identifying significant patterns and relevant variables.\n",
        "- **Predictive Modeling**: Useful in predictive modeling, especially when model interpretation is important.\n",
        "\n",
        "### Differences from Other Algorithms:\n",
        "\n",
        "- **Simplicity**: Unlike more complex models like neural networks, decision trees are simpler and easier to interpret.\n",
        "- **Non-linearity**: They can capture non-linear relationships between features and the target variable, unlike linear models like linear regression.\n",
        "- **Data Sensitivity**: More sensitive to variations in the data and can easily overfit, unlike more robust models like random forests."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "42323049-0981-40b4-8ed5-3a5aec98c2aa"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example: Decision Tree Classification in Python\n",
        "\n",
        "In this example, we will use a dataset from scikit-learn, convert it to a DataFrame, describe it, and then apply a decision tree model for classification. After obtaining the results, we will analyze the precision, recall, accuracy, and ROC curve."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "4954f970-459e-49e0-a904-4e185cebdf72"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import classification_report, roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load and prepare the dataset\n",
        "iris = load_iris()\n",
        "iris_df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
        "iris_df['target'] = iris.target\n",
        "\n",
        "# Display basic information about the dataset\n",
        "iris_df.info()\n",
        "iris_df.describe()\n",
        "\n",
        "# Split the dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris_df[iris.feature_names], iris_df['target'], test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize and train the decision tree classifier\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions and evaluate the model\n",
        "y_pred = clf.predict(X_test)\n",
        "report = classification_report(y_test, y_pred)\n",
        "print('Classification Report:\\n', report)\n",
        "\n",
        "# Calculate ROC curve and AUC\n",
        "fpr, tpr, thresholds = roc_curve(y_test, clf.predict_proba(X_test)[:,1], pos_label=1)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# Plotting the ROC curve\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        },
        "ExecuteTime": {
          "start_time": "2023-11-28T14:42:33.770796+00:00",
          "end_time": "2023-11-28T14:42:34.745845+00:00"
        }
      },
      "id": "c75f9f3b-8f62-4f69-bee5-69652f2432ff"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example: Decision Tree Classification with a Different Dataset\n",
        "\n",
        "In this example, we will use another dataset from scikit-learn, convert it to a DataFrame, describe it, and then apply a decision tree model for classification. We will follow the same steps as the previous example, including analyzing precision, recall, accuracy, and the ROC curve."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "e0632e9c-6022-4d3b-b5d7-d504c5054409"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import classification_report, roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load and prepare the dataset\n",
        "wine = load_wine()\n",
        "wine_df = pd.DataFrame(wine.data, columns=wine.feature_names)\n",
        "wine_df['target'] = wine.target\n",
        "\n",
        "# Display basic information about the dataset\n",
        "wine_df.info()\n",
        "wine_df.describe()\n",
        "\n",
        "# Split the dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(wine_df[wine.feature_names], wine_df['target'], test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize and train the decision tree classifier\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions and evaluate the model\n",
        "y_pred = clf.predict(X_test)\n",
        "report = classification_report(y_test, y_pred)\n",
        "print('Classification Report:\\n', report)\n",
        "\n",
        "# Calculate ROC curve and AUC for multi-class\n",
        "fpr = dict()\n",
        "tpr = dict()\n",
        "roc_auc = dict()\n",
        "for i in range(wine.target_names.size):\n",
        "    fpr[i], tpr[i], _ = roc_curve(y_test, clf.predict_proba(X_test)[:, i], pos_label=i)\n",
        "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "# Plotting the ROC curve for each class\n",
        "plt.figure()\n",
        "for i in range(wine.target_names.size):\n",
        "    plt.plot(fpr[i], tpr[i], lw=2, label='ROC curve of class %d (area = %0.2f)' % (i, roc_auc[i]))\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic for each class')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        },
        "ExecuteTime": {
          "start_time": "2023-11-28T14:46:46.626891+00:00",
          "end_time": "2023-11-28T14:46:47.024424+00:00"
        }
      },
      "id": "d9ff85b3-f1eb-4913-bff2-f099e5d9901a"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forests\n",
        "\n",
        "Random Forests are an ensemble learning method, primarily used for classification and regression. They operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees.\n",
        "\n",
        "### Characteristics of Random Forests:\n",
        "\n",
        "- **Ensemble Method**: Random Forests combine multiple decision trees to improve the predictive performance and control over-fitting.\n",
        "- **Robustness**: They are less prone to overfitting than individual decision trees.\n",
        "- **Handling of Unbalanced Data**: Effective in handling unbalanced datasets by balancing error in the minority class.\n",
        "- **Feature Importance**: They provide insights into the importance of each feature in making predictions.\n",
        "\n",
        "### Differences from Decision Trees:\n",
        "\n",
        "- **Complexity**: Random Forests are generally more complex than single decision trees.\n",
        "- **Performance**: They often provide better accuracy due to the averaging of multiple trees.\n",
        "- **Interpretability**: While individual trees are easy to interpret, the ensemble nature of Random Forests makes them more complex to interpret.\n",
        "\n",
        "### Metrics Used in Random Forests:\n",
        "\n",
        "- **Accuracy**: Measures the proportion of correct predictions.\n",
        "- **Precision and Recall**: Useful for evaluating class imbalance.\n",
        "- **F1 Score**: Harmonic mean of precision and recall.\n",
        "- **AUC-ROC Curve**: Measures the performance across all possible classification thresholds."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "ff120410-1435-4099-8975-581abbe5abd2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest Example with Iris Dataset\n",
        "\n",
        "Following the same steps as in the decision tree examples, we will now apply a Random Forest model to the Iris dataset. We will load the dataset, convert it to a DataFrame, describe it, apply the Random Forest model for classification, and analyze the precision, recall, accuracy, and ROC curve."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "6d731d35-4ff1-4e54-b46e-4732f50c3134"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Initialize and train the Random Forest classifier\n",
        "rf_clf = RandomForestClassifier(random_state=42)\n",
        "rf_clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions and evaluate the model\n",
        "y_pred_rf = rf_clf.predict(X_test)\n",
        "rf_report = classification_report(y_test, y_pred_rf)\n",
        "print('Random Forest Classification Report:\\n', rf_report)\n",
        "\n",
        "# Calculate ROC curve and AUC\n",
        "rf_fpr, rf_tpr, rf_thresholds = roc_curve(y_test, rf_clf.predict_proba(X_test)[:,1], pos_label=1)\n",
        "rf_roc_auc = auc(rf_fpr, rf_tpr)\n",
        "\n",
        "# Plotting the ROC curve\n",
        "plt.figure()\n",
        "plt.plot(rf_fpr, rf_tpr, color='darkorange', lw=2, label='Random Forest ROC curve (area = %0.2f)' % rf_roc_auc)\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Random Forest Receiver Operating Characteristic')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        }
      },
      "id": "0e055c07-e51b-463b-9ecd-596293bb1b91"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest Example with Wine Dataset\n",
        "\n",
        "Now, we will apply the Random Forest model to the Wine dataset, following the same procedure as before. We will load the dataset, convert it to a DataFrame, describe it, apply the Random Forest model for classification, and analyze the precision, recall, accuracy, and ROC curve for each class."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "9d73b642-c6c1-45bc-8d62-fc8df297825a"
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize and train the Random Forest classifier for the Wine dataset\n",
        "rf_clf_wine = RandomForestClassifier(random_state=42)\n",
        "rf_clf_wine.fit(X_train_wine, y_train_wine)\n",
        "\n",
        "# Make predictions and evaluate the model\n",
        "y_pred_rf_wine = rf_clf_wine.predict(X_test_wine)\n",
        "rf_report_wine = classification_report(y_test_wine, y_pred_rf_wine)\n",
        "print('Random Forest Classification Report for Wine Dataset:\\n', rf_report_wine)\n",
        "\n",
        "# Calculate ROC curve and AUC for multi-class\n",
        "rf_fpr_wine = dict()\n",
        "rf_tpr_wine = dict()\n",
        "rf_roc_auc_wine = dict()\n",
        "for i in range(wine.target_names.size):\n",
        "    rf_fpr_wine[i], rf_tpr_wine[i], _ = roc_curve(y_test_wine, rf_clf_wine.predict_proba(X_test_wine)[:, i], pos_label=i)\n",
        "    rf_roc_auc_wine[i] = auc(rf_fpr_wine[i], rf_tpr_wine[i])\n",
        "\n",
        "# Plotting the ROC curve for each class\n",
        "plt.figure()\n",
        "for i in range(wine.target_names.size):\n",
        "    plt.plot(rf_fpr_wine[i], rf_tpr_wine[i], lw=2, label='Random Forest ROC curve of class %d (area = %0.2f)' % (i, rf_roc_auc_wine[i]))\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Random Forest Receiver Operating Characteristic for Wine Dataset')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        }
      },
      "id": "8481c40f-9d13-4f58-b61b-a5754f225d8a"
    }
  ],
  "metadata": {
    "noteable-chatgpt": {
      "create_notebook": {
        "openai_conversation_id": "637dba12-cd93-52f1-b3a0-f7c71cd7914f",
        "openai_ephemeral_user_id": "7124bf84-37dc-52e1-91f6-64c019aab58f",
        "openai_subdivision1_iso_code": "CL-RM"
      }
    },
    "kernel_info": {
      "name": "python3"
    },
    "noteable": {
      "last_delta_id": "8a1c7789-31f0-420c-9c21-21decf624d4e"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.9",
      "language": "python",
      "language_version": "3.9",
      "identifier": "legacy"
    },
    "selected_hardware_size": "small",
    "nteract": {
      "version": "noteable@2.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}