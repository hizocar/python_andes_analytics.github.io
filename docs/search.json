[
  {
    "objectID": "data_science_ingemat/presentacion.html",
    "href": "data_science_ingemat/presentacion.html",
    "title": "Presentando un proyecto",
    "section": "",
    "text": "Objetivo del Proyecto: Comienza con una visión general del proyecto y sus objetivos. Explica por qué se realizó el proyecto y cuál es su relevancia para la empresa.\n\n\n\nProblema a Resolver: Describe el problema que tu proyecto de ciencia de datos está tratando de resolver. Haz esto de una manera que sea comprensible para personas que no son expertas en el tema, utilizando ejemplos y analogías simples cuando sea posible.\n\n\n\nRecopilación de Datos: Explica cómo y de dónde obtuviste los datos para el proyecto.\n\n\nDescribe cómo se prepararon los datos para el análisis. Puedes mencionar técnicas de limpieza de datos, pero evita entrar en demasiados detalles técnicos.\n\n\n\nDescribe las técnicas y algoritmos que utilizaste para desarrollar el modelo de ciencia de datos. Explica por qué elegiste esos enfoques y cómo se relacionan con el problema que estás tratando de resolver. Intenta mantener la explicación accesible y evita el uso de jerga técnica tanto como sea posible.\n\n\n\n\nInterpretación de Resultados: Presenta los resultados de tu análisis de una manera fácil de entender. Utiliza gráficos y tablas para visualizar los datos y destacar los hallazgos clave. Evita presentar demasiados números crudos; en su lugar, trata de resumir y explicar los resultados de una manera que sea fácil de entender.\n\n\nExplica lo que los resultados significan para el negocio. ¿Qué acciones debería tomar la empresa basándose en estos resultados? ¿Cómo ayudará esto a la empresa a alcanzar sus objetivos?\n\n\n\n\n\n\nResume los hallazgos clave de tu proyecto y cómo estos hallazgos pueden beneficiar a la empresa.\n\n\n\nProporciona recomendaciones para los próximos pasos. ¿Hay otras preguntas que podrían explorarse en el futuro? ¿Cómo puede la empresa implementar los resultados de tu proyecto?\n\n\n\n\nDeja tiempo para preguntas al final de la presentación. Esto da a la audiencia la oportunidad de aclarar cualquier aspecto que no entienda y permite que se realicen preguntas más profundas que pueden no ser adecuadas durante la presentación principal.\nRecuerda, la clave es mantener el lenguaje simple y accesible, y tratar de relacionar todo con los objetivos y necesidades de la empresa. Tu meta es hacer que la audiencia comprenda el valor de tu trabajo, no impresionarlos con jerga técnica."
  },
  {
    "objectID": "data_science_ingemat/presentacion.html#cómo-presentar-un-proyecto-de-ciencias-de-datos",
    "href": "data_science_ingemat/presentacion.html#cómo-presentar-un-proyecto-de-ciencias-de-datos",
    "title": "Presentando un proyecto",
    "section": "",
    "text": "Objetivo del Proyecto: Comienza con una visión general del proyecto y sus objetivos. Explica por qué se realizó el proyecto y cuál es su relevancia para la empresa.\n\n\n\nProblema a Resolver: Describe el problema que tu proyecto de ciencia de datos está tratando de resolver. Haz esto de una manera que sea comprensible para personas que no son expertas en el tema, utilizando ejemplos y analogías simples cuando sea posible.\n\n\n\nRecopilación de Datos: Explica cómo y de dónde obtuviste los datos para el proyecto.\n\n\nDescribe cómo se prepararon los datos para el análisis. Puedes mencionar técnicas de limpieza de datos, pero evita entrar en demasiados detalles técnicos.\n\n\n\nDescribe las técnicas y algoritmos que utilizaste para desarrollar el modelo de ciencia de datos. Explica por qué elegiste esos enfoques y cómo se relacionan con el problema que estás tratando de resolver. Intenta mantener la explicación accesible y evita el uso de jerga técnica tanto como sea posible.\n\n\n\n\nInterpretación de Resultados: Presenta los resultados de tu análisis de una manera fácil de entender. Utiliza gráficos y tablas para visualizar los datos y destacar los hallazgos clave. Evita presentar demasiados números crudos; en su lugar, trata de resumir y explicar los resultados de una manera que sea fácil de entender.\n\n\nExplica lo que los resultados significan para el negocio. ¿Qué acciones debería tomar la empresa basándose en estos resultados? ¿Cómo ayudará esto a la empresa a alcanzar sus objetivos?\n\n\n\n\n\n\nResume los hallazgos clave de tu proyecto y cómo estos hallazgos pueden beneficiar a la empresa.\n\n\n\nProporciona recomendaciones para los próximos pasos. ¿Hay otras preguntas que podrían explorarse en el futuro? ¿Cómo puede la empresa implementar los resultados de tu proyecto?\n\n\n\n\nDeja tiempo para preguntas al final de la presentación. Esto da a la audiencia la oportunidad de aclarar cualquier aspecto que no entienda y permite que se realicen preguntas más profundas que pueden no ser adecuadas durante la presentación principal.\nRecuerda, la clave es mantener el lenguaje simple y accesible, y tratar de relacionar todo con los objetivos y necesidades de la empresa. Tu meta es hacer que la audiencia comprenda el valor de tu trabajo, no impresionarlos con jerga técnica."
  },
  {
    "objectID": "data_science_ingemat/modulo2/ARIMA_SARIMA_Tutorial.html",
    "href": "data_science_ingemat/modulo2/ARIMA_SARIMA_Tutorial.html",
    "title": "Quiz: Modelos ARIMA y SARIMA",
    "section": "",
    "text": "ARIMA es un acrónimo que significa AutoRegressive Integrated Moving Average. Es una clase de modelos que explica una serie de tiempo dada en función de sus propios valores pasados, es decir, su propia lags. Puede ser percibido como una regresión lineal de los datos pasados.\nLos componentes de los modelos ARIMA son:\n\nAR: Autoregression. Un modelo que utiliza la relación dependiente entre una observación y un número de observaciones rezagadas (lagged observations).\nI: Integrated. El uso de la diferenciación de las observaciones en bruto (por ejemplo, restar una observación de una observación en el paso de tiempo anterior) para hacer que la serie de tiempo sea estacionaria.\nMA: Moving Average. Un modelo que utiliza la dependencia entre una observación y un error residual de un modelo de media móvil aplicado a observaciones rezagadas.\n\nUn modelo ARIMA se denota como ARIMA(p,d,q) donde:\n\np es el número de términos autorregresivos (AR part). Permite incorporar el efecto de los valores pasados en nuestro modelo.\nd es el número de diferencias no estacionales necesarias para la estacionariedad.\nq es el número de errores de pronóstico rezagados en la ecuación de predicción (MA part)."
  },
  {
    "objectID": "data_science_ingemat/modulo2/ARIMA_SARIMA_Tutorial.html#modelos-arima",
    "href": "data_science_ingemat/modulo2/ARIMA_SARIMA_Tutorial.html#modelos-arima",
    "title": "Quiz: Modelos ARIMA y SARIMA",
    "section": "",
    "text": "ARIMA es un acrónimo que significa AutoRegressive Integrated Moving Average. Es una clase de modelos que explica una serie de tiempo dada en función de sus propios valores pasados, es decir, su propia lags. Puede ser percibido como una regresión lineal de los datos pasados.\nLos componentes de los modelos ARIMA son:\n\nAR: Autoregression. Un modelo que utiliza la relación dependiente entre una observación y un número de observaciones rezagadas (lagged observations).\nI: Integrated. El uso de la diferenciación de las observaciones en bruto (por ejemplo, restar una observación de una observación en el paso de tiempo anterior) para hacer que la serie de tiempo sea estacionaria.\nMA: Moving Average. Un modelo que utiliza la dependencia entre una observación y un error residual de un modelo de media móvil aplicado a observaciones rezagadas.\n\nUn modelo ARIMA se denota como ARIMA(p,d,q) donde:\n\np es el número de términos autorregresivos (AR part). Permite incorporar el efecto de los valores pasados en nuestro modelo.\nd es el número de diferencias no estacionales necesarias para la estacionariedad.\nq es el número de errores de pronóstico rezagados en la ecuación de predicción (MA part)."
  },
  {
    "objectID": "data_science_ingemat/modulo2/ARIMA_SARIMA_Tutorial.html#modelos-sarima",
    "href": "data_science_ingemat/modulo2/ARIMA_SARIMA_Tutorial.html#modelos-sarima",
    "title": "Quiz: Modelos ARIMA y SARIMA",
    "section": "Modelos SARIMA",
    "text": "Modelos SARIMA\nLos modelos SARIMA (Seasonal Autoregressive Integrated Moving Average) son una extensión de los modelos ARIMA que soportan directamente las series de tiempo univariadas con un componente estacional.\nUn modelo SARIMA se denota como SARIMA(p,d,q)(P,D,Q)m donde:\n\np, d, q son los parámetros del modelo ARIMA para las tendencias no estacionales.\nP, D, Q son los parámetros del modelo ARIMA para las tendencias estacionales.\nm se refiere al número de periodos en cada temporada."
  },
  {
    "objectID": "data_science_ingemat/modulo2/ARIMA_SARIMA_Tutorial.html#ejemplo-en-python",
    "href": "data_science_ingemat/modulo2/ARIMA_SARIMA_Tutorial.html#ejemplo-en-python",
    "title": "Quiz: Modelos ARIMA y SARIMA",
    "section": "Ejemplo en Python",
    "text": "Ejemplo en Python\nAquí hay un ejemplo de cómo ajustar un modelo ARIMA y SARIMA a una serie de tiempo en Python usando la biblioteca statsmodels.\nimport pandas as pd\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\n\n# Cargar datos\nseries = pd.read_csv('your_data.csv', header=0, index_col=0, parse_dates=True, squeeze=True)\n\n# Ajustar modelo ARIMA\nmodel_arima = ARIMA(series, order=(5,1,0))\nmodel_arima_fit = model_arima.fit(disp=0)\nprint(model_arima_fit.summary())\n\n# Ajustar modelo SARIMA\nmodel_sarima = SARIMAX(series, order=(1, 1, 1), seasonal_order=(1, 1, 1, 12))\nmodel_sarima_fit = model_sarima.fit(disp=False)\nprint(model_sarima_fit.summary())\nPor favor, reemplace 'your_data.csv' con la ruta a su archivo de datos. Los parámetros (5,1,0) y (1, 1, 1, 12) son solo ejemplos y deben ser ajustados a sus datos específicos."
  },
  {
    "objectID": "data_science_ingemat/modulo2/ARIMA_SARIMA_Tutorial.html#detalles-adicionales-sobre-los-modelos-arima",
    "href": "data_science_ingemat/modulo2/ARIMA_SARIMA_Tutorial.html#detalles-adicionales-sobre-los-modelos-arima",
    "title": "Quiz: Modelos ARIMA y SARIMA",
    "section": "Detalles adicionales sobre los modelos ARIMA",
    "text": "Detalles adicionales sobre los modelos ARIMA\nLos modelos ARIMA son aplicados en algunos casos donde los datos muestran evidencia de no estacionariedad, donde una etapa inicial de diferenciación (correspondiente al componente ‘I’ en el modelo) puede ser aplicada una o más veces para eliminar la tendencia no estacionaria.\nEl componente AR del modelo ARIMA indica que la variable evolutiva de interés es regresada en sus propios valores rezagados, es decir, anteriores. Por ejemplo, si p es 5, los predictores para x(t) serán x(t-1)….x(t-5).\nEl componente MA del modelo ARIMA indica que el error de regresión es en realidad una combinación lineal de términos de error cuyos valores ocurrieron contemporáneamente y en varios momentos en el pasado.\nEl componente I del modelo ARIMA indica que los datos se han diferenciado al menos una vez para hacer la serie estacionaria."
  },
  {
    "objectID": "data_science_ingemat/modulo2/ARIMA_SARIMA_Tutorial.html#detalles-adicionales-sobre-los-modelos-sarima",
    "href": "data_science_ingemat/modulo2/ARIMA_SARIMA_Tutorial.html#detalles-adicionales-sobre-los-modelos-sarima",
    "title": "Quiz: Modelos ARIMA y SARIMA",
    "section": "Detalles adicionales sobre los modelos SARIMA",
    "text": "Detalles adicionales sobre los modelos SARIMA\nLos modelos SARIMA añaden una capa adicional de complejidad al modelado de series temporales. Estos modelos incorporan elementos de estacionalidad tanto en el componente AR como en el MA del modelo. Esto significa que el modelo no sólo considera las relaciones de los datos consigo mismo (AR), los errores (MA) y las diferencias para la estacionariedad (I), sino que también tiene en cuenta la estacionalidad de los datos.\nEl componente SAR del modelo SARIMA indica que la serie temporal tiene correlaciones estacionales autorregresivas. Por ejemplo, si P es 2, los predictores para x(t) serán x(t-12) y x(t-24) si la serie es mensual.\nEl componente SMA del modelo SARIMA indica que la serie temporal tiene correlaciones estacionales de media móvil. Por ejemplo, si Q es 1, los predictores para x(t) serán los errores en los periodos t-12 y t-24 si la serie es mensual.\nEl componente SI del modelo SARIMA indica que los datos se han diferenciado al menos una vez para eliminar la estacionalidad.\nEl parámetro m en el modelo SARIMA se refiere al número de periodos por temporada, y es fundamental para modelar correctamente la estacionalidad. Por ejemplo, m sería 4 para datos trimestrales, 12 para datos mensuales, o 7 para datos diarios si se espera una estacionalidad semanal.\n\nQUIZ:\nA continuación responde el siguiente quiz: https://forms.gle/aA9Mx1D5BNvhb1SL7"
  },
  {
    "objectID": "data_science_ingemat/modulo2/complemento.html",
    "href": "data_science_ingemat/modulo2/complemento.html",
    "title": "Temas complementarios",
    "section": "",
    "text": "Los modelos ARIMA y SARIMA son una extensión de los modelos de regresión lineal que se utilizan para modelar series de tiempo no estacionarias. Estos modelos pueden capturar una amplia gama de estructuras de dependencia temporal.\n\nLectura recomendada: ARIMA model as a tool in forecasting the variables: A case study\n\n\n\n\nLos modelos de volatilidad estocástica son útiles para modelar series de tiempo financieras, donde la volatilidad (la varianza de los errores) puede cambiar con el tiempo.\n\nLectura recomendada: Stochastic Volatility Modeling\n\n\n\n\nEstos modelos permiten que los parámetros de un modelo de regresión cambien en diferentes “regímenes” o estados del mundo.\n\nLectura recomendada: A new approach to the economic analysis of nonstationary time series and the business cycle\n\n\n\n\nLos modelos de espacio de estados son una clase muy general de modelos de series de tiempo que incluyen modelos ARIMA y muchos otros como casos especiales. El filtro de Kalman es un algoritmo que se utiliza para estimar las variables latentes en estos modelos.\n\nLectura recomendada: Time Series Analysis by State Space Methods\n\n\n\n\nEstos modelos permiten analizar varias series de tiempo a la vez, capturando las interdependencias entre las series.\n\nLectura recomendada: Applied Multivariate Time Series Analysis: Foundations and Trends in Econometrics\n\n\n\n\nEl aprendizaje automático ofrece una serie de técnicas que pueden ser útiles para modelar series de tiempo, especialmente cuando se dispone de grandes cantidades de datos.\n\nLectura recomendada: Deep Learning for Time Series Forecasting"
  },
  {
    "objectID": "data_science_ingemat/modulo2/complemento.html#modelos-arima-y-sarima",
    "href": "data_science_ingemat/modulo2/complemento.html#modelos-arima-y-sarima",
    "title": "Temas complementarios",
    "section": "",
    "text": "Los modelos ARIMA y SARIMA son una extensión de los modelos de regresión lineal que se utilizan para modelar series de tiempo no estacionarias. Estos modelos pueden capturar una amplia gama de estructuras de dependencia temporal.\n\nLectura recomendada: ARIMA model as a tool in forecasting the variables: A case study"
  },
  {
    "objectID": "data_science_ingemat/modulo2/complemento.html#modelos-de-volatilidad-estocástica",
    "href": "data_science_ingemat/modulo2/complemento.html#modelos-de-volatilidad-estocástica",
    "title": "Temas complementarios",
    "section": "",
    "text": "Los modelos de volatilidad estocástica son útiles para modelar series de tiempo financieras, donde la volatilidad (la varianza de los errores) puede cambiar con el tiempo.\n\nLectura recomendada: Stochastic Volatility Modeling"
  },
  {
    "objectID": "data_science_ingemat/modulo2/complemento.html#modelos-de-regresión-con-cambios-de-régimen",
    "href": "data_science_ingemat/modulo2/complemento.html#modelos-de-regresión-con-cambios-de-régimen",
    "title": "Temas complementarios",
    "section": "",
    "text": "Estos modelos permiten que los parámetros de un modelo de regresión cambien en diferentes “regímenes” o estados del mundo.\n\nLectura recomendada: A new approach to the economic analysis of nonstationary time series and the business cycle"
  },
  {
    "objectID": "data_science_ingemat/modulo2/complemento.html#modelos-de-espacio-de-estados-y-filtros-de-kalman",
    "href": "data_science_ingemat/modulo2/complemento.html#modelos-de-espacio-de-estados-y-filtros-de-kalman",
    "title": "Temas complementarios",
    "section": "",
    "text": "Los modelos de espacio de estados son una clase muy general de modelos de series de tiempo que incluyen modelos ARIMA y muchos otros como casos especiales. El filtro de Kalman es un algoritmo que se utiliza para estimar las variables latentes en estos modelos.\n\nLectura recomendada: Time Series Analysis by State Space Methods"
  },
  {
    "objectID": "data_science_ingemat/modulo2/complemento.html#modelos-de-series-de-tiempo-multivariadas",
    "href": "data_science_ingemat/modulo2/complemento.html#modelos-de-series-de-tiempo-multivariadas",
    "title": "Temas complementarios",
    "section": "",
    "text": "Estos modelos permiten analizar varias series de tiempo a la vez, capturando las interdependencias entre las series.\n\nLectura recomendada: Applied Multivariate Time Series Analysis: Foundations and Trends in Econometrics"
  },
  {
    "objectID": "data_science_ingemat/modulo2/complemento.html#modelos-de-series-de-tiempo-con-machine-learning",
    "href": "data_science_ingemat/modulo2/complemento.html#modelos-de-series-de-tiempo-con-machine-learning",
    "title": "Temas complementarios",
    "section": "",
    "text": "El aprendizaje automático ofrece una serie de técnicas que pueden ser útiles para modelar series de tiempo, especialmente cuando se dispone de grandes cantidades de datos.\n\nLectura recomendada: Deep Learning for Time Series Forecasting"
  },
  {
    "objectID": "data_science_ingemat/modulo2/introduccion.html",
    "href": "data_science_ingemat/modulo2/introduccion.html",
    "title": "Tema 1: Introducción",
    "section": "",
    "text": "Introducción a las Series de Tiempo\nUna serie de tiempo es una secuencia de observaciones tomadas secuencialmente en el tiempo. Es una herramienta esencial en el análisis de datos temporales y se utiliza en muchos campos, como la economía, las ciencias sociales, la física, y la ingeniería.\n\nDefinición Formal\nFormalmente, una serie de tiempo puede ser vista como un conjunto de puntos de datos indexados (o listados o graficados) en el orden del tiempo. Generalmente, una serie de tiempo se representa como:\nX = {X1, X2,..., Xt,..., Xn}\ndonde Xt representa la observación en el tiempo t.\n\n\nAplicaciones de las Series de Tiempo\nLas series de tiempo se utilizan en una amplia variedad de aplicaciones, incluyendo:\n\nEconomía: para el análisis de indicadores económicos, como el producto interno bruto, la inflación, las tasas de desempleo, etc.\nFinanzas: para el seguimiento y la predicción de los precios de las acciones, las tasas de interés, y otros indicadores financieros.\nMeteorología: para predecir el clima y los patrones climáticos.\nCiencias de la Salud: para rastrear la propagación de enfermedades o la eficacia de un tratamiento a lo largo del tiempo.\n\n\n\nClasificación de las Series de Tiempo\nLas series de tiempo pueden ser clasificadas de acuerdo a su estructura en:\n\nSeries de Tiempo Estacionarias: Estas series tienen propiedades que no dependen del tiempo. En otras palabras, poseen una media y una varianza constantes a lo largo del tiempo.\nSeries de Tiempo No Estacionarias: Estas series muestran tendencias y/o patrones estacionales. No tienen una media y/o varianza constantes a lo largo del tiempo.\nSeries de Tiempo Estacionales: Estas son un tipo especial de series no estacionarias que exhiben una tendencia estacional, es decir, un patrón que se repite en intervalos de tiempo específicos."
  },
  {
    "objectID": "data_science_ingemat/classes.html",
    "href": "data_science_ingemat/classes.html",
    "title": "Data Science for Math Engineers USACH",
    "section": "",
    "text": "Programa\nPresentación"
  },
  {
    "objectID": "data_science_ingemat/classes.html#documentos-del-curso",
    "href": "data_science_ingemat/classes.html#documentos-del-curso",
    "title": "Data Science for Math Engineers USACH",
    "section": "",
    "text": "Programa\nPresentación"
  },
  {
    "objectID": "data_science_ingemat/classes.html#clases",
    "href": "data_science_ingemat/classes.html#clases",
    "title": "Data Science for Math Engineers USACH",
    "section": "Clases",
    "text": "Clases\n\nModulo 2\n\nIntroducción\nEstacionariedad\nProphet\nComplemento\nQuiz\nProyecto"
  },
  {
    "objectID": "ejemplo.html",
    "href": "ejemplo.html",
    "title": "TITULO DEL EJEMPLO DE JUPYTER",
    "section": "",
    "text": "print('Hola')"
  },
  {
    "objectID": "machine_learning_egei/homeworks/lab1.html",
    "href": "machine_learning_egei/homeworks/lab1.html",
    "title": "Self-Learning Lab: Python Dataframes with Examples",
    "section": "",
    "text": "Welcome to this self-learning lab on Python Dataframes! In this lab, you will explore and practice key concepts related to dataframes using the pandas library. The lab is designed to be completed in approximately two hours. For each topic, we will provide a brief overview, an example, and then an exercise for you to complete. The exercises are designed with a business context in mind, so you’ll be able to apply these concepts in real-world scenarios. ## Contents: 1. Introduction to Dataframes in Pandas 2. Data Transformation: Creating New Columns 3. Filtering and Indexing in Dataframes 4. The apply Function and Lambda Expressions 5. One Hot Encoding Let’s dive in!"
  },
  {
    "objectID": "machine_learning_egei/homeworks/lab1.html#introduction-to-dataframes-in-pandas",
    "href": "machine_learning_egei/homeworks/lab1.html#introduction-to-dataframes-in-pandas",
    "title": "Self-Learning Lab: Python Dataframes with Examples",
    "section": "1. Introduction to Dataframes in Pandas",
    "text": "1. Introduction to Dataframes in Pandas\nA DataFrame is a 2-dimensional labeled data structure with columns that can be of different types. It is similar to a spreadsheet, a SQL table, or the data.frame in R. The DataFrame is one of the most commonly used pandas objects and is designed to handle a mix of numeric and non-numeric data. ### Example: Let’s create a simple dataframe with product sales data.\nimport pandas as pd\ndata = {\n    'Product Name': ['Laptop', 'Mouse', 'Keyboard'],\n    'Units Sold': [50, 150, 80],\n    'Price per Unit': [800, 20, 30]\n}\ndf = pd.DataFrame(data)\nprint(df)\n\nExercise 1.1:\nImagine you are a data analyst at a retail company. You have been given sales data for the past month. The data includes the product name, the number of units sold, and the price per unit. Using the pandas library, create a dataframe with the following data: | Product Name | Units Sold | Price per Unit | |————–|————|—————-| | Laptop | 50 | 800 | | Mouse | 150 | 20 | | Keyboard | 80 | 30 | Calculate the total sales for each product and add it as a new column to the dataframe."
  },
  {
    "objectID": "machine_learning_egei/homeworks/lab1.html#data-transformation-creating-new-columns",
    "href": "machine_learning_egei/homeworks/lab1.html#data-transformation-creating-new-columns",
    "title": "Self-Learning Lab: Python Dataframes with Examples",
    "section": "2. Data Transformation: Creating New Columns",
    "text": "2. Data Transformation: Creating New Columns\nOften, while working with data, you might want to create new columns based on the existing columns in the dataframe. This can be done using various operations and functions. ### Example: Given the sales data, let’s calculate the total sales for each product.\ndf['Total Sales'] = df['Units Sold'] * df['Price per Unit']\nprint(df)\n\nExercise 2.1:\nContinuing with the retail company scenario, let’s say you have been given additional data regarding the cost of each product. You are required to calculate the profit for each product. Add the following data to your dataframe: | Product Name | Cost per Unit | |————–|—————| | Laptop | 600 | | Mouse | 10 | | Keyboard | 20 | Create a new column in the dataframe to calculate the profit for each product. Profit is calculated as (Price per Unit - Cost per Unit) * Units Sold."
  },
  {
    "objectID": "machine_learning_egei/homeworks/lab1.html#filtering-and-indexing-in-dataframes",
    "href": "machine_learning_egei/homeworks/lab1.html#filtering-and-indexing-in-dataframes",
    "title": "Self-Learning Lab: Python Dataframes with Examples",
    "section": "3. Filtering and Indexing in Dataframes",
    "text": "3. Filtering and Indexing in Dataframes\nFiltering allows you to select specific rows based on a condition. Indexing, on the other hand, helps in selecting specific rows and columns from the dataframe. ### Example: From the sales data, let’s extract the details of products that have sold more than 100 units.\nhigh_selling_products = df[df['Units Sold'] &gt; 100]\nprint(high_selling_products[['Product Name', 'Units Sold']])\n\nExercise 3.1:\nFrom the sales data, extract the details of products that have sold more than 100 units. Display only the ‘Product Name’ and ‘Units Sold’ columns. ### Exercise 3.2: From the sales data, extract the details of the product that has the highest profit. Display the ‘Product Name’ and ‘Profit’ columns."
  },
  {
    "objectID": "machine_learning_egei/homeworks/lab1.html#the-apply-function-and-lambda-expressions",
    "href": "machine_learning_egei/homeworks/lab1.html#the-apply-function-and-lambda-expressions",
    "title": "Self-Learning Lab: Python Dataframes with Examples",
    "section": "4. The apply Function and Lambda Expressions",
    "text": "4. The apply Function and Lambda Expressions\nThe apply function is used to apply a function along the axis of the dataframe. Lambda expressions are small anonymous functions that can be used with functions like apply. ### Example: Using the sales data, let’s create a new column ‘Category’ that categorizes products as ‘High Selling’ if units sold are greater than 100, and ‘Low Selling’ otherwise.\ndf['Category'] = df['Units Sold'].apply(lambda x: 'High Selling' if x &gt; 100 else 'Low Selling')\nprint(df[['Product Name', 'Category']])\n\nExercise 4.1:\nUsing the sales data, create a new column ‘Category’ that categorizes products as ‘High Selling’ if units sold are greater than 100, and ‘Low Selling’ otherwise. Use the apply function and a lambda expression to achieve this. ### Exercise 4.2: Calculate the average profit per unit for each product and add it as a new column to the dataframe. Use the apply function to achieve this."
  },
  {
    "objectID": "machine_learning_egei/homeworks/lab1.html#one-hot-encoding",
    "href": "machine_learning_egei/homeworks/lab1.html#one-hot-encoding",
    "title": "Self-Learning Lab: Python Dataframes with Examples",
    "section": "5. One Hot Encoding",
    "text": "5. One Hot Encoding\nOne hot encoding is a process of converting categorical data variables so they can be provided to machine learning algorithms to improve predictions. It involves converting each value in a column to a new column and assigning a 1 or 0 (True/False) value to the column. ### Example: Using the sales data, let’s one-hot encode the ‘Category’ column.\nencoded_df = pd.get_dummies(df, columns=['Category'])\nprint(encoded_df)\n\nExercise 5.1:\nUsing the sales data, one-hot encode the ‘Category’ column. This will be useful when you want to use this data for machine learning algorithms. ### Exercise 5.2: Imagine you have another column ‘Region’ with values ‘North’, ‘South’, ‘East’, and ‘West’. One-hot encode this column."
  },
  {
    "objectID": "machine_learning_egei/homeworks/homework_III.html",
    "href": "machine_learning_egei/homeworks/homework_III.html",
    "title": "Dealing with Missing Data in a Dataset",
    "section": "",
    "text": "The first step in dealing with missing data is to identify its presence and location within your dataset. This can be done using various methods, depending on the tools and programming language you are using. In Python, for instance, you can use libraries like Pandas to easily find missing values.\n\nimport pandas as pd\n\n\n# Assuming 'df' is your DataFrame\n\nmissing_values = df.isnull()\n\nprint(missing_values.sum())\nThis code will give you a count of missing values in each column of your DataFrame."
  },
  {
    "objectID": "machine_learning_egei/homeworks/homework_III.html#identifying-missing-data",
    "href": "machine_learning_egei/homeworks/homework_III.html#identifying-missing-data",
    "title": "Dealing with Missing Data in a Dataset",
    "section": "",
    "text": "The first step in dealing with missing data is to identify its presence and location within your dataset. This can be done using various methods, depending on the tools and programming language you are using. In Python, for instance, you can use libraries like Pandas to easily find missing values.\n\nimport pandas as pd\n\n\n# Assuming 'df' is your DataFrame\n\nmissing_values = df.isnull()\n\nprint(missing_values.sum())\nThis code will give you a count of missing values in each column of your DataFrame."
  },
  {
    "objectID": "machine_learning_egei/homeworks/homework_III.html#handling-missing-data",
    "href": "machine_learning_egei/homeworks/homework_III.html#handling-missing-data",
    "title": "Dealing with Missing Data in a Dataset",
    "section": "Handling Missing Data",
    "text": "Handling Missing Data\nOnce you’ve identified the missing data, the next step is to decide how to handle it. The approach depends on the nature of your data and the amount of missing information. Here are some common strategies:\n\n1. Removing Data\n\nDrop rows with missing values: If the dataset is large and the number of rows with missing data is small, you might consider removing these rows.\n\n\ndf.dropna(inplace=True)\n\nDrop columns with missing values: If a specific column has a significant number of missing values, it might be better to remove the entire column.\n\n\ndf.dropna(axis=1, inplace=True)\n\n\n2. Imputing Data\n\nMean/Median/Mode Imputation: Replace missing values with the mean, median, or mode of the column. This is useful for numerical data.\n\n\n# For mean imputation\n\ndf.fillna(df.mean(), inplace=True)\n\n\n# For median imputation\n\ndf.fillna(df.median(), inplace=True)\n\n\n# For mode imputation (for categorical data)\n\ndf.fillna(df.mode().iloc[0], inplace=True)\n\nCustom Imputation: Use domain knowledge or other algorithms to impute missing values.\n\n\n\n3. Using Algorithms that Support Missing Values\n\nSome machine learning algorithms can handle missing values inherently. For example, decision trees and random forests can handle missing data without imputation.\n\n\nimport pandas as pd\n\n# Example dataset with missing values\n\ndata = {'Name': ['Alice', 'Bob', 'Charlie', None],\n\n        'Age': [25, None, 30, 22],\n\n        'Salary': [70000, 80000, None, 40000]}\n\ndf = pd.DataFrame(data)\n\nprint(df)\n\n\n# Identifying missing values\n\nprint(\"Missing values in each column:\")\n\nprint(df.isnull().sum())\n\n\n# Handling missing data\n\n# Option 1: Removing rows with missing values\n\ndf_dropped_rows = df.dropna()\n\nprint(\"DataFrame after dropping rows with missing values:\")\n\nprint(df_dropped_rows)\n\n\n# Option 2: Imputing missing values\n\n# For simplicity, we'll use mean for numerical columns and mode for categorical columns\n\ndf_imputed = df.copy()\n\ndf_imputed['Age'].fillna(df['Age'].mean(), inplace=True)\n\ndf_imputed['Salary'].fillna(df['Salary'].mean(), inplace=True)\n\ndf_imputed['Name'].fillna(df['Name'].mode()[0], inplace=True)\n\nprint(\"\\nDataFrame after imputing missing values:\")\n\nprint(df_imputed)"
  },
  {
    "objectID": "machine_learning_egei/homeworks/homework_III.html#understanding-and-handling-outliers",
    "href": "machine_learning_egei/homeworks/homework_III.html#understanding-and-handling-outliers",
    "title": "Dealing with Missing Data in a Dataset",
    "section": "Understanding and Handling Outliers",
    "text": "Understanding and Handling Outliers\nOutliers are data points that differ significantly from other observations. They can occur due to variability in the measurement or may indicate experimental errors. Handling outliers is crucial as they can lead to misleading representations and affect the results of data analysis.\n\nSteps to Handle Outliers:\n\nVisualizing Data: Using plots like boxplots to identify outliers.\nIdentifying Outliers: Determining which data points are considered outliers.\nRemoving Outliers: Deciding on a strategy to handle outliers, often by removing them.\n\nWe will demonstrate these steps using a dataset with more than 50 rows, analyze it with a boxplot, identify outliers, and then remove them.\n\nimport numpy as np\n\nimport matplotlib.pyplot as plt\n\n\n# Generating a dataset with more than 50 rows\n\nnp.random.seed(0)\n\ndata = np.random.normal(100, 20, 60)\n\n\n# Introducing outliers\n\ndata = np.append(data, [300, 305])\n\n\n# Creating a DataFrame\n\ndf_outliers = pd.DataFrame(data, columns=['Values'])\n\n\n# Displaying the first few rows\n\nprint(df_outliers.head())\n\n\n# Visualizing the data with a boxplot\n\nplt.figure(figsize=(10, 6))\n\nplt.boxplot(df_outliers['Values'])\n\nplt.title('Boxplot of Values')\n\nplt.ylabel('Value')\n\nplt.show()\n\n\n# This boxplot will help us identify the outliers visually.\n\n\n# Identifying and removing outliers\n\nQ1 = df_outliers['Values'].quantile(0.25)\n\nQ3 = df_outliers['Values'].quantile(0.75)\n\nIQR = Q3 - Q1\n\n\n# Defining bounds for outliers\n\nlower_bound = Q1 - 1.5 * IQR\n\nupper_bound = Q3 + 1.5 * IQR\n\n\n# Filtering out the outliers\n\ndf_filtered = df_outliers[(df_outliers['Values'] &gt;= lower_bound) & (df_outliers['Values'] &lt;= upper_bound)]\n\n\n# Displaying the filtered DataFrame\n\nprint(\"DataFrame after removing outliers:\")\n\nprint(df_filtered)"
  },
  {
    "objectID": "machine_learning_egei/homeworks/homework_III.html#selecting-variables-for-classification-using-the-iris-dataset",
    "href": "machine_learning_egei/homeworks/homework_III.html#selecting-variables-for-classification-using-the-iris-dataset",
    "title": "Dealing with Missing Data in a Dataset",
    "section": "Selecting Variables for Classification Using the Iris Dataset",
    "text": "Selecting Variables for Classification Using the Iris Dataset\nIn this section, we will demonstrate how to select variables for a classification model using the Iris dataset. This dataset is a classic in machine learning, featuring measurements of iris flowers and their species. We will explore different techniques to determine which features are most important for classification.\n\nfrom sklearn.datasets import load_iris\n\nfrom sklearn.feature_selection import SelectKBest\n\nfrom sklearn.feature_selection import chi2\n\n\n# Loading the Iris dataset\n\niris = load_iris()\n\nX = iris.data\n\ny = iris.target\n\n\n# Displaying the first few rows of the dataset\n\nprint(X[:5, :])\n\nprint(y[:5])\n\n\n# Feature Selection using SelectKBest and Chi-Squared Test\n\n# Selecting the top 2 features\n\nselector = SelectKBest(score_func=chi2, k=2)\n\nX_selected = selector.fit_transform(X, y)\n\n\n# Displaying the selected features\n\nprint(\"Selected Features:\")\n\nprint(X_selected[:5, :])\n\n\n# The scores for each feature\n\nprint(\"Feature Scores:\")\n\nprint(selector.scores_)"
  },
  {
    "objectID": "machine_learning_egei/homeworks/homework_III.html#feature-selection-using-random-forest-on-the-wine-dataset",
    "href": "machine_learning_egei/homeworks/homework_III.html#feature-selection-using-random-forest-on-the-wine-dataset",
    "title": "Dealing with Missing Data in a Dataset",
    "section": "Feature Selection Using Random Forest on the Wine Dataset",
    "text": "Feature Selection Using Random Forest on the Wine Dataset\nAnother popular technique for feature selection is using a Random Forest classifier. This method is particularly useful for understanding feature importance in classification tasks. We will use the Wine dataset, another classic dataset in machine learning, to demonstrate this technique.\n\nfrom sklearn.datasets import load_wine\n\nfrom sklearn.ensemble import RandomForestClassifier\n\n\n# Loading the Wine dataset\n\nwine = load_wine()\n\nX_wine = wine.data\n\ny_wine = wine.target\n\n\n# Displaying the first few rows of the dataset\n\nprint(X_wine[:5, :])\n\nprint(y_wine[:5])\n\n\n# Feature Selection using Random Forest\n\nrf = RandomForestClassifier(n_estimators=100)\n\nrf.fit(X_wine, y_wine)\n\n\n# Getting feature importances\n\nimportances = rf.feature_importances_\n\n\n# Sorting the feature importances in descending order\n\nindices = np.argsort(importances)[::-1]\n\n\n# Displaying the feature importances\n\nprint(\"Feature ranking:\")\n\nfor f in range(X_wine.shape[1]):\n\n    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n\n\n# Plotting feature importances\n\nplt.figure(figsize=(12, 6))\n\nplt.title(\"Feature Importances in the Wine Dataset\")\n\nplt.bar(range(X_wine.shape[1]), importances[indices], color=\"r\", align=\"center\")\n\nplt.xticks(range(X_wine.shape[1]), wine.feature_names, rotation=45)\n\nplt.xlim([-1, X_wine.shape[1]])\n\nplt.ylabel('Importance')\n\nplt.xlabel('Features')\n\nplt.show()\n\n\n# This plot will help us visually assess which features are most important."
  },
  {
    "objectID": "machine_learning_egei/homeworks/homework_III.html#conclusion-on-feature-selection-for-the-wine-dataset",
    "href": "machine_learning_egei/homeworks/homework_III.html#conclusion-on-feature-selection-for-the-wine-dataset",
    "title": "Dealing with Missing Data in a Dataset",
    "section": "Conclusion on Feature Selection for the Wine Dataset",
    "text": "Conclusion on Feature Selection for the Wine Dataset\nBased on the feature importance rankings and the visual representation, we can conclude which features are most significant for the classification model. Generally, features with higher importance scores are more influential in predicting the target variable. In this case, we would select the top-ranking features as they have the highest impact on the model’s performance."
  },
  {
    "objectID": "machine_learning_egei/homeworks/homework_III.html#feature-selection-using-principal-component-analysis-pca-on-the-breast-cancer-dataset",
    "href": "machine_learning_egei/homeworks/homework_III.html#feature-selection-using-principal-component-analysis-pca-on-the-breast-cancer-dataset",
    "title": "Dealing with Missing Data in a Dataset",
    "section": "Feature Selection Using Principal Component Analysis (PCA) on the Breast Cancer Dataset",
    "text": "Feature Selection Using Principal Component Analysis (PCA) on the Breast Cancer Dataset\nPrincipal Component Analysis (PCA) is a technique used for dimensionality reduction, which can also be helpful in feature selection. It transforms the data into a new set of variables, the principal components, which are orthogonal and uncorrelated. We will use the Breast Cancer dataset to demonstrate this technique.\n\nfrom sklearn.datasets import load_breast_cancer\n\nfrom sklearn.decomposition import PCA\n\nimport pandas as pd\n\n\n# Loading the Breast Cancer dataset\n\nbreast_cancer = load_breast_cancer()\n\nX_cancer = breast_cancer.data\n\ny_cancer = breast_cancer.target\n\n\n# Creating a DataFrame for better visualization\n\ndf_cancer = pd.DataFrame(X_cancer, columns=breast_cancer.feature_names)\n\ndf_cancer['target'] = y_cancer\n\n\n# Displaying the first few rows of the dataset\n\ndf_cancer.head()\n\n\n# Applying PCA\n\npca = PCA(n_components=2)\n\nX_pca = pca.fit_transform(X_cancer)\n\n\n# Creating a DataFrame for the PCA results\n\ndf_pca = pd.DataFrame(X_pca, columns=['PC1', 'PC2'])\n\ndf_pca['target'] = y_cancer\n\n\n# Displaying the first few rows of the PCA results\n\ndf_pca.head()\n\n\n# Visualizing the PCA results\n\nimport matplotlib.pyplot as plt\n\nimport seaborn as sns\n\n\n# Plotting the PCA components\n\nplt.figure(figsize=(10, 6))\n\nsns.scatterplot(x='PC1', y='PC2', hue='target', data=df_pca, palette='Set1')\n\nplt.title('PCA of Breast Cancer Dataset')\n\nplt.xlabel('Principal Component 1')\n\nplt.ylabel('Principal Component 2')\n\nplt.legend(title='Target')\n\nplt.show()\n\n\n# This plot will help us understand the distribution of the data in the new feature space created by PCA."
  },
  {
    "objectID": "machine_learning_egei/homeworks/homework_III.html#conclusion-on-feature-selection-with-pca",
    "href": "machine_learning_egei/homeworks/homework_III.html#conclusion-on-feature-selection-with-pca",
    "title": "Dealing with Missing Data in a Dataset",
    "section": "Conclusion on Feature Selection with PCA",
    "text": "Conclusion on Feature Selection with PCA\nAfter applying PCA to the Breast Cancer dataset, we can observe how the data is distributed across the principal components. PCA helps in reducing the dimensionality of the dataset while retaining the most significant features. In this case, we transformed the data into two principal components. These components can be used for further analysis or in building classification models, as they encapsulate the most variance of the dataset."
  },
  {
    "objectID": "machine_learning_egei/homeworks/homework_III.html#student-exercise-preprocessing-and-model-selection-with-cardio-dataset",
    "href": "machine_learning_egei/homeworks/homework_III.html#student-exercise-preprocessing-and-model-selection-with-cardio-dataset",
    "title": "Dealing with Missing Data in a Dataset",
    "section": "Student Exercise: Preprocessing and Model Selection with Cardio Dataset",
    "text": "Student Exercise: Preprocessing and Model Selection with Cardio Dataset\nNow, it’s your turn to apply the techniques we’ve discussed using the cardio.csv dataset. Your tasks are as follows:\n\nHandle Missing Data: Either remove missing values or impute them based on the techniques discussed.\nEliminate Outliers: Identify and remove outliers from each column.\nFeature Selection: Choose the best variables for the model using the methods we’ve explored.\nModel Building and Comparison: Build models using KNN (K-Nearest Neighbors) and Logistic Regression. Compare the performance of these models with the metrics obtained in the previous class.\n\nThis exercise will help you solidify your understanding of data preprocessing and model selection. Good luck!"
  },
  {
    "objectID": "machine_learning_egei/class4.html",
    "href": "machine_learning_egei/class4.html",
    "title": "Classification",
    "section": "",
    "text": "Classification is a type of supervised learning where the goal is to predict the categorical class labels of new instances, based on past observations. It involves assigning a class label to input data, and the classification is binary if there are only two classes to predict, or multiclass if there are more than two classes."
  },
  {
    "objectID": "machine_learning_egei/class4.html#examples-of-classification",
    "href": "machine_learning_egei/class4.html#examples-of-classification",
    "title": "Classification",
    "section": "Examples of Classification",
    "text": "Examples of Classification\n\nEmail spam filter: Classifying emails as ‘Spam’ or ‘Not Spam’.\nMedical diagnosis: Determining if a patient has a disease or not based on their medical records.\nCredit scoring: Assessing if an applicant is a ‘high’ or ‘low’ credit risk.\n\nClassification is used in various domains such as finance, healthcare, marketing, and more, making it a fundamental technique in the field of data science and machine learning."
  },
  {
    "objectID": "machine_learning_egei/class4.html#how-logistic-regression-works",
    "href": "machine_learning_egei/class4.html#how-logistic-regression-works",
    "title": "Classification",
    "section": "How Logistic Regression Works",
    "text": "How Logistic Regression Works\n\nLogistic regression measures the relationship between the categorical dependent variable and one or more independent variables by estimating probabilities using a logistic function.\nThis logistic function is an S-shaped curve that can take any real-valued number and map it into a value between 0 and 1, but never exactly at those limits.\nThe function is defined as: \\[\\frac{1}{1 + e^{-(\\beta_0 + \\beta_1x)}}\\] where $ _0 $ is the intercept and $ _1 $ are the coefficients of the independent variables $ x $."
  },
  {
    "objectID": "machine_learning_egei/class4.html#use-cases-for-logistic-regression",
    "href": "machine_learning_egei/class4.html#use-cases-for-logistic-regression",
    "title": "Classification",
    "section": "Use Cases for Logistic Regression",
    "text": "Use Cases for Logistic Regression\n\nPredicting the probability of a customer purchasing a product.\nEstimating the odds of a student being admitted to a college, based on their grades and test scores.\nDetermining whether a transaction is fraudulent or not.\n\n\n# Python example demonstrating logistic regression with a dataset\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, confusion_matrix\n\n# Load dataset\ndata = load_iris()\nX = data.data\ny = data.target\n\n# Since logistic regression is for binary classification, we will only use two classes\nX = X[y != 2]\ny = y[y != 2]\n\n# Split the data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize and train logistic regression model\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\n\n# Predictions\npredictions = model.predict(X_test)\n\n# Performance\nprint(confusion_matrix(y_test, predictions))\nprint(classification_report(y_test, predictions))"
  },
  {
    "objectID": "machine_learning_egei/class4.html#accuracy",
    "href": "machine_learning_egei/class4.html#accuracy",
    "title": "Classification",
    "section": "Accuracy",
    "text": "Accuracy\nAccuracy is the most intuitive performance measure and it is simply a ratio of correctly predicted observation to the total observations. It is a measure of how many classifications are correct. The formula for accuracy is:\n\\[\n\\text{Accuracy} = \\frac{\\text{Number of correct predictions}}{\\text{Total number of predictions made}}\n\\]"
  },
  {
    "objectID": "machine_learning_egei/class4.html#precision",
    "href": "machine_learning_egei/class4.html#precision",
    "title": "Classification",
    "section": "Precision",
    "text": "Precision\nPrecision is the ratio of correctly predicted positive observations to the total predicted positive observations. High precision relates to the low false positive rate. It is a measure of the quality of a positive prediction made by the model. The formula for precision is:\n\\[\n\\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Positives}}\n\\]"
  },
  {
    "objectID": "machine_learning_egei/class4.html#recall-sensitivity",
    "href": "machine_learning_egei/class4.html#recall-sensitivity",
    "title": "Classification",
    "section": "Recall (Sensitivity)",
    "text": "Recall (Sensitivity)\nRecall is the ratio of correctly predicted positive observations to the all observations in actual class - yes. It is a measure of the ability of a model to find all the relevant cases within a dataset. The formula for recall is:\n\\[\n\\text{Recall} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}}\n\\]"
  },
  {
    "objectID": "machine_learning_egei/class4.html#f1-score",
    "href": "machine_learning_egei/class4.html#f1-score",
    "title": "Classification",
    "section": "F1 Score",
    "text": "F1 Score\nThe F1 Score is the 2((precisionrecall)/(precision+recall)). It is also known as the F-Score or the F-Measure. The F1 Score is the harmonic mean of precision and recall taking both false positives and false negatives into account. It is a measure of the test’s accuracy. The formula for the F1 score is:\n\\[\n\\text{F1 Score} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n\\]\nIn the next Python example, we will calculate these metrics to better understand their interpretation.\n\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\n# Let's assume the following confusion matrix for a binary classifier\n# Confusion matrix\n#               Predicted\n#               No     Yes\n# Actual No      TN      FP\n#        Yes     FN      TP\n\n# True Positive (TP)\nTP = 30\n\n# True Negative (TN)\nTN = 45\n\n# False Positive (FP)\nFP = 5\n\n# False Negative (FN)\nFN = 20\n\n# Total number of predictions\ntotal_predictions = TP + TN + FP + FN\n\n# Calculating metrics\naccuracy = (TP + TN) / total_predictions\nprecision = TP / (TP + FP)\nrecall = TP / (TP + FN)\nf1 = 2 * (precision * recall) / (precision + recall)\n\nprint(f'Accuracy: {accuracy:.2f}')\nprint(f'Precision: {precision:.2f}')\nprint(f'Recall: {recall:.2f}')\nprint(f'F1 Score: {f1:.2f}')"
  },
  {
    "objectID": "machine_learning_egei/class4.html#when-to-use-precision-vs-recall",
    "href": "machine_learning_egei/class4.html#when-to-use-precision-vs-recall",
    "title": "Classification",
    "section": "When to Use Precision vs Recall",
    "text": "When to Use Precision vs Recall\n\nPrecision is used when the cost of a false positive is high. For example, in email spam detection, a false positive means that a regular email is incorrectly classified as spam. The consequence is that an important email might be missed if it’s sent to the spam folder.\nRecall is used when the cost of a false negative is high. For example, in fraud detection or disease screening, a false negative means that a fraudulent transaction or a disease is not identified. The consequence could be very serious, leading to financial loss or harm to health.\n\nIn the following Python example, we will create a confusion matrix for a hypothetical classifier and discuss the implications of precision and recall in a practical scenario.\n\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Hypothetical predictions and true labels\ny_true = [0, 1, 0, 1, 0, 1, 0, 0, 0, 1]\ny_pred = [0, 1, 0, 1, 0, 0, 1, 0, 0, 1]\n\n# Generating the confusion matrix\ncm = confusion_matrix(y_true, y_pred)\n\n# Plotting the confusion matrix\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.title('Confusion Matrix')\nplt.show()"
  },
  {
    "objectID": "machine_learning_egei/class4.html#exercise-1-calculating-precision-and-recall",
    "href": "machine_learning_egei/class4.html#exercise-1-calculating-precision-and-recall",
    "title": "Classification",
    "section": "Exercise 1: Calculating Precision and Recall",
    "text": "Exercise 1: Calculating Precision and Recall\nGiven a logistic regression model that predicts whether an email is spam or not, you have the following classification results on the test set:\n\nTrue Positives (TP): 90\nFalse Positives (FP): 10\nTrue Negatives (TN): 50\nFalse Negatives (FN): 30\n\nCalculate the precision and recall of the model.\n\nPrecision\nPrecision is the ratio of correctly predicted positive observations to the total predicted positives. It is a measure of a classifier’s exactness. High precision relates to a low false positive rate. Calculate the precision using the formula:\n\\[ Precision = \\frac{TP}{TP + FP} \\]\n\n\nRecall\nRecall (Sensitivity) is the ratio of correctly predicted positive observations to all observations in the actual class. It is a measure of a classifier’s completeness. High recall relates to a low false negative rate. Calculate the recall using the formula:\n\\[ Recall = \\frac{TP}{TP + FN} \\]\nWrite a Python function to calculate precision and recall, and then calculate these metrics using the given data."
  },
  {
    "objectID": "machine_learning_egei/class4.html#exercise-2-confusion-matrix",
    "href": "machine_learning_egei/class4.html#exercise-2-confusion-matrix",
    "title": "Classification",
    "section": "Exercise 2: Confusion Matrix",
    "text": "Exercise 2: Confusion Matrix\nUsing the same data from Exercise 1, create a confusion matrix. A confusion matrix is a table that is often used to describe the performance of a classification model on a set of test data for which the true values are known. It allows the visualization of the performance of an algorithm.\nCreate a Python function that takes in the values of TP, FP, TN, and FN and outputs a confusion matrix in a readable format."
  },
  {
    "objectID": "machine_learning_egei/class4.html#exercise-3-real-world-scenario",
    "href": "machine_learning_egei/class4.html#exercise-3-real-world-scenario",
    "title": "Classification",
    "section": "Exercise 3: Real-world Scenario",
    "text": "Exercise 3: Real-world Scenario\nConsider a logistic regression model that has been trained to detect fraud in credit card transactions. The model has produced the following results on a test dataset:\n\nTrue Positives (TP): 120\nFalse Positives (FP): 30\nTrue Negatives (TN): 900\nFalse Negatives (FN): 60\n\nCalculate the precision, recall, and F1-score for the model. The F1-score is the harmonic mean of precision and recall and is a balance between the two. It is calculated using the formula:\n\\[ F1 = 2 \\times \\frac{Precision \\times Recall}{Precision + Recall} \\]\nWrite a Python function to calculate the F1-score and apply it to the given data.\n\ndef calculate_precision_recall(TP, FP, FN):\n    precision = TP / (TP + FP)\n    recall = TP / (TP + FN)\n    return precision, recall\n\n# Given data\nTP = 90\nFP = 10\nFN = 30\n\n# Calculate precision and recall\nprecision, recall = calculate_precision_recall(TP, FP, FN)\n\nprint(f'Precision: {precision:.2f}')\nprint(f'Recall: {recall:.2f}')\n\n\ndef create_confusion_matrix(TP, FP, TN, FN):\n    confusion_matrix = {\n        'Predicted Positive': {'Actual Positive': TP, 'Actual Negative': FP},\n        'Predicted Negative': {'Actual Positive': FN, 'Actual Negative': TN}\n    }\n    return confusion_matrix\n\n# Given data\nTP = 90\nFP = 10\nTN = 50\nFN = 30\n\n# Create confusion matrix\nconfusion_matrix = create_confusion_matrix(TP, FP, TN, FN)\n\nfor predicted, actual in confusion_matrix.items():\n    print(f'{predicted}: {actual}')\n\n\ndef calculate_f1_score(TP, FP, TN, FN):\n    precision = TP / (TP + FP)\n    recall = TP / (TP + FN)\n    f1_score = 2 * (precision * recall) / (precision + recall)\n    return f1_score\n\n# Given data for a fraud detection model\nTP = 120\nFP = 30\nTN = 900\nFN = 60\n\n# Calculate F1-score\nf1_score = calculate_f1_score(TP, FP, TN, FN)\n\nprint(f'F1-Score: {f1_score:.2f}')"
  },
  {
    "objectID": "machine_learning_egei/class4.html#exercise-4-logistic-regression-implementation",
    "href": "machine_learning_egei/class4.html#exercise-4-logistic-regression-implementation",
    "title": "Classification",
    "section": "Exercise 4: Logistic Regression Implementation",
    "text": "Exercise 4: Logistic Regression Implementation\nIn this exercise, you will implement a logistic regression model on a dataset other than the commonly used Iris dataset. You will use the Pima Indians Diabetes Database, which is a standard dataset used in machine learning for binary classification problems. The dataset contains various diagnostic measurements and a binary outcome indicating whether the patient has diabetes.\nYour tasks are as follows:\n\nLoad the dataset.\nPerform any necessary preprocessing, such as handling missing values, feature scaling, etc.\nSplit the dataset into training and testing sets.\nImplement a logistic regression model.\nTrain the model on the training set.\nEvaluate the model on the test set using accuracy, precision, recall, and the confusion matrix.\nInterpret the results.\n\nYou will need to write Python code to accomplish these tasks. You can use libraries such as pandas for data manipulation, scikit-learn for logistic regression and evaluation metrics, and matplotlib or seaborn for data visualization if needed.\n\n!pip install -q pandas scikit-learn matplotlib seaborn\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Load the dataset\nurl = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv'\ncolumn_names = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome']\ndata = pd.read_csv(url, names=column_names)\n\n# Data preprocessing\n# Here you would handle missing values, feature scaling, etc.\n\n# Split the dataset\nX = data.drop('Outcome', axis=1)\ny = data['Outcome']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Implement the logistic regression model\nmodel = LogisticRegression()\n\n# Train the model\nmodel.fit(X_train, y_train)\n\n# Evaluate the model\ny_pred = model.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred)\nrecall = recall_score(y_test, y_pred)\nconf_matrix = confusion_matrix(y_test, y_pred)\n\n# Results\nprint(f'Accuracy: {accuracy:.2f}')\nprint(f'Precision: {precision:.2f}')\nprint(f'Recall: {recall:.2f}')\nprint('Confusion Matrix:')\nsns.heatmap(conf_matrix, annot=True, fmt='g')\nplt.show()"
  },
  {
    "objectID": "machine_learning_egei/class5.html",
    "href": "machine_learning_egei/class5.html",
    "title": "Decision Trees",
    "section": "",
    "text": "Decision trees are a type of supervised learning algorithm used for both classification and regression tasks. Their aim is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features."
  },
  {
    "objectID": "machine_learning_egei/class5.html#example-decision-tree-classification-in-python",
    "href": "machine_learning_egei/class5.html#example-decision-tree-classification-in-python",
    "title": "Decision Trees",
    "section": "Example: Decision Tree Classification in Python",
    "text": "Example: Decision Tree Classification in Python\nIn this example, we will use a dataset from scikit-learn, convert it to a DataFrame, describe it, and then apply a decision tree model for classification. After obtaining the results, we will analyze the precision, recall, accuracy, and ROC curve.\n\nimport pandas as pd\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import classification_report, roc_curve, auc\nimport matplotlib.pyplot as plt\n\n# Load and prepare the dataset\niris = load_iris()\niris_df = pd.DataFrame(iris.data, columns=iris.feature_names)\niris_df['target'] = iris.target\n\n# Display basic information about the dataset\niris_df.info()\niris_df.describe()\n\n# Split the dataset into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(iris_df[iris.feature_names], iris_df['target'], test_size=0.3, random_state=42)\n\n# Initialize and train the decision tree classifier\nclf = DecisionTreeClassifier(random_state=42)\nclf.fit(X_train, y_train)\n\n# Make predictions and evaluate the model\ny_pred = clf.predict(X_test)\nreport = classification_report(y_test, y_pred)\nprint('Classification Report:\\n', report)\n\n# Calculate ROC curve and AUC\nfpr, tpr, thresholds = roc_curve(y_test, clf.predict_proba(X_test)[:,1], pos_label=1)\nroc_auc = auc(fpr, tpr)\n\n# Plotting the ROC curve\nplt.figure()\nplt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic')\nplt.legend(loc='lower right')\nplt.show()"
  },
  {
    "objectID": "machine_learning_egei/class5.html#example-decision-tree-classification-with-a-different-dataset",
    "href": "machine_learning_egei/class5.html#example-decision-tree-classification-with-a-different-dataset",
    "title": "Decision Trees",
    "section": "Example: Decision Tree Classification with a Different Dataset",
    "text": "Example: Decision Tree Classification with a Different Dataset\nIn this example, we will use another dataset from scikit-learn, convert it to a DataFrame, describe it, and then apply a decision tree model for classification. We will follow the same steps as the previous example, including analyzing precision, recall, accuracy, and the ROC curve.\n\nimport pandas as pd\nfrom sklearn.datasets import load_wine\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import classification_report, roc_curve, auc\nimport matplotlib.pyplot as plt\n\n# Load and prepare the dataset\nwine = load_wine()\nwine_df = pd.DataFrame(wine.data, columns=wine.feature_names)\nwine_df['target'] = wine.target\n\n# Display basic information about the dataset\nwine_df.info()\nwine_df.describe()\n\n# Split the dataset into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(wine_df[wine.feature_names], wine_df['target'], test_size=0.3, random_state=42)\n\n# Initialize and train the decision tree classifier\nclf = DecisionTreeClassifier(random_state=42)\nclf.fit(X_train, y_train)\n\n# Make predictions and evaluate the model\ny_pred = clf.predict(X_test)\nreport = classification_report(y_test, y_pred)\nprint('Classification Report:\\n', report)\n\n# Calculate ROC curve and AUC for multi-class\nfpr = dict()\ntpr = dict()\nroc_auc = dict()\nfor i in range(wine.target_names.size):\n    fpr[i], tpr[i], _ = roc_curve(y_test, clf.predict_proba(X_test)[:, i], pos_label=i)\n    roc_auc[i] = auc(fpr[i], tpr[i])\n\n# Plotting the ROC curve for each class\nplt.figure()\nfor i in range(wine.target_names.size):\n    plt.plot(fpr[i], tpr[i], lw=2, label='ROC curve of class %d (area = %0.2f)' % (i, roc_auc[i]))\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic for each class')\nplt.legend(loc='lower right')\nplt.show()"
  },
  {
    "objectID": "machine_learning_egei/class5.html#random-forests",
    "href": "machine_learning_egei/class5.html#random-forests",
    "title": "Decision Trees",
    "section": "Random Forests",
    "text": "Random Forests\nRandom Forests are an ensemble learning method, primarily used for classification and regression. They operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees.\n\nCharacteristics of Random Forests:\n\nEnsemble Method: Random Forests combine multiple decision trees to improve the predictive performance and control over-fitting.\nRobustness: They are less prone to overfitting than individual decision trees.\nHandling of Unbalanced Data: Effective in handling unbalanced datasets by balancing error in the minority class.\nFeature Importance: They provide insights into the importance of each feature in making predictions.\n\n\n\nDifferences from Decision Trees:\n\nComplexity: Random Forests are generally more complex than single decision trees.\nPerformance: They often provide better accuracy due to the averaging of multiple trees.\nInterpretability: While individual trees are easy to interpret, the ensemble nature of Random Forests makes them more complex to interpret.\n\n\n\nMetrics Used in Random Forests:\n\nAccuracy: Measures the proportion of correct predictions.\nPrecision and Recall: Useful for evaluating class imbalance.\nF1 Score: Harmonic mean of precision and recall.\nAUC-ROC Curve: Measures the performance across all possible classification thresholds."
  },
  {
    "objectID": "machine_learning_egei/class5.html#random-forest-example-with-iris-dataset",
    "href": "machine_learning_egei/class5.html#random-forest-example-with-iris-dataset",
    "title": "Decision Trees",
    "section": "Random Forest Example with Iris Dataset",
    "text": "Random Forest Example with Iris Dataset\nFollowing the same steps as in the decision tree examples, we will now apply a Random Forest model to the Iris dataset. We will load the dataset, convert it to a DataFrame, describe it, apply the Random Forest model for classification, and analyze the precision, recall, accuracy, and ROC curve.\n\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Initialize and train the Random Forest classifier\nrf_clf = RandomForestClassifier(random_state=42)\nrf_clf.fit(X_train, y_train)\n\n# Make predictions and evaluate the model\ny_pred_rf = rf_clf.predict(X_test)\nrf_report = classification_report(y_test, y_pred_rf)\nprint('Random Forest Classification Report:\\n', rf_report)\n\n# Calculate ROC curve and AUC\nrf_fpr, rf_tpr, rf_thresholds = roc_curve(y_test, rf_clf.predict_proba(X_test)[:,1], pos_label=1)\nrf_roc_auc = auc(rf_fpr, rf_tpr)\n\n# Plotting the ROC curve\nplt.figure()\nplt.plot(rf_fpr, rf_tpr, color='darkorange', lw=2, label='Random Forest ROC curve (area = %0.2f)' % rf_roc_auc)\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Random Forest Receiver Operating Characteristic')\nplt.legend(loc='lower right')\nplt.show()"
  },
  {
    "objectID": "machine_learning_egei/class5.html#random-forest-example-with-wine-dataset",
    "href": "machine_learning_egei/class5.html#random-forest-example-with-wine-dataset",
    "title": "Decision Trees",
    "section": "Random Forest Example with Wine Dataset",
    "text": "Random Forest Example with Wine Dataset\nNow, we will apply the Random Forest model to the Wine dataset, following the same procedure as before. We will load the dataset, convert it to a DataFrame, describe it, apply the Random Forest model for classification, and analyze the precision, recall, accuracy, and ROC curve for each class.\n\n# Initialize and train the Random Forest classifier for the Wine dataset\nrf_clf_wine = RandomForestClassifier(random_state=42)\nrf_clf_wine.fit(X_train_wine, y_train_wine)\n\n# Make predictions and evaluate the model\ny_pred_rf_wine = rf_clf_wine.predict(X_test_wine)\nrf_report_wine = classification_report(y_test_wine, y_pred_rf_wine)\nprint('Random Forest Classification Report for Wine Dataset:\\n', rf_report_wine)\n\n# Calculate ROC curve and AUC for multi-class\nrf_fpr_wine = dict()\nrf_tpr_wine = dict()\nrf_roc_auc_wine = dict()\nfor i in range(wine.target_names.size):\n    rf_fpr_wine[i], rf_tpr_wine[i], _ = roc_curve(y_test_wine, rf_clf_wine.predict_proba(X_test_wine)[:, i], pos_label=i)\n    rf_roc_auc_wine[i] = auc(rf_fpr_wine[i], rf_tpr_wine[i])\n\n# Plotting the ROC curve for each class\nplt.figure()\nfor i in range(wine.target_names.size):\n    plt.plot(rf_fpr_wine[i], rf_tpr_wine[i], lw=2, label='Random Forest ROC curve of class %d (area = %0.2f)' % (i, rf_roc_auc_wine[i]))\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Random Forest Receiver Operating Characteristic for Wine Dataset')\nplt.legend(loc='lower right')\nplt.show()"
  },
  {
    "objectID": "machine_learning_egei/classes.html",
    "href": "machine_learning_egei/classes.html",
    "title": "Machine Learning with Business Application UTFSM",
    "section": "",
    "text": "Program"
  },
  {
    "objectID": "machine_learning_egei/classes.html#students-projects-and-presentations-2023",
    "href": "machine_learning_egei/classes.html#students-projects-and-presentations-2023",
    "title": "Machine Learning with Business Application UTFSM",
    "section": "Students’ projects and presentations (2023)",
    "text": "Students’ projects and presentations (2023)\nClustering\nRegression\nFinal project: Heart Failure prediction\nFinal project: Loan default prediction\nFinal project: Stroke_prediction\nFinal Scores"
  },
  {
    "objectID": "personal/academica.html",
    "href": "personal/academica.html",
    "title": "Academic Experience",
    "section": "",
    "text": "Bachelor of Mathematics and Mathematical Engineering (2009 - 2015)"
  },
  {
    "objectID": "personal/academica.html#universidad-de-santiago",
    "href": "personal/academica.html#universidad-de-santiago",
    "title": "Academic Experience",
    "section": "",
    "text": "Bachelor of Mathematics and Mathematical Engineering (2009 - 2015)"
  },
  {
    "objectID": "personal/academica.html#universidad-diego-portales",
    "href": "personal/academica.html#universidad-diego-portales",
    "title": "Academic Experience",
    "section": "Universidad Diego Portales",
    "text": "Universidad Diego Portales\n\nLecturer for Undergraduate Courses in Mathematics (2015 - 2020)\nCoordinator of the Mathematics Department (2015 - 2020)\nDiploma in University Teaching (2019 - 2020)"
  },
  {
    "objectID": "personal/academica.html#universidad-del-desarrollo",
    "href": "personal/academica.html#universidad-del-desarrollo",
    "title": "Academic Experience",
    "section": "Universidad del Desarrollo",
    "text": "Universidad del Desarrollo\n\nMaster’s in Data Science (2020 - 2021)\nOnline Course in Big Data (2022) - Offered by e-class"
  },
  {
    "objectID": "personal/academica.html#universidad-técnica-federico-santa-maría",
    "href": "personal/academica.html#universidad-técnica-federico-santa-maría",
    "title": "Academic Experience",
    "section": "Universidad Técnica Federico Santa María",
    "text": "Universidad Técnica Federico Santa María\n\nLecturer for the course: Machine Learning with Business Applications (2022 - 2023) - Linked to the Erasmus Mundus Master’s program in Europe"
  },
  {
    "objectID": "personal/academica.html#universidad-de-santiago-1",
    "href": "personal/academica.html#universidad-de-santiago-1",
    "title": "Academic Experience",
    "section": "Universidad de Santiago",
    "text": "Universidad de Santiago\n\nLecturer for the course: Data Science for Mathematical Engineers (First semester, 2023)"
  },
  {
    "objectID": "personal/academica.html#desafio-latam-bootcamp",
    "href": "personal/academica.html#desafio-latam-bootcamp",
    "title": "Academic Experience",
    "section": "Desafio Latam Bootcamp",
    "text": "Desafio Latam Bootcamp\n\nInstructor for Data Science and Data Analyst courses (2023)"
  },
  {
    "objectID": "introduction_to_python/class4.html#control-structures-in-python",
    "href": "introduction_to_python/class4.html#control-structures-in-python",
    "title": "Class 4: Control Structures, functions and list comprehension",
    "section": "Control Structures in Python",
    "text": "Control Structures in Python\nControl structures in Python provide a way to direct the flow of the program’s execution. They allow the program to make decisions, repeat certain actions, or execute specific blocks of code based on conditions. The primary control structures in Python are:\n\nConditional Statements (if, elif, else): These statements allow the program to execute specific blocks of code based on whether a condition is true or false.\nif condition:\n    # code to execute if condition is true\nelif another_condition:\n    # code to execute if another_condition is true\nelse:\n    # code to execute if none of the above conditions are true\nLoops (for, while): Loops allow the program to repeat a block of code multiple times.\n\nfor loop: Iterates over a sequence (like a list or range) and executes the block of code for each item in the sequence.\nfor item in sequence:\n    # code to execute for each item\nwhile loop: Repeats a block of code as long as a condition is true.\nwhile condition:\n    # code to execute while condition is true\n\nControl Keywords (break, continue, pass):\n\nbreak: Exits the current loop prematurely.\ncontinue: Skips the rest of the current iteration and moves to the next iteration of the loop.\npass: A placeholder that does nothing; it’s used when a statement is syntactically required but no action is needed.\n\nfor item in sequence:\n    if condition:\n        break\n    elif another_condition:\n        continue\n    else:\n        pass\n\nUnderstanding and effectively using these control structures is fundamental to writing efficient and organized Python code.\n\n# Example of Conditional Statements\nx = 10\nif x &gt; 5:\n    print('x is greater than 5')\nelif x == 5:\n    print('x is equal to 5')\nelse:\n    print('x is less than 5')\n\n\n# Example of Loops\n\n# for loop\nprint('Example of for loop:')\nfor i in range(3):\n    print(f'Iteration {i}')\n\n# while loop\nprint('\\nExample of while loop:')\ncount = 0\nwhile count &lt; 3:\n    print(f'Count is {count}')\n    count += 1\n\n\n# Example of Control Keywords\n\n# break\nprint('Example of break:')\nfor i in range(5):\n    if i == 3:\n        break\n    print(i)\n\n# continue\nprint('\\nExample of continue:')\nfor i in range(5):\n    if i == 3:\n        continue\n    print(i)\n\n# pass\nprint('\\nExample of pass:')\nfor i in range(5):\n    if i == 3:\n        pass\n    print(i)\n\n\n# Complex example of 'if' statement\n\n# Determine if a year is a leap year\nyear = 2000\nif (year % 4 == 0 and year % 100 != 0) or (year % 400 == 0):\n    print(f'{year} is a leap year.')\nelse:\n    print(f'{year} is not a leap year.')\n\n\n# Complex example of 'for' loop\n\n# Calculate the factorial of a number using a for loop\nnum = 5\nfactorial = 1\nfor i in range(1, num + 1):\n    factorial *= i\nprint(f'Factorial of {num} is {factorial}')\n\n\n# Complex example of 'while' loop\n\n# Generate the Fibonacci sequence up to a certain number using a while loop\nn = 10\na, b = 0, 1\nprint('Fibonacci sequence up to', n, ':')\nwhile a &lt; n:\n    print(a, end=', ')\n    a, b = b, a+b\n\n\n# Complex example of 'break'\n\n# Find the first number in a list that is divisible by 7 and 5\nnumbers = list(range(1, 100))\nfor num in numbers:\n    if num % 7 == 0 and num % 5 == 0:\n        print(f'The first number divisible by 7 and 5 is: {num}')\n        break\n\n\n# Complex example of 'continue'\n\n# Print numbers from a list, but skip numbers that are divisible by 3\nnumbers = list(range(1, 15))\nfor num in numbers:\n    if num % 3 == 0:\n        continue\n    print(num, end=', ')\n\n\n# Complex example of 'pass'\n\n# Placeholder for future implementation of a function\ndef complex_function():\n    # TODO: Implement this function in the future\n    pass\n\nprint('Function defined, but not yet implemented.')"
  },
  {
    "objectID": "introduction_to_python/class4.html#functions-in-python",
    "href": "introduction_to_python/class4.html#functions-in-python",
    "title": "Class 4: Control Structures, functions and list comprehension",
    "section": "Functions in Python",
    "text": "Functions in Python\nIn Python, a function is a block of organized, reusable code that is used to perform a single, related action. Functions provide better modularity for your application and a high degree of code reusing. They are defined using the def keyword, followed by the function name and parentheses ().\nFunctions are essential in programming as they help you break down complex tasks into smaller, more manageable pieces. This not only makes the code more readable but also allows for code reuse.\nLet’s dive into some examples with different data types.\n\nFunction with Lists\n\ndef list_operations(input_list):\n    \"\"\"Perform some basic operations on a list.\"\"\"\n    # Append an element\n    input_list.append('new_element')\n    # Remove the first element\n    input_list.pop(0)\n    # Reverse the list\n    input_list.reverse()\n    return input_list\n\n# Test the function\nsample_list = [1, 2, 3, 4, 5]\nlist_operations(sample_list)\n\n\n\nFunction with Dictionaries\n\ndef dict_operations(input_dict):\n    \"\"\"Perform some basic operations on a dictionary.\"\"\"\n    # Add a new key-value pair\n    input_dict['new_key'] = 'new_value'\n    # Remove a key-value pair\n    del input_dict['key1']\n    # Get value of a key with default\n    value = input_dict.get('non_existent_key', 'default_value')\n    return input_dict, value\n\n# Test the function\nsample_dict = {'key1': 'value1', 'key2': 'value2'}\ndict_operations(sample_dict)\n\n\n\nFunction with numpy Arrays\n\nimport numpy as np\n\ndef numpy_operations(input_array):\n    \"\"\"Perform some basic operations on a numpy array.\"\"\"\n    # Square each element\n    squared_array = np.square(input_array)\n    # Calculate the mean\n    mean_value = np.mean(input_array)\n    # Reshape the array\n    reshaped_array = input_array.reshape(2, 2)\n    return squared_array, mean_value, reshaped_array\n\n# Test the function\nsample_array = np.array([1, 2, 3, 4])\nnumpy_operations(sample_array)\n\n\n\nFunction with pandas Series\n\nimport pandas as pd\n\ndef series_operations(input_series):\n    \"\"\"Perform some basic operations on a pandas Series.\"\"\"\n    # Add a new element\n    input_series['d'] = 4\n    # Remove an element\n    input_series.drop('a', inplace=True)\n    # Calculate the sum\n    sum_value = input_series.sum()\n    return input_series, sum_value\n\n# Test the function\nsample_series = pd.Series({'a': 1, 'b': 2, 'c': 3})\nseries_operations(sample_series)\n\n\n\nFunction with pandas DataFrame\n\ndef dataframe_operations(input_df):\n    \"\"\"Perform some basic operations on a pandas DataFrame.\"\"\"\n    # Add a new column\n    input_df['column3'] = [7, 8, 9]\n    # Drop a column\n    input_df.drop('column1', axis=1, inplace=True)\n    # Calculate the mean of column2\n    mean_value = input_df['column2'].mean()\n    return input_df, mean_value\n\n# Test the function\nsample_df = pd.DataFrame({'column1': [1, 2, 3], 'column2': [4, 5, 6]})\ndataframe_operations(sample_df)"
  },
  {
    "objectID": "introduction_to_python/class4.html#list-comprehension",
    "href": "introduction_to_python/class4.html#list-comprehension",
    "title": "Class 4: Control Structures, functions and list comprehension",
    "section": "List Comprehension",
    "text": "List Comprehension\nList comprehension is a concise way to create lists in Python. It offers a shorter syntax when you want to create a new list based on the values of an existing list or iterable. The primary motivation behind using list comprehension is to make code more readable and expressive.\nThe basic syntax is:\n[expression for item in iterable if condition]\n\nexpression is the current item in the iteration, but it is also the outcome, which can be manipulated before it ends up like a list item in the new list.\nitem is the current iteration/item.\niterable is any object that can return its elements one at a time, such as a list, a tuple, a string, etc.\ncondition is like a filter that only accepts the items that evaluate as True.\n\nList comprehensions are related to control structures, particularly loops and conditionals. In essence, a list comprehension combines a for loop and an if statement to create a new list. The for loop iterates over each element in the iterable, and the if statement filters out the unwanted elements.\nFor example, if you want to create a list of squares for even numbers from 0 to 9, you can use a list comprehension with a conditional statement to filter out odd numbers.\n\n# Using list comprehension to get squares of even numbers from 0 to 9\nsquares_with_comprehension = [x**2 for x in range(10) if x % 2 == 0]\n\n# Without using list comprehension\nsquares_without_comprehension = []\nfor x in range(10):\n    if x % 2 == 0:\n        squares_without_comprehension.append(x**2)\n\nsquares_with_comprehension, squares_without_comprehension\n\n\n# Example 1: Get all vowels from a string\n\n# Using list comprehension\ninput_string = 'Hello, World!'\nvowels_with_comprehension = [char for char in input_string if char.lower() in 'aeiou']\n\n# Without using list comprehension\nvowels_without_comprehension = []\nfor char in input_string:\n    if char.lower() in 'aeiou':\n        vowels_without_comprehension.append(char)\n\nvowels_with_comprehension, vowels_without_comprehension\n\n\n# Example 2: Get numbers from a list that are greater than 5\n\n# Using list comprehension\nnumbers = [1, 3, 7, 9, 10, 12]\ngreater_than_five_with_comprehension = [num for num in numbers if num &gt; 5]\n\n# Without using list comprehension\ngreater_than_five_without_comprehension = []\nfor num in numbers:\n    if num &gt; 5:\n        greater_than_five_without_comprehension.append(num)\n\ngreater_than_five_with_comprehension, greater_than_five_without_comprehension\n\n\n# Example 3: Get even indices from a list\n\n# Using list comprehension\nitems = ['a', 'b', 'c', 'd', 'e', 'f']\neven_indices_with_comprehension = [items[i] for i in range(len(items)) if i % 2 == 0]\n\n# Without using list comprehension\neven_indices_without_comprehension = []\nfor i in range(len(items)):\n    if i % 2 == 0:\n        even_indices_without_comprehension.append(items[i])\n\neven_indices_with_comprehension, even_indices_without_comprehension"
  },
  {
    "objectID": "introduction_to_python/class4.html#exercises",
    "href": "introduction_to_python/class4.html#exercises",
    "title": "Class 4: Control Structures, functions and list comprehension",
    "section": "Exercises",
    "text": "Exercises\nTo test your understanding of the concepts covered in this notebook, try the following exercises:\n\nEasy\n\nControl Structures: Write a program that prints numbers from 1 to 10 using a for loop.\nFunctions: Create a function that takes a list of numbers and returns their average.\n\n\n\nMedium\n\nList Comprehension: Use list comprehension to create a list of squares for numbers from 1 to 20 that are divisible by 3.\nFunctions with Data Structures: Create a function that takes a dictionary and returns a list of keys whose values are strings.\n\n\n\nHard\n\nComplex Control Structures: Write a program that prints the Fibonacci sequence up to the 10th term using a while loop.\nAdvanced Functions: Create a function that takes a pandas DataFrame and returns a new DataFrame with only the numeric columns, and each value squared."
  },
  {
    "objectID": "introduction_to_python/class5.html#set-logic",
    "href": "introduction_to_python/class5.html#set-logic",
    "title": "Class 5: Data Manipulation I",
    "section": "Set Logic",
    "text": "Set Logic\nIn mathematics, a set is a collection of distinct objects, considered as an object in its own right. The operations associated with sets include:\n\nUnion: Represents all elements that are in either set. Denoted as \\(A \\cup B\\).\nIntersection: Represents all elements that are common to both sets. Denoted as \\(A \\cap B\\).\nDifference: Represents all elements that are in the first set but not in the second. Denoted as \\(A - B\\).\nComplement: Represents all elements that are not in the given set. Denoted as \\(\\overline{A}\\).\n\nThese operations form the basic foundation for set theory and are crucial when dealing with collections of data."
  },
  {
    "objectID": "introduction_to_python/class5.html#pandas-merge-function",
    "href": "introduction_to_python/class5.html#pandas-merge-function",
    "title": "Class 5: Data Manipulation I",
    "section": "Pandas Merge Function",
    "text": "Pandas Merge Function\nThe merge function in pandas is used to combine two DataFrames based on common columns or indices. It’s similar to SQL JOIN operations. The key parameters include:\n\nright: The DataFrame to merge with.\non: Column(s) that should be used to join the DataFrames. These columns should exist in both DataFrames.\nleft_on: Columns from the left DataFrame to use as keys.\nright_on: Columns from the right DataFrame to use as keys.\n\nLet’s create two example DataFrames to demonstrate the use of merge with right_on and left_on.\n\nimport pandas as pd\n\n# Creating two example DataFrames\nstudents = pd.DataFrame({\n    'student_id': [101, 102, 103],\n    'name': ['Alice', 'Bob', 'Charlie']\n})\n\ncourses = pd.DataFrame({\n    'registration_no': [101, 102, 104],\n    'course': ['Math', 'Physics', 'Chemistry']\n})\n\n# Merging the DataFrames using merge with right_on and left_on\nmerged_df = students.merge(courses, left_on='student_id', right_on='registration_no', how='inner')\n\nmerged_df\n\nUnable to display output for mime type(s): application/vnd.dataresource+json"
  },
  {
    "objectID": "introduction_to_python/class5.html#outer-join-vs-inner-join",
    "href": "introduction_to_python/class5.html#outer-join-vs-inner-join",
    "title": "Class 5: Data Manipulation I",
    "section": "Outer Join vs Inner Join",
    "text": "Outer Join vs Inner Join\nWhen merging or joining DataFrames in pandas, it’s essential to understand the type of join you want to perform. The two most common types are outer join and inner join:\n\nOuter Join: This type of join returns all the rows when there is a match in one of the DataFrames. Therefore, it returns all the rows from the left DataFrame and all the rows from the right DataFrame. If there is no match, the missing side will contain NaN.\nInner Join: This type of join returns only the rows where there is a match in both DataFrames. If there’s no match in one of the DataFrames, that row will not be in the result.\n\nTo illustrate the difference between these two types of joins, let’s consider an example. Suppose we have a DataFrame of authors and their books, and another DataFrame of books and their prices. We want to merge these DataFrames based on the book titles. Let’s first create and display these DataFrames.\n\n# Creating two example DataFrames\nauthors = pd.DataFrame({\n    'book_title': ['The Great Gatsby', 'Moby Dick', 'Pride and Prejudice'],\n    'author': ['F. Scott Fitzgerald', 'Herman Melville', 'Jane Austen']\n})\n\nprices = pd.DataFrame({\n    'book_title': ['The Great Gatsby', 'Moby Dick', 'War and Peace'],\n    'price': [10, 15, 20]\n})\n\nauthors, prices\n\n(            book_title               author\n 0     The Great Gatsby  F. Scott Fitzgerald\n 1            Moby Dick      Herman Melville\n 2  Pride and Prejudice          Jane Austen,\n          book_title  price\n 0  The Great Gatsby     10\n 1         Moby Dick     15\n 2     War and Peace     20)\n\n\n\n# Performing an outer join\nouter_join_result = authors.merge(prices, on='book_title', how='outer')\n\n# Performing an inner join\ninner_join_result = authors.merge(prices, on='book_title', how='inner')\n\nouter_join_result, inner_join_result\n\n(            book_title               author  price\n 0     The Great Gatsby  F. Scott Fitzgerald   10.0\n 1            Moby Dick      Herman Melville   15.0\n 2  Pride and Prejudice          Jane Austen    NaN\n 3        War and Peace                  NaN   20.0,\n          book_title               author  price\n 0  The Great Gatsby  F. Scott Fitzgerald     10\n 1         Moby Dick      Herman Melville     15)"
  },
  {
    "objectID": "introduction_to_python/class5.html#pivot-tables-transposition-and-lambda-functions",
    "href": "introduction_to_python/class5.html#pivot-tables-transposition-and-lambda-functions",
    "title": "Class 5: Data Manipulation I",
    "section": "Pivot Tables, Transposition, and Lambda Functions",
    "text": "Pivot Tables, Transposition, and Lambda Functions\nIn this section, we’ll delve into some advanced data manipulation techniques in pandas, including pivot tables, transposition, and the use of lambda functions.\n\nPivot Tables\nA pivot table is a data summarization tool used in spreadsheets and other data visualization software. It aggregates a table of data by one or more keys, arranging the data in a rectangle with some of the group keys along the rows and some along the columns. In pandas, the pivot_table function provides this functionality.\nThe primary parameters of the pivot_table function are:\n\nvalues: Column(s) to aggregate.\nindex: Column(s) to use as the row of the pivot table.\ncolumns: Column(s) to use as the columns of the pivot table.\naggfunc: Aggregation function to use on the data. It can be ‘sum’, ‘mean’, etc. or a custom function.\n\nLet’s look at two practical examples to understand the use of pivot tables.\n\n# Creating two example DataFrames for pivot_table demonstration\n\n# Example 1: Sales data\nsales_data = pd.DataFrame({\n    'Date': ['2022-01-01', '2022-01-01', '2022-01-02', '2022-01-02'],\n    'Product': ['A', 'B', 'A', 'B'],\n    'Sales': [100, 150, 110, 160]\n})\n\n# Pivot table to get total sales for each product across dates\npivot_sales = sales_data.pivot_table(values='Sales', index='Date', columns='Product', aggfunc='sum')\n\n# Example 2: Student grades\ngrades_data = pd.DataFrame({\n    'Student': ['Alice', 'Bob', 'Alice', 'Bob'],\n    'Subject': ['Math', 'Math', 'History', 'History'],\n    'Grade': [85, 90, 78, 88]\n})\n\n# Pivot table to get grades for each student across subjects\npivot_grades = grades_data.pivot_table(values='Grade', index='Student', columns='Subject')\n\npivot_sales, pivot_grades\n\n(Product       A    B\n Date                \n 2022-01-01  100  150\n 2022-01-02  110  160,\n Subject  History  Math\n Student               \n Alice         78    85\n Bob           88    90)\n\n\n\n\nTransposition\nTransposition is a fundamental operation in linear algebra and data manipulation. In the context of DataFrames, transposition refers to switching the rows and columns with each other. In pandas, you can transpose a DataFrame using the transpose() method or its shorthand .T.\nTransposing can be useful in various scenarios, such as when you want to change the orientation of your data for visualization or when you want to treat columns as rows for certain operations.\nLet’s look at an example to understand the use of transposition.\n\n# Example DataFrame for transposition\ndf_transpose = pd.DataFrame({\n    'Name': ['Alice', 'Bob', 'Charlie'],\n    'Age': [25, 30, 35],\n    'City': ['New York', 'London', 'Paris']\n})\n\n# Transposing the DataFrame\ntransposed_df = df_transpose.transpose()\n\ndf_transpose, transposed_df\n\n(      Name  Age      City\n 0    Alice   25  New York\n 1      Bob   30    London\n 2  Charlie   35     Paris,\n              0       1        2\n Name     Alice     Bob  Charlie\n Age         25      30       35\n City  New York  London    Paris)\n\n\n\n\nLambda Functions with Pivot Tables\nLambda functions are small, anonymous functions that can have any number of arguments, but can only have one expression. They are useful for performing simple operations without the need to define a full function. In the context of pivot_table, lambda functions can be used as custom aggregation functions.\nFor instance, you might want to aggregate data in a way that’s not directly supported by built-in functions. In such cases, a lambda function can be handy.\nLet’s look at an example where we use a lambda function with pivot_table to calculate the range (difference between max and min) of values.\n\n# Example DataFrame for lambda function with pivot_table\ndf_lambda = pd.DataFrame({\n    'Category': ['A', 'A', 'B', 'B', 'A', 'B'],\n    'Values': [10, 15, 20, 25, 30, 35]\n})\n\n# Using pivot_table with lambda function to calculate the range of values for each category\npivot_lambda = df_lambda.pivot_table(index='Category', values='Values', aggfunc=lambda x: x.max() - x.min())\n\npivot_lambda\n\nUnable to display output for mime type(s): application/vnd.dataresource+json"
  },
  {
    "objectID": "introduction_to_python/class5.html#data-cleaning-and-transformation",
    "href": "introduction_to_python/class5.html#data-cleaning-and-transformation",
    "title": "Class 5: Data Manipulation I",
    "section": "Data Cleaning and Transformation",
    "text": "Data Cleaning and Transformation\nOne of the essential steps in the data preprocessing pipeline is data cleaning and transformation. This step ensures that the data is of high quality and is ready for analysis. A common issue encountered during this phase is the presence of duplicate records.\n\nDuplicates in a DataFrame\nDuplicates refer to rows in a DataFrame that are identical to another row. These can arise due to various reasons such as data entry errors, merging datasets, or not handling data updates correctly. Duplicates can lead to incorrect analysis results, so it’s crucial to identify and handle them appropriately.\nIn pandas, you can use the drop_duplicates() method to remove duplicate rows and the duplicated() method to identify them. The subset parameter allows you to consider certain columns for identifying duplicates.\nLet’s dive into an entertaining example to understand the use of these functions.\n\n# Example DataFrame with duplicate records\ndf_duplicates = pd.DataFrame({\n    'Superhero': ['Spider-Man', 'Iron Man', 'Spider-Man', 'Thor', 'Iron Man', 'Hulk'],\n    'Real Name': ['Peter Parker', 'Tony Stark', 'Peter Parker', 'Thor Odinson', 'Tony Stark', 'Bruce Banner'],\n    'City': ['New York', 'New York', 'New York', 'Asgard', 'Los Angeles', 'New York']\n})\n\n# Identifying duplicate rows\nduplicated_rows = df_duplicates[df_duplicates.duplicated(subset=['Superhero', 'Real Name'])]\n\n# Removing duplicate rows\ndf_cleaned = df_duplicates.drop_duplicates(subset=['Superhero', 'Real Name'])\n\ndf_duplicates, duplicated_rows, df_cleaned\n\n(    Superhero     Real Name         City\n 0  Spider-Man  Peter Parker     New York\n 1    Iron Man    Tony Stark     New York\n 2  Spider-Man  Peter Parker     New York\n 3        Thor  Thor Odinson       Asgard\n 4    Iron Man    Tony Stark  Los Angeles\n 5        Hulk  Bruce Banner     New York,\n     Superhero     Real Name         City\n 2  Spider-Man  Peter Parker     New York\n 4    Iron Man    Tony Stark  Los Angeles,\n     Superhero     Real Name      City\n 0  Spider-Man  Peter Parker  New York\n 1    Iron Man    Tony Stark  New York\n 3        Thor  Thor Odinson    Asgard\n 5        Hulk  Bruce Banner  New York)\n\n\n\n\nOriginal DataFrame (Superhero Records)\nThis DataFrame contains records of superheroes, their real names, and cities. As you can observe, there are some duplicate entries in the data.\n\ndf_duplicates\n\n\n\nIdentified Duplicate Rows\nUsing the duplicated() function with the subset parameter, we can identify rows where both the ‘Superhero’ and ‘Real Name’ columns are duplicated. Here are the rows that have been identified as duplicates:\n\nduplicated_rows\n\n\n\nCleaned DataFrame (After Removing Duplicates)\nAfter identifying the duplicate rows, we can use the drop_duplicates() function with the subset parameter to remove these rows and obtain a cleaned DataFrame. Here’s the DataFrame after removing the duplicates:\n\ndf_cleaned"
  },
  {
    "objectID": "introduction_to_python/class5.html#missing-data",
    "href": "introduction_to_python/class5.html#missing-data",
    "title": "Class 5: Data Manipulation I",
    "section": "Missing Data",
    "text": "Missing Data\nIn the realm of data analysis, missing data, often represented as NaN (Not a Number), is a common occurrence. It can arise due to various reasons, such as data entry errors, unrecorded observations, or during data processing. Handling missing data is crucial as it can significantly impact the results of your analysis.\n\nIdentifying Missing Data\nBefore handling missing data, it’s essential to identify them in your dataset. In pandas, the isna() or isnull() methods can be used to detect missing values. These methods return a DataFrame of the same shape as the original, but with True where the data is missing and False where it’s not.\n\n# Example DataFrame with missing data\ndf_missing = pd.DataFrame({\n    'Name': ['Alice', 'Bob', 'Charlie', 'David'],\n    'Age': [25, np.nan, 35, 40],\n    'City': ['New York', 'Los Angeles', np.nan, 'Chicago']\n})\n\n# Identifying missing data\ndf_missing.isna()\n\n\n\nRemoving Missing Data\nOnce you’ve identified missing data, you might decide to remove them from your dataset. Pandas provides two main methods for this:\n\ndropna(): This method allows you to drop rows or columns containing missing data.\nfillna(): Instead of dropping missing data, you can also replace them with a specific value or a computed value (like mean, median, etc.).\n\n\n# Removing rows with missing data\ndf_no_missing_rows = df_missing.dropna()\n\n# Removing columns with missing data\ndf_no_missing_columns = df_missing.dropna(axis=1)\n\ndf_no_missing_rows, df_no_missing_columns\n\n\n\nFilling Missing Data\nInstead of removing missing data, another approach is to fill or replace them. The fillna() method in pandas allows you to replace missing values with a specific value, forward fill, backward fill, or even a computed value like the mean or median of the column.\n\n# Filling missing data with a specific value\ndf_filled_value = df_missing.fillna('Unknown')\n\n# Filling missing data with mean of the column\ndf_filled_mean = df_missing.copy()\ndf_filled_mean['Age'] = df_filled_mean['Age'].fillna(df_filled_mean['Age'].mean())\n\ndf_filled_value, df_filled_mean"
  },
  {
    "objectID": "introduction_to_python/class5.html#apply-function",
    "href": "introduction_to_python/class5.html#apply-function",
    "title": "Class 5: Data Manipulation I",
    "section": "Apply Function",
    "text": "Apply Function\nThe apply() function in pandas is a powerful tool that allows you to apply a function along the axis of a DataFrame (either rows or columns). It’s especially useful when you want to perform custom operations that are not readily available through built-in pandas functions.\nLet’s explore some entertaining examples to understand the versatility of the apply() function.\n\n# Example 1: Capitalizing names\ndf_names = pd.DataFrame({\n    'Name': ['alice', 'bob', 'charlie', 'david']\n})\n\ndf_names['Capitalized Name'] = df_names['Name'].apply(lambda x: x.capitalize())\ndf_names\n\n\n# Example 2: Calculating the length of strings\ndf_phrases = pd.DataFrame({\n    'Phrase': ['Hello World', 'Pandas is awesome', 'Apply function rocks!', 'Data Science']\n})\n\ndf_phrases['Length'] = df_phrases['Phrase'].apply(len)\ndf_phrases\n\n\n# Example 3: Categorizing based on age\ndf_age = pd.DataFrame({\n    'Name': ['Alice', 'Bob', 'Charlie', 'David'],\n    'Age': [15, 28, 35, 42]\n})\n\ndef age_category(age):\n    if age &lt; 18:\n        return 'Minor'\n    elif 18 &lt;= age &lt; 35:\n        return 'Young Adult'\n    else:\n        return 'Adult'\n\ndf_age['Category'] = df_age['Age'].apply(age_category)\ndf_age"
  },
  {
    "objectID": "introduction_to_python/class5.html#differences-between-apply-and-applymap",
    "href": "introduction_to_python/class5.html#differences-between-apply-and-applymap",
    "title": "Class 5: Data Manipulation I",
    "section": "Differences between apply and applymap",
    "text": "Differences between apply and applymap\nBoth apply and applymap are essential tools in pandas for element-wise operations, but they serve different purposes and are used in different contexts.\n\napply: This function is used on both Series and DataFrame. When used on a Series, it applies a function to each element. When used on a DataFrame, it can apply a function along the axis (either rows or columns).\napplymap: This function is exclusive to DataFrames and is used to apply a function to each element of the DataFrame.\n\nLet’s delve into some examples to understand when to use one over the other.\n\n# Example DataFrame\ndf_example = pd.DataFrame({\n    'A': [1, 2, 3],\n    'B': [4, 5, 6],\n    'C': [7, 8, 9]\n})\n\n# Using apply on a Series\nseries_squared = df_example['A'].apply(lambda x: x**2)\n\n# Using apply on a DataFrame to get the sum of each column\ncolumn_sum = df_example.apply(sum, axis=0)\n\n# Using applymap to square each element of the DataFrame\ndf_squared = df_example.applymap(lambda x: x**2)\n\n\nseries_squared\n\nUnable to display output for mime type(s): application/vnd.dataresource+json\n\n\n\ncolumn_sum\n\nUnable to display output for mime type(s): application/vnd.dataresource+json\n\n\n\ndf_squared\n\nUnable to display output for mime type(s): application/vnd.dataresource+json"
  },
  {
    "objectID": "introduction_to_python/class5.html#replacing-data",
    "href": "introduction_to_python/class5.html#replacing-data",
    "title": "Class 5: Data Manipulation I",
    "section": "Replacing Data",
    "text": "Replacing Data\nIn data manipulation, there are often scenarios where you need to replace certain values in your dataset. The replace() method in pandas is a versatile tool that allows you to replace values in a DataFrame or Series.\nLet’s start with a basic example of the replace() method.\n\n# Example DataFrame\ndf_replace = pd.DataFrame({\n    'Fruit': ['Apple', 'Banana', 'Cherry', 'Apple'],\n    'Color': ['Red', 'Yellow', 'Red', 'Green']\n})\n\n# Using replace to change 'Apple' to 'Mango'\ndf_replaced = df_replace.replace('Apple', 'Mango')\ndf_replaced"
  },
  {
    "objectID": "introduction_to_python/class5.html#regular-expressions-regex",
    "href": "introduction_to_python/class5.html#regular-expressions-regex",
    "title": "Class 5: Data Manipulation I",
    "section": "Regular Expressions (Regex)",
    "text": "Regular Expressions (Regex)\nRegular expressions, often abbreviated as regex, are sequences of characters that define a search pattern. They are incredibly powerful for string matching and manipulation. In pandas, you can use regex with the replace() method to perform more complex replacements.\nLet’s delve into the world of regex and explore some examples to understand its capabilities.\n\n# Example DataFrame with phone numbers\ndf_phone = pd.DataFrame({\n    'Name': ['Alice', 'Bob', 'Charlie'],\n    'Phone': ['123-456-7890', '(123) 456-7890', '123.456.7890']\n})\n\n# Using regex to standardize phone number format\ndf_phone_standardized = df_phone.copy()\ndf_phone_standardized['Phone'] = df_phone_standardized['Phone'].replace(r'\\D', '', regex=True)\ndf_phone_standardized['Phone'] = df_phone_standardized['Phone'].replace(r'(\\d{3})(\\d{3})(\\d{4})', r'\\1-\\2-\\3', regex=True)\n\n\ndf_phone\n\nUnable to display output for mime type(s): application/vnd.dataresource+json\n\n\n\ndf_phone_standardized\n\nUnable to display output for mime type(s): application/vnd.dataresource+json"
  },
  {
    "objectID": "introduction_to_python/class5.html#deep-dive-into-regular-expressions-regex",
    "href": "introduction_to_python/class5.html#deep-dive-into-regular-expressions-regex",
    "title": "Class 5: Data Manipulation I",
    "section": "Deep Dive into Regular Expressions (Regex)",
    "text": "Deep Dive into Regular Expressions (Regex)\nRegular expressions (regex) are a powerful tool for working with text data. They allow you to create search patterns using sequences of characters, which can be used for string matching and manipulation. The true power of regex lies in its flexibility and the wide range of patterns it can match.\nLet’s break down some of the fundamental components of regex and explore various examples to understand its capabilities.\n\nBasic Regex Patterns\n\n. : Matches any character except a newline.\n^ : Matches the start of a string.\n$ : Matches the end of a string.\n* : Matches 0 or more repetitions of the preceding character or group.\n+ : Matches 1 or more repetitions of the preceding character or group.\n? : Matches 0 or 1 repetition of the preceding character or group.\n\\d : Matches any decimal digit. Equivalent to [0-9].\n\\D : Matches any non-digit character.\n\\w : Matches any alphanumeric character or underscore. Equivalent to [a-zA-Z0-9_].\n\\W : Matches any non-alphanumeric character.\n\\s : Matches any whitespace character (spaces, tabs, line breaks).\n\\S : Matches any non-whitespace character.\n\nLet’s see some of these patterns in action with examples.\n\nimport re\n\n# Sample text\ntext = 'My phone number is 123-456-7890 and my zip code is 98765.'\n\n# Extracting phone number using regex\nphone_pattern = r'\\d{3}-\\d{3}-\\d{4}'\nphone_match = re.search(phone_pattern, text)\nphone_number = phone_match.group() if phone_match else None\n\n# Extracting zip code using regex\nzip_pattern = r'\\d{5}$'\nzip_match = re.search(zip_pattern, text)\nzip_code = zip_match.group() if zip_match else None\n\nphone_number, zip_code\n\nIn the example above, we used the following regex patterns:\n\n\\d{3}-\\d{3}-\\d{4}: This pattern matches a phone number format where there are three digits, followed by a hyphen, another three digits, another hyphen, and finally four digits.\n\\d{5}$: This pattern matches five digits at the end of a string, which is a common format for zip codes in the US.\n\nNow, let’s explore more advanced regex patterns and their applications.\n\n\nAdvanced Regex Patterns\n\nCharacter Sets [...]: Matches any one of the characters inside the square brackets. For example, [aeiou] matches any vowel.\nNegated Character Sets [^...]: Matches any character that is not inside the square brackets. For example, [^aeiou] matches any non-vowel character.\nQuantifiers {m,n}: Matches between m and n repetitions of the preceding character or group. For example, a{2,4} matches ‘aa’, ‘aaa’, or ‘aaaa’.\nNon-capturing Groups (?:...): Groups multiple patterns together without creating a capture group.\nPositive Lookahead (?=...): Asserts that what directly follows the current position in the string matches the pattern inside the lookahead, but doesn’t consume any characters.\nNegative Lookahead (?!...): Asserts that what directly follows the current position in the string does not match the pattern inside the lookahead.\n\nLet’s see some of these advanced patterns in action with examples.\n\n# Sample text for regex examples\ntext_advanced = 'The price is $100.00, but there's a discount of 10% if you pay within 5 days.'\n\n# Extracting all prices using regex (character sets)\nprice_pattern = r'\\$[0-9]+\\.[0-9]{2}'\nprices = re.findall(price_pattern, text_advanced)\n\n# Extracting words that don't start with a vowel (negated character sets)\nnon_vowel_pattern = r'\\b[^aeiouAEIOU \\d][a-zA-Z]*'\nnon_vowel_words = re.findall(non_vowel_pattern, text_advanced)\n\n# Using positive lookahead to find 'discount' if it's followed by '10%'\nlookahead_pattern = r'discount(?= of 10%)'\nlookahead_match = re.search(lookahead_pattern, text_advanced)\nlookahead_word = lookahead_match.group() if lookahead_match else None\n\nprices, non_vowel_words, lookahead_word\n\nIn the examples above, we utilized various advanced regex patterns:\n\nCharacter Sets: The pattern \\$[0-9]+\\.[0-9]{2} matches dollar amounts. It looks for a dollar sign, followed by one or more digits, a period, and exactly two digits after the period.\nNegated Character Sets: The pattern \\b[^aeiouAEIOU \\d][a-zA-Z]* matches words that don’t start with a vowel. It looks for word boundaries (\\b), followed by any character that is not a vowel or a digit, and then any sequence of alphabetic characters.\nPositive Lookahead: The pattern discount(?= of 10%) matches the word ‘discount’ only if it’s directly followed by ’ of 10%’. The positive lookahead (?=...) checks for the presence of a pattern without consuming any characters, allowing us to match based on what follows our main pattern.\n\nRegular expressions are a vast topic, and there’s a lot more to explore. The key is to practice and experiment with different patterns to become proficient."
  },
  {
    "objectID": "introduction_to_python/class5.html#exercises",
    "href": "introduction_to_python/class5.html#exercises",
    "title": "Class 5: Data Manipulation I",
    "section": "Exercises",
    "text": "Exercises\nNow that you’ve learned about various data manipulation techniques in pandas and regular expressions, it’s time to test your knowledge! Below are 10 exercises that cover the topics discussed in this notebook. Try to solve each one to reinforce your understanding.\n\nMerging DataFrames: Given two DataFrames, df1 with columns ['A', 'B'] and df2 with columns ['B', 'C'], merge them on column ‘B’.\nPivot Tables: Create a pivot table from a DataFrame df with columns ['Date', 'Product', 'Sales'] to show the total sales for each product over time.\nTranspose: Transpose a DataFrame df and explain what happens to its indices and columns.\nLambda Functions: Use a lambda function to square all the values in a DataFrame column ‘X’.\nRegex Phone Numbers: Extract all phone numbers from a text string. Consider phone numbers to be in the formats 123-456-7890, (123) 456-7890, and 123.456.7890.\nHandling Duplicates: Identify and remove any duplicate rows in a DataFrame df based on columns ‘A’ and ‘B’.\nHandling Missing Data: Replace all NaN values in a DataFrame df with the mean of the column they are in.\nApply Function: Apply a function that calculates the length of each string in the ‘Name’ column of a DataFrame df.\nReplace with Regex: Replace all occurrences of the word ‘color’ (case-insensitive) in a text string with the word ‘hue’ using regex.\nAdvanced Regex: Extract all email addresses from a text string. Consider email addresses to be in the format name@domain.com."
  },
  {
    "objectID": "introduction_to_python/classes.html",
    "href": "introduction_to_python/classes.html",
    "title": "Introduction to Python",
    "section": "",
    "text": "Class 1: Getting to know python | \nClass 2: Numpy Arrays | \nClass 3: Pandas Dataframe | \nClass 4: Control Structures, functions and list comprehension | \nClass 5: Data Manipulation I | \nClass 6: Data Manipulation II |"
  },
  {
    "objectID": "introduction_to_python/class1.html",
    "href": "introduction_to_python/class1.html",
    "title": "Class 1: Getting to Know Python",
    "section": "",
    "text": "Python is a high-level, interpreted programming language known for its readability and simplicity. It supports multiple programming paradigms, including procedural, object-oriented, and functional programming. Python is widely used in data analysis, web development, machine learning, and artificial intelligence. Its extensive library support and vibrant community make it a popular choice for both beginners and experienced developers.\nHere is a simple example of Python code that takes an input and produces an output:\n# Input\nname = input('What is your name? ')\n\n# Output\nprint(f'Hello, {name}!')"
  },
  {
    "objectID": "introduction_to_python/class1.html#data-types-in-python",
    "href": "introduction_to_python/class1.html#data-types-in-python",
    "title": "Class 1: Getting to Know Python",
    "section": "Data Types in Python",
    "text": "Data Types in Python\nPython has several built-in data types. Here are some of the most common ones:\n\nIntegers (int): Whole numbers, such as 3, 4, or 100.\nFloating point numbers (float): Numbers with a decimal point, such as 2.3 or 4.6.\nStrings (str): Sequences of characters, such as ‘hello’ or ‘python’.\nBooleans (bool): True or false values.\n\nLet’s see some examples:\n\n# Integer\nx = 10\nprint(type(x))\n\n# Float\ny = 3.14\nprint(type(y))\n\n# String\nz = 'Hello, Python!'\nprint(type(z))\n\n# Boolean\nw = True\nprint(type(w))"
  },
  {
    "objectID": "introduction_to_python/class1.html#lists-in-python",
    "href": "introduction_to_python/class1.html#lists-in-python",
    "title": "Class 1: Getting to Know Python",
    "section": "Lists in Python",
    "text": "Lists in Python\nA list in Python is an ordered collection of items that can be of any type. Lists are very flexible and can be modified after they are created (they are mutable).\nHere is an example of a list:\n\n# List of integers\nnumbers = [1, 2, 3, 4, 5]\nprint(numbers)\n\n# List of different types\nmixed = [1, 'two', 3.0, True]\nprint(mixed)"
  },
  {
    "objectID": "introduction_to_python/class1.html#dictionaries-in-python",
    "href": "introduction_to_python/class1.html#dictionaries-in-python",
    "title": "Class 1: Getting to Know Python",
    "section": "Dictionaries in Python",
    "text": "Dictionaries in Python\nA dictionary in Python is an unordered collection of items. Each item in a dictionary has a key and a corresponding value. Dictionaries are mutable, meaning they can be changed after they are created.\nHere is an example of a dictionary:\n\n# Dictionary\nperson = {\n    'name': 'John',\n    'age': 30,\n    'is_married': True\n}\nprint(person)"
  },
  {
    "objectID": "introduction_to_python/class1.html#comparing-lists-and-dictionaries",
    "href": "introduction_to_python/class1.html#comparing-lists-and-dictionaries",
    "title": "Class 1: Getting to Know Python",
    "section": "Comparing Lists and Dictionaries",
    "text": "Comparing Lists and Dictionaries\nLists and dictionaries are both versatile data structures in Python, but they are used in different scenarios.\nLists are ordered collections of items, and they are best when the order of items matters. You can use lists when you have a collection of items that you want to keep in a specific order, or when you want to easily add or remove items from the collection.\nDictionaries are unordered collections of key-value pairs, and they are best when you need to associate values with keys, so you can look them up efficiently by key. Dictionaries are ideal for data that is labeled, where each item can be accessed by a unique key (like a product code or name).\nLet’s see some examples:\n\n# List example\nfruits = ['apple', 'banana', 'cherry']\nprint(fruits)\n\n# Add an item to the list\nfruits.append('date')\nprint(fruits)\n\n# Dictionary example\nfruit_colors = {\n    'apple': 'red',\n    'banana': 'yellow',\n    'cherry': 'red'\n}\nprint(fruit_colors)\n\n# Add an item to the dictionary\nfruit_colors['date'] = 'brown'\nprint(fruit_colors)\n\nIt is possible to convert a list to a dictionary and vice versa, but the way you do it depends on the structure of your data.\nFor example, if you have a list of pairs, you can convert it to a dictionary using the dict() function. And if you have a dictionary, you can convert it to a list of pairs using the items() method.\nLet’s see some examples:\n\n# List of pairs\npairs = [('one', 1), ('two', 2), ('three', 3)]\n\n# Convert list to dictionary\ndict_from_list = dict(pairs)\nprint(dict_from_list)\n\n# Dictionary\ndictionary = {'one': 1, 'two': 2, 'three': 3}\n\n# Convert dictionary to list\nlist_from_dict = list(dictionary.items())\nprint(list_from_dict)"
  },
  {
    "objectID": "introduction_to_python/class1.html#operations-and-methods-in-python",
    "href": "introduction_to_python/class1.html#operations-and-methods-in-python",
    "title": "Class 1: Getting to Know Python",
    "section": "Operations and Methods in Python",
    "text": "Operations and Methods in Python\nIn Python, an operation is an action that is carried out on one or more values. For example, the + operation adds two numbers together, and the * operation multiplies them.\nA method is a function that is associated with a particular type of object. For example, the count method can be used on a list to count the number of times a particular value appears in the list, and the upper method can be used on a string to convert all the characters to uppercase.\nThe len function is a built-in Python function that returns the length of a sequence, such as a list or a string.\nLet’s see some examples:\n\n# Operations\nprint(3 + 4)  # Addition\nprint(3 * 4)  # Multiplication\n\n# Methods\nnumbers = [1, 2, 3, 2, 1]\nprint(numbers.count(2))  # Count method\n\ntext = 'hello'\nprint(text.upper())  # Upper method\n\n# Len function\nprint(len(numbers))  # Length of a list\nprint(len(text))  # Length of a string"
  },
  {
    "objectID": "introduction_to_python/class1.html#combining-different-data-types-with-and",
    "href": "introduction_to_python/class1.html#combining-different-data-types-with-and",
    "title": "Class 1: Getting to Know Python",
    "section": "Combining Different Data Types with + and *",
    "text": "Combining Different Data Types with + and *\nIn Python, the + and * operators can be used with different types of data, but the behavior can be different depending on the types of the operands.\nWhen used with integers (int), the + operator performs addition and the * operator performs multiplication.\nWhen used with strings (str), the + operator concatenates the strings, and the * operator repeats the string a certain number of times.\nHowever, trying to use + or * with an integer and a string will result in a TypeError.\nLet’s see some examples:\n\n# Addition with integers\nprint(3 + 4)\n\n# Multiplication with integers\nprint(3 * 4)\n\n# Concatenation with strings\nprint('hello' + 'world')\n\n# Repetition with strings\nprint('hello' * 3)\n\n# Trying to add an integer and a string\ntry:\n    print(3 + 'hello')\nexcept TypeError as e:\n    print(e)\n\n# Trying to multiply an integer and a string\ntry:\n    print(3 * 'hello')\nexcept TypeError as e:\n    print(e)\n\n7\n12\nhelloworld\nhellohellohello\nunsupported operand type(s) for +: 'int' and 'str'\nhellohellohello"
  },
  {
    "objectID": "introduction_to_python/class1.html#libraries-in-python",
    "href": "introduction_to_python/class1.html#libraries-in-python",
    "title": "Class 1: Getting to Know Python",
    "section": "Libraries in Python",
    "text": "Libraries in Python\nIn Python, a library is a collection of modules, which are files containing Python code that define functions, classes, and variables that you can use once the module is imported.\nPython comes with a standard library that includes many useful modules, and there are also many third-party libraries available that provide additional functionality.\nYou can import a module using the import statement. Once a module is imported, you can use its functions and variables by prefixing them with the module name and a dot.\nIf a library is not installed, you can install it using the pip tool, which is the package installer for Python.\nLet’s see some examples:\n\n# Importing a module from the standard library\nimport math\n\n# Using a function from the math module\nprint(math.sqrt(16))\n\n# Note: The following code is commented out because it requires user interaction and might not work in this notebook interface.\n# But you can run it in your local Python environment.\n\n# Installing a third-party library (uncomment to run)\n# !pip install numpy\n\n4.0\n\n\nIf you only need a specific function from a module, you can import just that function using the from ... import ... statement. This allows you to use the function directly without prefixing it with the module name.\nLet’s see an example:\n\n# Importing a specific function from the math module\nfrom math import sqrt\n\n# Using the function directly\nprint(sqrt(16))\n\n4.0"
  },
  {
    "objectID": "introduction_to_python/class1.html#google-colab",
    "href": "introduction_to_python/class1.html#google-colab",
    "title": "Class 1: Getting to Know Python",
    "section": "Google Colab",
    "text": "Google Colab\nGoogle Colab is a cloud-based Python development environment that provides a platform for anyone to write and execute Python code through the browser. It is especially useful for machine learning, data analysis, and education.\n\nAdvantages\n\nNo setup required\nFree access to GPUs\nEasy sharing\n\nWhether you’re a student, a data scientist, or an AI researcher, Colab can make your work easier. You can write and execute code, save and share your analyses, and access powerful computing resources, all for free from your browser.\n\n\nDisadvantages\n\nInternet connection is required\nLimited resources\n\nDespite its advantages, Google Colab does have some limitations. It requires an internet connection to save and run notebooks, and the available resources (RAM and disk space) are limited.\nTo start a new notebook in Google Colab, you can follow this link."
  },
  {
    "objectID": "introduction_to_python/class1.html#exercises",
    "href": "introduction_to_python/class1.html#exercises",
    "title": "Class 1: Getting to Know Python",
    "section": "Exercises",
    "text": "Exercises\n\nData Types: Write a Python function that takes two inputs, determines their data types, and returns a message indicating the data type of each input.\nList Operations: Given a list of numbers, write a Python function that returns a new list containing the square of each number. Use a loop in your solution.\nDictionary Operations: Write a Python function that takes a dictionary and a key as input, checks if the key is in the dictionary, and returns a message indicating whether the key was found.\nString Methods: Write a Python function that takes a string as input and returns a new string with the first letter of each word capitalized.\nLibrary Usage: Use the math library to write a Python function that takes a number as input and returns the square root of the number."
  },
  {
    "objectID": "introduction_to_python/class3.html",
    "href": "introduction_to_python/class3.html",
    "title": "Dataframes in Python",
    "section": "",
    "text": "A DataFrame is a two-dimensional labeled data structure with columns of potentially different types. It is generally the most commonly used pandas object. You can think of it like a spreadsheet or SQL table, or a dictionary of Series objects. It is generally the most commonly used pandas object.\nDataFrames can be created in various ways, but for this example, we’ll create a DataFrame from a dictionary of pandas Series.\nimport pandas as pd\n\ndata = {\n    'apples': pd.Series([3, 2, 0, 1]),\n    'oranges': pd.Series([0, 3, 7, 2])\n}\n\ndf = pd.DataFrame(data)\n\ndf\n\nUnable to display output for mime type(s): application/vnd.dataresource+json"
  },
  {
    "objectID": "introduction_to_python/class3.html#dataframe-indexing",
    "href": "introduction_to_python/class3.html#dataframe-indexing",
    "title": "Dataframes in Python",
    "section": "DataFrame Indexing",
    "text": "DataFrame Indexing\nIndexing in pandas means simply selecting particular rows and columns of data from a DataFrame. Indexes can be used to select specific rows and columns that you want to manipulate. They can also be used to modify the structure of the DataFrame itself, for example, by adding rows or columns.\nLet’s explore some examples of how to work with DataFrame indexes.\n\n# Set a column as the index\ndf_indexed = df.set_index('apples')\ndf_indexed\n\nUnable to display output for mime type(s): application/vnd.dataresource+json\n\n\n\n# Reset the index\ndf_reset = df_indexed.reset_index()\ndf_reset\n\nUnable to display output for mime type(s): application/vnd.dataresource+json"
  },
  {
    "objectID": "introduction_to_python/class3.html#creating-dataframes",
    "href": "introduction_to_python/class3.html#creating-dataframes",
    "title": "Dataframes in Python",
    "section": "Creating DataFrames",
    "text": "Creating DataFrames\nPandas DataFrames can be created in various ways. Some of the most common methods are: from a list, from a dictionary, from a list of dictionaries, and from a NumPy array. Let’s explore examples of each.\n\n# Creating a DataFrame from a list\nlist_data = [['Alex',10],['Bob',12],['Clarke',13]]\ndf_list = pd.DataFrame(list_data, columns=['Name','Age'])\ndf_list\n\nUnable to display output for mime type(s): application/vnd.dataresource+json\n\n\n\n# Creating a DataFrame from a dictionary\ndict_data = {'Name':['Tom', 'Nick', 'John'], 'Age':[20, 21, 19]}\ndf_dict = pd.DataFrame(dict_data)\ndf_dict\n\nUnable to display output for mime type(s): application/vnd.dataresource+json\n\n\n\n# Creating a DataFrame from a list of dictionaries\nlist_dict_data = [{'a': 1, 'b': 2},{'a': 3, 'b': 4, 'c': 5}]\ndf_list_dict = pd.DataFrame(list_dict_data)\ndf_list_dict\n\nUnable to display output for mime type(s): application/vnd.dataresource+json\n\n\n\n# Creating a DataFrame from a NumPy array\nimport numpy as np\n\nnumpy_data = np.array([[1, 2], [3, 4]])\ndf_numpy = pd.DataFrame(numpy_data, columns=['Column1', 'Column2'])\ndf_numpy\n\nUnable to display output for mime type(s): application/vnd.dataresource+json"
  },
  {
    "objectID": "introduction_to_python/class3.html#dataframe-operations-and-methods",
    "href": "introduction_to_python/class3.html#dataframe-operations-and-methods",
    "title": "Dataframes in Python",
    "section": "DataFrame Operations and Methods",
    "text": "DataFrame Operations and Methods\nPandas DataFrames offer a wide range of operations and methods that can be used to manipulate and analyze data. In this section, we’ll explore how to create new columns, how to create columns from other columns through operations, and how to combine DataFrames using the concatenate method.\n\n# Creating a new column\ndf['bananas'] = pd.Series([1, 0, 2, 4])\ndf\n\nUnable to display output for mime type(s): application/vnd.dataresource+json\n\n\n\n# Creating a column from other columns\ndf['total_fruits'] = df['apples'] + df['oranges'] + df['bananas']\ndf\n\nUnable to display output for mime type(s): application/vnd.dataresource+json\n\n\n\n# Creating another DataFrame to concatenate\ndf2 = pd.DataFrame({'apples': [5, 3], 'oranges': [2, 4], 'bananas': [7, 6]}, index=[4, 5])\n\n# Concatenating DataFrames\ndf_concat = pd.concat([df, df2])\ndf_concat\n\nUnable to display output for mime type(s): application/vnd.dataresource+json\n\n\n\nThe ignore_index Parameter\nWhen concatenating DataFrames, pandas provides an ignore_index parameter. If ignore_index is set to True, the resulting DataFrame will have a new integer index, ignoring the original indices of the concatenated DataFrames. Let’s see an example.\n\n# Concatenating DataFrames with ignore_index\ndf_concat_ignore_index = pd.concat([df, df2], ignore_index=True)\ndf_concat_ignore_index\n\nUnable to display output for mime type(s): application/vnd.dataresource+json"
  },
  {
    "objectID": "introduction_to_python/class3.html#exploring-a-dataframe",
    "href": "introduction_to_python/class3.html#exploring-a-dataframe",
    "title": "Dataframes in Python",
    "section": "Exploring a DataFrame",
    "text": "Exploring a DataFrame\nPandas provides several methods that are useful for quickly summarizing and gaining insights from your data. In this section, we’ll explore the value_counts, unique, nunique, and describe methods. Let’s first create a new DataFrame for these examples.\n\n# Creating a new DataFrame\ndata = {\n    'A': np.random.randint(1, 10, 20),\n    'B': np.random.choice(['red', 'green', 'blue'], 20),\n    'C': np.random.normal(0, 1, 20),\n    'D': np.random.choice(['cat', 'dog', 'rabbit'], 20),\n    'E': np.random.randint(1, 100, 20)\n}\ndf_explore = pd.DataFrame(data)\ndf_explore\n\nUnable to display output for mime type(s): application/vnd.dataresource+json\n\n\n\n# value_counts method\ndf_explore['B'].value_counts()\n\nUnable to display output for mime type(s): application/vnd.dataresource+json\n\n\n\n# unique method\ndf_explore['D'].unique()\n\narray(['rabbit', 'dog', 'cat'], dtype=object)\n\n\n\n# nunique method\ndf_explore['A'].nunique()\n\n9\n\n\n\n# describe method\ndf_explore.describe()\n\nUnable to display output for mime type(s): application/vnd.dataresource+json"
  },
  {
    "objectID": "introduction_to_python/class3.html#sorting-and-ranking-in-a-dataframe",
    "href": "introduction_to_python/class3.html#sorting-and-ranking-in-a-dataframe",
    "title": "Dataframes in Python",
    "section": "Sorting and Ranking in a DataFrame",
    "text": "Sorting and Ranking in a DataFrame\nPandas provides several methods for sorting and ranking data in a DataFrame. In this section, we’ll explore the sort_values, iloc, and loc methods. The sort_values method sorts a DataFrame by one or more columns, while iloc and loc are used for indexing and selecting data.\n\n# sort_values method\ndf_explore_sorted = df_explore.sort_values(by='A')\ndf_explore_sorted.head()\n\nUnable to display output for mime type(s): application/vnd.dataresource+json\n\n\n\n# iloc method\ndf_explore_iloc = df_explore_sorted.iloc[0:5]\ndf_explore_iloc\n\nUnable to display output for mime type(s): application/vnd.dataresource+json\n\n\n\n# loc method\ndf_explore_loc = df_explore_sorted.loc[df_explore_sorted['B'] == 'red']\ndf_explore_loc.head()\n\nUnable to display output for mime type(s): application/vnd.dataresource+json"
  },
  {
    "objectID": "introduction_to_python/class3.html#boolean-indexing",
    "href": "introduction_to_python/class3.html#boolean-indexing",
    "title": "Dataframes in Python",
    "section": "Boolean Indexing",
    "text": "Boolean Indexing\nBoolean indexing is a powerful tool that allows you to select data that meets certain conditions. This can be done using comparison operators (&gt;, &lt;, ==) and logical operators (& for ‘and’, | for ‘or’). Let’s see some examples.\n\n# Boolean indexing with '&gt;'\ndf_explore[df_explore['A'] &gt; 5]\n\nUnable to display output for mime type(s): application/vnd.dataresource+json\n\n\n\n# Boolean indexing with '&lt;'\ndf_explore[df_explore['A'] &lt; 5]\n\nUnable to display output for mime type(s): application/vnd.dataresource+json\n\n\n\n# Boolean indexing with '=='\ndf_explore[df_explore['B'] == 'red']\n\nUnable to display output for mime type(s): application/vnd.dataresource+json\n\n\n\n# Boolean indexing with '&' (and)\ndf_explore[(df_explore['A'] &gt; 5) & (df_explore['B'] == 'red')]\n\nUnable to display output for mime type(s): application/vnd.dataresource+json\n\n\n\n# Boolean indexing with '|' (or)\ndf_explore[(df_explore['A'] &lt; 5) | (df_explore['B'] == 'red')]\n\nUnable to display output for mime type(s): application/vnd.dataresource+json"
  },
  {
    "objectID": "introduction_to_python/class3.html#exporting-dataframes",
    "href": "introduction_to_python/class3.html#exporting-dataframes",
    "title": "Dataframes in Python",
    "section": "Exporting DataFrames",
    "text": "Exporting DataFrames\nPandas provides several methods to export a DataFrame to different file formats. This can be very useful when you want to save your data for later use or to share it with others. In this section, we’ll explore how to export a DataFrame to CSV, Excel, and JSON formats.\n\n# Exporting to CSV\n# df_explore.to_csv('df_explore.csv', index=False)\n\n\n# Exporting to Excel\n# df_explore.to_excel('df_explore.xlsx', index=False)\n\n\n# Exporting to JSON\n# df_explore.to_json('df_explore.json', orient='records')"
  },
  {
    "objectID": "introduction_to_python/class3.html#exercises",
    "href": "introduction_to_python/class3.html#exercises",
    "title": "Dataframes in Python",
    "section": "Exercises",
    "text": "Exercises\nNow that we have learned about pandas DataFrames, let’s put our knowledge into practice with some exercises. These exercises cover all the topics we have discussed in this notebook and vary in difficulty from easy to hard.\n\nExercise 1 (Easy)\nCreate a DataFrame from a dictionary with keys ‘Name’, ‘Age’, and ‘City’. The ‘Name’ column should contain five different names, the ‘Age’ column should contain ages between 20 and 40, and the ‘City’ column should contain the names of five different cities.\n\n\nExercise 2 (Medium)\nGiven the DataFrame created in Exercise 1, perform the following operations:\n\nSet the ‘Name’ column as the index of the DataFrame.\nReset the index of the DataFrame.\nSelect only the rows where ‘Age’ is greater than 30.\n\n\n\nExercise 3 (Medium)\nCreate a DataFrame with 10 rows and 3 columns named ‘A’, ‘B’, and ‘C’. The ‘A’ column should contain random integers between 1 and 10, the ‘B’ column should contain random floats between 0 and 1, and the ‘C’ column should contain the string ‘random’ for all rows. Then, export this DataFrame to a CSV file named ‘random.csv’.\n\n\nExercise 4 (Hard)\nGiven the DataFrame created in Exercise 3, perform the following operations:\n\nCreate a new column ‘D’ that is the result of ‘A’ divided by ‘B’.\nReplace all ‘inf’ values in column ‘D’ with ‘NaN’.\nDrop all rows with ‘NaN’ values.\nReset the index of the DataFrame without adding a new ‘index’ column.\n\n\n\nExercise 5 (Hard)\nGiven a DataFrame with 100 rows and 5 columns named ‘A’, ‘B’, ‘C’, ‘D’, and ‘E’. All columns should contain random integers between 1 and 100. Perform the following operations:\n\nSelect only the rows where ‘A’ is greater than 50 and ‘B’ is less than 50.\nFrom the resulting DataFrame, select only the ‘C’, ‘D’, and ‘E’ columns.\nCalculate the sum of the ‘C’, ‘D’, and ‘E’ columns for each row (you should end up with a Series).\nFind the row (index) with the highest sum."
  },
  {
    "objectID": "introduction_to_python/class2.html",
    "href": "introduction_to_python/class2.html",
    "title": "Introduction to NumPy",
    "section": "",
    "text": "NumPy, which stands for Numerical Python, is a foundational package for numerical computations in Python. It provides support for large multi-dimensional arrays and matrices, along with a collection of mathematical functions to operate on these arrays."
  },
  {
    "objectID": "introduction_to_python/class2.html#examples-of-numpy-arrays",
    "href": "introduction_to_python/class2.html#examples-of-numpy-arrays",
    "title": "Introduction to NumPy",
    "section": "Examples of NumPy Arrays",
    "text": "Examples of NumPy Arrays\nIn NumPy, arrays are the main way to store and manipulate data. Let’s look at some examples of creating and working with arrays.\n\nimport numpy as np\n\n# Creating a 1-dimensional array\narr_1d = np.array([1, 2, 3, 4, 5])\nprint('1D Array:', arr_1d)\n\n# Creating a 2-dimensional array\narr_2d = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nprint('\\n2D Array:')\nprint(arr_2d)\n\n# Creating a 3-dimensional array\narr_3d = np.array([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\nprint('\\n3D Array:')\nprint(arr_3d)"
  },
  {
    "objectID": "introduction_to_python/class2.html#array-operations",
    "href": "introduction_to_python/class2.html#array-operations",
    "title": "Introduction to NumPy",
    "section": "Array Operations",
    "text": "Array Operations\nArrays in NumPy can be operated on in a variety of ways. One common operation is element-wise addition. Let’s see what happens when we try to add arrays of different shapes.\n\n# Adding two 1D arrays of the same shape\narr1 = np.array([1, 2, 3])\narr2 = np.array([4, 5, 6])\nsum_1d = arr1 + arr2\nprint('Sum of two 1D arrays:', sum_1d)\n\n# Adding two 2D arrays of the same shape\narr3 = np.array([[1, 2], [3, 4]])\narr4 = np.array([[5, 6], [7, 8]])\nsum_2d = arr3 + arr4\nprint('\\nSum of two 2D arrays:')\nprint(sum_2d)\n\n# Trying to add arrays of different shapes (This will raise an error)\ntry:\n    sum_diff_shapes = arr1 + arr3\nexcept ValueError as e:\n    print('\\nError when trying to add arrays of different shapes:', e)"
  },
  {
    "objectID": "introduction_to_python/class2.html#numpy-functions-and-attributes",
    "href": "introduction_to_python/class2.html#numpy-functions-and-attributes",
    "title": "Introduction to NumPy",
    "section": "NumPy Functions and Attributes",
    "text": "NumPy Functions and Attributes\nNumPy provides a wide range of functions and attributes to perform operations on arrays. Let’s explore some of the commonly used ones.\n\n# Sample array for demonstration\narr = np.array([1, 5, 3, 7, 2, 8, 4, 6])\n\n# logical_and and logical_or\ncondition_and = np.logical_and(arr &gt; 2, arr &lt; 7)\ncondition_or = np.logical_or(arr &lt; 3, arr &gt; 6)\nprint('logical_and result:', condition_and)\nprint('logical_or result:', condition_or)\n\n# .shape attribute\nprint('\\nShape of the array:', arr.shape)\n\n# argmin() and argmax()\nprint('\\nIndex of minimum value:', arr.argmin())\nprint('Index of maximum value:', arr.argmax())\n\n# mean, median, std, var\nprint('\\nMean of the array:', np.mean(arr))\nprint('Median of the array:', np.median(arr))\nprint('Standard deviation of the array:', np.std(arr))\nprint('Variance of the array:', np.var(arr))\n\n# percentile\nprint('\\n25th percentile:', np.percentile(arr, 25))\nprint('75th percentile:', np.percentile(arr, 75))\n\n\nExplanation of the functions and attributes used:\n\nlogical_and: This function performs element-wise logical AND operation. It returns an array of the same shape as the input arrays, with True where the condition is met and False otherwise.\nlogical_or: Similar to logical_and, this function performs element-wise logical OR operation.\n.shape: This attribute returns a tuple representing the dimensions of the array.\nargmin(): Returns the index of the minimum value in the array.\nargmax(): Returns the index of the maximum value in the array.\nmean: Computes the arithmetic mean of the array elements.\nmedian: Returns the median of the array elements.\nstd: Computes the standard deviation of the array elements.\nvar: Computes the variance of the array elements.\npercentile: Computes the specified percentile of the array elements."
  },
  {
    "objectID": "introduction_to_python/class2.html#additional-numpy-functions",
    "href": "introduction_to_python/class2.html#additional-numpy-functions",
    "title": "Introduction to NumPy",
    "section": "Additional NumPy Functions",
    "text": "Additional NumPy Functions\nNumPy offers a plethora of functions for various mathematical and statistical operations. Here are some more functions that haven’t been covered yet:\n\ndot: Computes the dot product of two arrays.\ncross: Computes the cross product of two arrays.\nlinspace: Returns evenly spaced numbers over a specified range.\narange: Returns evenly spaced values within a given interval.\nreshape: Gives a new shape to an array without changing its data.\nconcatenate: Joins two or more arrays along an existing axis.\nsplit: Divides an array into multiple sub-arrays.\nsin, cos, tan: Trigonometric functions.\nexp: Computes the exponential of all elements in the input array.\nlog, log10, log2: Natural logarithm, base-10 logarithm, and base-2 logarithm.\nsqrt: Returns the non-negative square-root of an array, element-wise.\nunique: Finds the unique elements of an array.\nsort: Returns a sorted copy of an array.\n\n\n# Sample arrays for demonstration\na = np.array([1, 2, 3])\nb = np.array([4, 5, 6])\n\n# dot\ndot_product = np.dot(a, b)\nprint('Dot product of a and b:', dot_product)\n\n# cross\ncross_product = np.cross(a, b)\nprint('\\nCross product of a and b:', cross_product)\n\n# linspace\nlinspace_array = np.linspace(0, 10, 5)\nprint('\\nLinspace array:', linspace_array)\n\n# arange\narange_array = np.arange(0, 10, 2)\nprint('\\nArange array:', arange_array)\n\n# reshape\nreshaped_array = np.arange(9).reshape(3, 3)\nprint('\\nReshaped array:')\nprint(reshaped_array)\n\n# concatenate\nconcatenated_array = np.concatenate((a, b))\nprint('\\nConcatenated array:', concatenated_array)\n\n# split\nsplit_arrays = np.split(concatenated_array, 2)\nprint('\\nSplit arrays:', split_arrays)\n\n# sin, cos, tan\nprint('\\nSin of a:', np.sin(a))\nprint('Cos of a:', np.cos(a))\nprint('Tan of a:', np.tan(a))\n\n# exp\nprint('\\nExponential of a:', np.exp(a))\n\n# log, log10, log2\nprint('\\nNatural logarithm of a:', np.log(a))\nprint('Base-10 logarithm of a:', np.log10(a))\nprint('Base-2 logarithm of a:', np.log2(a))\n\n# sqrt\nprint('\\nSquare-root of a:', np.sqrt(a))\n\n# unique\nunique_array = np.unique(np.array([1, 2, 2, 3, 3, 3]))\nprint('\\nUnique elements:', unique_array)\n\n# sort\nsorted_array = np.sort(np.array([3, 1, 2]))\nprint('\\nSorted array:', sorted_array)"
  },
  {
    "objectID": "introduction_to_python/class2.html#exercises",
    "href": "introduction_to_python/class2.html#exercises",
    "title": "Introduction to NumPy",
    "section": "Exercises",
    "text": "Exercises\nTo test your understanding of the concepts covered in this notebook, try out the following exercises:\n\nEasy\n\nArray Creation: Create a 1-dimensional NumPy array containing the numbers from 1 to 10.\nArray Reshaping: Take the array you created in the previous exercise and reshape it into a 2x5 matrix.\n\n\n\nMedium\n\nArray Operations: Given two arrays a = np.array([1, 2, 3, 4]) and b = np.array([5, 6, 7, 8]), compute the dot product and the element-wise sum.\nArray Functions: For the array c = np.array([10, 20, 30, 40, 50, 60, 70, 80, 90, 100]), compute the mean, median, and standard deviation. Also, find the 25th and 75th percentiles.\n\n\n\nHard\n\nLogical Operations: Given the array d = np.array([3, 7, 1, 10, 5, 8, 2, 9, 4, 6]), use logical operations to find all numbers in the array that are greater than 3 and less than 8. Then, compute the sum of these numbers."
  },
  {
    "objectID": "introduction_to_python/class6.html",
    "href": "introduction_to_python/class6.html",
    "title": "Class 6: Data Manipulation II",
    "section": "",
    "text": "Discretization refers to the process of converting continuous variables into discrete ones. This can be particularly useful when we want to categorize continuous data into specific bins or intervals. In data analysis, sometimes, it’s more meaningful to work with categories or intervals rather than raw continuous values.\n\n\n\nContinuous Variable: A variable that can take any value within a given range. Examples include height, weight, and temperature.\nDiscrete Variable: A variable that can take specific values, often in the form of integers. Examples include the number of cars in a household or the number of students in a class.\n\nThe pandas library in Python provides a function called pd.cut() that helps in discretizing continuous variables. Let’s delve deeper into its parameters and see how it works.\n\n\n\n\nx: The input array to be binned. Must be 1-dimensional.\nbins: Defines the bin edges for the discretization. It can be an integer, sequence of scalars, or an IntervalIndex.\nright: Indicates whether the bins include the rightmost edge or not. Default is True.\nlabels: Specifies the labels for the returned bins. Must be the same length as the resulting bins.\nretbins: Whether to return the bin edges (True or False). Default is False.\nprecision: The precision at which to store and display the bins labels.\ninclude_lowest: Whether the first interval should be left-inclusive or not. Default is False.\n\nNow, let’s see some examples of how to use pd.cut().\n\nimport pandas as pd\n\n# Example 1: Basic usage of pd.cut()\ndata = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\nbins = [0, 2, 4, 6, 8, 10]\ncategories = pd.cut(data, bins)\ncategories\n\n[(0, 2], (0, 2], (2, 4], (2, 4], (4, 6], (4, 6], (6, 8], (6, 8], (8, 10], (8, 10]]\nCategories (5, interval[int64, right]): [(0, 2] &lt; (2, 4] &lt; (4, 6] &lt; (6, 8] &lt; (8, 10]]\n\n\n\n# Example 2: Using labels parameter\nbin_labels = ['low', 'medium', 'high', 'very high', 'extreme']\ncategories_with_labels = pd.cut(data, bins, labels=bin_labels)\ncategories_with_labels\n\n['low', 'low', 'medium', 'medium', 'high', 'high', 'very high', 'very high', 'extreme', 'extreme']\nCategories (5, object): ['low' &lt; 'medium' &lt; 'high' &lt; 'very high' &lt; 'extreme']\n\n\n\n# Example 3: Using right parameter to exclude the rightmost edge\ncategories_excluding_right = pd.cut(data, bins, right=False)\ncategories_excluding_right\n\n[[0.0, 2.0), [2.0, 4.0), [2.0, 4.0), [4.0, 6.0), [4.0, 6.0), [6.0, 8.0), [6.0, 8.0), [8.0, 10.0), [8.0, 10.0), NaN]\nCategories (5, interval[int64, left]): [[0, 2) &lt; [2, 4) &lt; [4, 6) &lt; [6, 8) &lt; [8, 10)]"
  },
  {
    "objectID": "introduction_to_python/class6.html#discretization-of-data",
    "href": "introduction_to_python/class6.html#discretization-of-data",
    "title": "Class 6: Data Manipulation II",
    "section": "",
    "text": "Discretization refers to the process of converting continuous variables into discrete ones. This can be particularly useful when we want to categorize continuous data into specific bins or intervals. In data analysis, sometimes, it’s more meaningful to work with categories or intervals rather than raw continuous values.\n\n\n\nContinuous Variable: A variable that can take any value within a given range. Examples include height, weight, and temperature.\nDiscrete Variable: A variable that can take specific values, often in the form of integers. Examples include the number of cars in a household or the number of students in a class.\n\nThe pandas library in Python provides a function called pd.cut() that helps in discretizing continuous variables. Let’s delve deeper into its parameters and see how it works.\n\n\n\n\nx: The input array to be binned. Must be 1-dimensional.\nbins: Defines the bin edges for the discretization. It can be an integer, sequence of scalars, or an IntervalIndex.\nright: Indicates whether the bins include the rightmost edge or not. Default is True.\nlabels: Specifies the labels for the returned bins. Must be the same length as the resulting bins.\nretbins: Whether to return the bin edges (True or False). Default is False.\nprecision: The precision at which to store and display the bins labels.\ninclude_lowest: Whether the first interval should be left-inclusive or not. Default is False.\n\nNow, let’s see some examples of how to use pd.cut().\n\nimport pandas as pd\n\n# Example 1: Basic usage of pd.cut()\ndata = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\nbins = [0, 2, 4, 6, 8, 10]\ncategories = pd.cut(data, bins)\ncategories\n\n[(0, 2], (0, 2], (2, 4], (2, 4], (4, 6], (4, 6], (6, 8], (6, 8], (8, 10], (8, 10]]\nCategories (5, interval[int64, right]): [(0, 2] &lt; (2, 4] &lt; (4, 6] &lt; (6, 8] &lt; (8, 10]]\n\n\n\n# Example 2: Using labels parameter\nbin_labels = ['low', 'medium', 'high', 'very high', 'extreme']\ncategories_with_labels = pd.cut(data, bins, labels=bin_labels)\ncategories_with_labels\n\n['low', 'low', 'medium', 'medium', 'high', 'high', 'very high', 'very high', 'extreme', 'extreme']\nCategories (5, object): ['low' &lt; 'medium' &lt; 'high' &lt; 'very high' &lt; 'extreme']\n\n\n\n# Example 3: Using right parameter to exclude the rightmost edge\ncategories_excluding_right = pd.cut(data, bins, right=False)\ncategories_excluding_right\n\n[[0.0, 2.0), [2.0, 4.0), [2.0, 4.0), [4.0, 6.0), [4.0, 6.0), [6.0, 8.0), [6.0, 8.0), [8.0, 10.0), [8.0, 10.0), NaN]\nCategories (5, interval[int64, left]): [[0, 2) &lt; [2, 4) &lt; [4, 6) &lt; [6, 8) &lt; [8, 10)]"
  },
  {
    "objectID": "introduction_to_python/class6.html#using-np.where-with-pandas-dataframes",
    "href": "introduction_to_python/class6.html#using-np.where-with-pandas-dataframes",
    "title": "Class 6: Data Manipulation II",
    "section": "Using np.where with Pandas DataFrames",
    "text": "Using np.where with Pandas DataFrames\nThe np.where function from the NumPy library is a versatile tool that allows for conditional logic based on array-like structures. In essence, it’s a vectorized version of the ternary expression x if condition else y.\nWhen working with Pandas DataFrames, np.where can be particularly useful for creating or modifying columns based on certain conditions.\nThe basic syntax is as follows:\nnp.where(condition, value_if_true, value_if_false)\nLet’s explore some examples to understand its usage with Pandas DataFrames.\n\nimport numpy as np\n\n# Example 1: Basic usage of np.where with DataFrame\ndf1 = pd.DataFrame({'A': [1, 2, 3, 4, 5], 'B': [10, 20, 30, 40, 50]})\ndf1['C'] = np.where(df1['A'] &gt; 3, 'high', 'low')\ndf1\n\nUnable to display output for mime type(s): application/vnd.dataresource+json\n\n\n\n# Example 2: Using np.where with multiple conditions\ndf2 = pd.DataFrame({'Score': [85, 90, 78, 88, 76, 95, 89]})\nconditions = [df2['Score'] &lt; 80, (df2['Score'] &gt;= 80) & (df2['Score'] &lt; 90), df2['Score'] &gt;= 90]\nchoices = ['Fail', 'Pass', 'Excellent']\ndf2['Grade'] = np.select(conditions, choices, default='Unknown')\ndf2\n\nUnable to display output for mime type(s): application/vnd.dataresource+json\n\n\n\n# Example 3: Modifying an existing column based on a condition\ndf3 = pd.DataFrame({'Age': [25, 30, 35, 40, 45, 50, 55]})\ndf3['Category'] = np.where(df3['Age'] &lt; 40, 'Young', 'Old')\ndf3\n\nUnable to display output for mime type(s): application/vnd.dataresource+json"
  },
  {
    "objectID": "introduction_to_python/class6.html#one-hot-encoding-with-pd.get_dummies",
    "href": "introduction_to_python/class6.html#one-hot-encoding-with-pd.get_dummies",
    "title": "Class 6: Data Manipulation II",
    "section": "One-Hot Encoding with pd.get_dummies()",
    "text": "One-Hot Encoding with pd.get_dummies()\nOne of the common challenges in data preprocessing, especially when dealing with categorical data, is how to convert these categories into a format that can be provided to machine learning algorithms. One popular way to convert categorical variables is by using one-hot encoding.\nOne-hot encoding is a process of converting categorical data variables so they can be provided to machine learning algorithms to improve predictions. The pd.get_dummies() function is used for this purpose. It converts categorical variable(s) into dummy/indicator variables.\nThe basic idea is to convert each category into a new column, and assign a 1 or 0 (True/False) value to the column. This has the benefit of not weighting a value improperly.\nLet’s explore some examples to understand its usage with Pandas DataFrames.\n\n# Example 1: Basic usage of pd.get_dummies()\ndf1 = pd.DataFrame({'Fruit': ['Apple', 'Banana', 'Cherry', 'Apple', 'Banana']})\ndummies1 = pd.get_dummies(df1, columns=['Fruit'], prefix='', prefix_sep='')\ndummies1\n\nUnable to display output for mime type(s): application/vnd.dataresource+json\n\n\n\n# Example 2: Using prefix and prefix_sep parameters\ndf2 = pd.DataFrame({'Color': ['Red', 'Blue', 'Green', 'Red', 'Green']})\ndummies2 = pd.get_dummies(df2, columns=['Color'], prefix='Color', prefix_sep='_')\ndummies2\n\nUnable to display output for mime type(s): application/vnd.dataresource+json\n\n\n\n# Example 3: Handling multiple categorical columns\ndf3 = pd.DataFrame({'Animal': ['Dog', 'Cat', 'Bird', 'Dog', 'Bird'],\n                   'Size': ['Small', 'Medium', 'Small', 'Large', 'Medium']})\ndummies3 = pd.get_dummies(df3, columns=['Animal', 'Size'], prefix=['Animal', 'Size'])\ndummies3\n\nUnable to display output for mime type(s): application/vnd.dataresource+json"
  },
  {
    "objectID": "introduction_to_python/class6.html#dropping-a-dummy-variable-avoiding-the-dummy-variable-trap",
    "href": "introduction_to_python/class6.html#dropping-a-dummy-variable-avoiding-the-dummy-variable-trap",
    "title": "Class 6: Data Manipulation II",
    "section": "Dropping a Dummy Variable (Avoiding the Dummy Variable Trap)",
    "text": "Dropping a Dummy Variable (Avoiding the Dummy Variable Trap)\nWhen we use one-hot encoding, we can sometimes fall into the ‘dummy variable trap’, which can mess up the results of some machine learning models. The dummy variable trap is a scenario where variables are highly correlated, meaning one variable can be predicted from the others.\nFor instance, if we have three categories (A, B, and C) and we know the values of A and B dummy variables, we can easily infer the value of the C dummy variable. This is because if A=0 and B=0, then C must be 1. This multicollinearity can lead to issues in certain machine learning models.\nTo avoid the dummy variable trap, one common practice is to drop one of the dummy variables if we have n categories. By doing this, we will have n-1 dummy variables. This is often referred to as ‘dropping the first level’.\nLet’s see an example to understand this better.\n\n# Creating a sample DataFrame with three categories\ndf = pd.DataFrame({'Category': ['A', 'B', 'C', 'A', 'B', 'C', 'A']})\n\n# Using pd.get_dummies() and dropping the first level\ndummies = pd.get_dummies(df, columns=['Category'], drop_first=True)\ndummies"
  },
  {
    "objectID": "introduction_to_python/class6.html#string-manipulation",
    "href": "introduction_to_python/class6.html#string-manipulation",
    "title": "Class 6: Data Manipulation II",
    "section": "String Manipulation",
    "text": "String Manipulation\nIn data analysis, often we encounter columns that contain string (text) data. Pandas provides a set of string functions which make it easy to operate on string data. These functions are available in the str attribute of the Series/DataFrame.\nLet’s explore some of the commonly used string functions:\n\n.lower(): Converts all characters in string to lowercase.\n.str: Allows us to apply string functions on a Series of a DataFrame.\n.strip(): Removes leading and trailing whitespaces (including spaces).\n\n\n# Example for .lower()\ndf1 = pd.DataFrame({'Names': ['ALICE', 'BOB', 'CHARLIE', 'DAVID']})\ndf1['Names_lower'] = df1['Names'].str.lower()\ndf1\n\n\n# Example for .strip()\ndf2 = pd.DataFrame({'Data': ['  apple  ', ' banana ', ' cherry  ', ' date']})\ndf2['Stripped_Data'] = df2['Data'].str.strip()\ndf2"
  },
  {
    "objectID": "introduction_to_python/class6.html#date-manipulation-with-pandas",
    "href": "introduction_to_python/class6.html#date-manipulation-with-pandas",
    "title": "Class 6: Data Manipulation II",
    "section": "Date Manipulation with Pandas",
    "text": "Date Manipulation with Pandas\nHandling dates and time is a common task in data analysis. Pandas provides a rich set of functions to work with date and time, which makes it easy to perform operations like parsing dates, extracting date components, and even doing arithmetic with dates.\nOne of the most commonly used functions for date manipulation in Pandas is pd.to_datetime(). This function is used to convert arguments to datetime format.\n\n# Example: Converting a Series of strings to datetime format\ndf_dates = pd.DataFrame({'Date_String': ['2023-01-01', '2023-02-15', '2023-03-20', '2023-04-25']})\ndf_dates['Date'] = pd.to_datetime(df_dates['Date_String'])\ndf_dates\n\nThe pd.to_datetime() function has several parameters that allow for customization. Some of the commonly used parameters are:\n\nformat: This is used to specify the date format. For example, ‘%Y-%m-%d’ represents the format ‘YYYY-MM-DD’.\nerrors: This parameter specifies what should be returned if parsing fails. The default is ‘raise’, which means an exception is raised. Other options include ‘coerce’ (returns NaT for invalid parsing) and ‘ignore’ (returns the original input).\n\nOnce we have our dates in datetime format, we can easily extract various components from it. Let’s see how to extract the year, month, day of the week, and the name of the month from our datetime column.\n\n# Extracting various components from the datetime column\ndf_dates['Year'] = df_dates['Date'].dt.year\ndf_dates['Month'] = df_dates['Date'].dt.month\ndf_dates['Day_of_Week'] = df_dates['Date'].dt.dayofweek\ndf_dates['Month_Name'] = df_dates['Date'].dt.strftime('%B')\ndf_dates\n\nThe format parameter in pd.to_datetime() allows us to specify the expected date format of our input. This can be particularly useful when dealing with dates that are in non-standard formats. By providing the correct format, we can ensure that our dates are parsed correctly.\nLet’s look at some examples with different date formats:\n\n# Example with different date formats\ndf_formats = pd.DataFrame({'Date_String': ['01-23-2023', '15/02/2023', '20.03.2023']})\n\n# Parsing dates with specified formats\ndf_formats['Date_1'] = pd.to_datetime(df_formats['Date_String'], format='%m-%d-%Y')\ndf_formats['Date_2'] = pd.to_datetime(df_formats['Date_String'], format='%d/%m/%Y', errors='coerce')\ndf_formats['Date_3'] = pd.to_datetime(df_formats['Date_String'], format='%d.%m.%Y', errors='coerce')\ndf_formats"
  },
  {
    "objectID": "introduction_to_python/class6.html#exercises",
    "href": "introduction_to_python/class6.html#exercises",
    "title": "Class 6: Data Manipulation II",
    "section": "Exercises",
    "text": "Exercises\nNow that you’ve learned various data manipulation techniques with Pandas, it’s time to put your knowledge to the test! Below are exercises that cover the topics we’ve discussed. Each exercise provides a sample dataframe for you to work with.\n\nDiscretization of Data\n\nAge Groups:\n\nGiven the following dataset of customers with their ages, create age groups as ‘Young (0-18)’, ‘Adult (19-50)’, and ‘Senior (51 and above)’. How many customers fall into each category?\ndf_ages = pd.DataFrame({'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eva'],\n                       'Age': [15, 25, 55, 60, 30]})\n\nSalary Brackets:\n\nGiven the following dataset of employee salaries, categorize them into ‘Low’, ‘Medium’, ‘High’ salary brackets. Define the salary ranges for each bracket yourself.\ndf_salaries = pd.DataFrame({'Employee': ['John', 'Doe', 'Smith', 'Jane', 'Emily'],\n                           'Salary': [3000, 5000, 7000, 10000, 4000]})\n\n\nnp.where Usage\n\nTemperature Analysis:\n\nGiven the following dataset of daily temperatures, identify days with temperatures below freezing (0°C) and label them as ‘Cold’. Days with temperatures between 0°C and 20°C are ‘Moderate’, and days above 20°C are ‘Hot’.\ndf_temperatures = pd.DataFrame({'Day': ['Mon', 'Tue', 'Wed', 'Thu', 'Fri'],\n                               'Temperature': [-5, 10, 15, 25, 30]})\n\nProduct Sales:\n\nGiven the following dataset of product sales, identify products with sales less than 100 as ‘Low Selling’, between 100 and 500 as ‘Moderate Selling’, and above 500 as ‘High Selling’.\ndf_sales = pd.DataFrame({'Product': ['A', 'B', 'C', 'D', 'E'],\n                        'Sales': [50, 150, 300, 600, 800]})\n\n\nOne-Hot Encoding with pd.get_dummies\n\nCar Brands:\n\nGiven the following dataset of car sales, perform one-hot encoding on the ‘Brand’ column.\ndf_cars = pd.DataFrame({'Brand': ['Toyota', 'Honda', 'Ford', 'Toyota', 'Ford'],\n                       'Sales': [200, 150, 300, 400, 250]})\n\nStudent Majors:\n\nGiven the following dataset of students and their majors, perform one-hot encoding on the ‘Major’ column, but only keep columns for ‘Engineering’ and ‘Business’.\ndf_students = pd.DataFrame({'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eva'],\n                           'Major': ['Engineering', 'Business', 'Arts', 'Business', 'Science']})\n\n\nString Manipulation\n\nName Formatting:\n\nGiven the following dataset of names, convert all names to lowercase.\ndf_names = pd.DataFrame({'Name': ['ALICE', 'bOB', 'ChArLiE', 'David', 'EVA']})\n\nWhitespace Removal:\n\nGiven the following dataset of product descriptions, remove any leading or trailing whitespace from the ‘Description’ column.\ndf_products = pd.DataFrame({'Product': ['A', 'B', 'C', 'D', 'E'],\n                           'Description': ['  toy ', 'book  ', '  pen', ' notebook', ' pencil ']})\n\n\nDate Manipulation\n\nDate Conversion:\n\nGiven the following dataset of events with dates in the format ‘dd-mm-yyyy’, convert the ‘Date’ column to a datetime format.\ndf_events = pd.DataFrame({'Event': ['Concert', 'Festival', 'Exhibition', 'Play', 'Conference'],\n                         'Date': ['01-05-2022', '15-06-2022', '20-07-2022', '10-08-2022', '25-09-2022']})\n\nExtracting Date Components:\n\nGiven the following dataset of birthdays, extract the month and day of the week for each date.\ndf_birthdays = pd.DataFrame({'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eva'],\n                            'Birthday': ['1995-05-01', '1987-06-15', '1992-07-20', '1985-08-10', '1990-09-25']})\nRemember to practice regularly and apply these techniques to real-world datasets to reinforce your understanding. Happy coding!"
  },
  {
    "objectID": "personal/laboral.html",
    "href": "personal/laboral.html",
    "title": "Data Experience",
    "section": "",
    "text": "Data Scientist (2021)\n\nPromoted to Senior Data Scientist\nFurther promoted to Expert Data Scientist\nLed a team focusing on People Analytics and Text Analysis"
  },
  {
    "objectID": "personal/laboral.html#caja-los-andes",
    "href": "personal/laboral.html#caja-los-andes",
    "title": "Data Experience",
    "section": "",
    "text": "Data Scientist (2021)\n\nPromoted to Senior Data Scientist\nFurther promoted to Expert Data Scientist\nLed a team focusing on People Analytics and Text Analysis"
  },
  {
    "objectID": "personal/laboral.html#walmart",
    "href": "personal/laboral.html#walmart",
    "title": "Data Experience",
    "section": "Walmart",
    "text": "Walmart\n\nData Scientist (Mid 2023 - Present)\n\nSpecialized in Commercial Data Analysis"
  },
  {
    "objectID": "personal/laboral.html#universidad-del-desarrollo",
    "href": "personal/laboral.html#universidad-del-desarrollo",
    "title": "Data Experience",
    "section": "Universidad del Desarrollo",
    "text": "Universidad del Desarrollo\n\nOnline Big Data Course (2022)\n\nOffered by e-class"
  },
  {
    "objectID": "personal/laboral.html#universidad-técnica-federico-santa-maría",
    "href": "personal/laboral.html#universidad-técnica-federico-santa-maría",
    "title": "Data Experience",
    "section": "Universidad Técnica Federico Santa María",
    "text": "Universidad Técnica Federico Santa María\n\nLecturer for the course: Machine Learning with Business Applications (2022 - 2023)\n\nLinked to the Erasmus Mundus Master’s program in Europe"
  },
  {
    "objectID": "personal/laboral.html#universidad-de-santiago",
    "href": "personal/laboral.html#universidad-de-santiago",
    "title": "Data Experience",
    "section": "Universidad de Santiago",
    "text": "Universidad de Santiago\n\nLecturer for the course: Data Science for Mathematical Engineers (First semester, 2023)"
  },
  {
    "objectID": "personal/laboral.html#desafio-latam-bootcamp",
    "href": "personal/laboral.html#desafio-latam-bootcamp",
    "title": "Data Experience",
    "section": "Desafio Latam Bootcamp",
    "text": "Desafio Latam Bootcamp\n\nInstructor for Data Science and Data Analyst courses (2023)"
  },
  {
    "objectID": "machine_learning_egei/class1.html",
    "href": "machine_learning_egei/class1.html",
    "title": "Class1: Different types of Data",
    "section": "",
    "text": "In the field of data science, we often talk about different types of data. Understanding these types is crucial for proper data analysis and interpretation. Here are some of the most common types of data:\nIn Python, we have several data types that are commonly used. Here are some of them with examples:\nPython has a variety of operators that can be used with different data types. Here are some examples:"
  },
  {
    "objectID": "machine_learning_egei/class1.html#python-data-types-exercises",
    "href": "machine_learning_egei/class1.html#python-data-types-exercises",
    "title": "Class1: Different types of Data",
    "section": "Python Data Types: Exercises",
    "text": "Python Data Types: Exercises\n\n# Exercise 1: Working with integers and floats\nnum1 = 10\nnum2 = 3\n\n# Addition\nprint('Addition:', num1 + num2)\n\n# Subtraction\nprint('Subtraction:', num1 - num2)\n\n# Multiplication\nprint('Multiplication:', num1 * num2)\n\n# Division\nprint('Division:', num1 / num2)\n\n# Floor Division\nprint('Floor Division:', num1 // num2)\n\n# Modulus\nprint('Modulus:', num1 % num2)\n\n# Exponentiation\nprint('Exponentiation:', num1 ** num2)\n\nAddition: 13\nSubtraction: 7\nMultiplication: 30\nDivision: 3.3333333333333335\nFloor Division: 3\nModulus: 1\nExponentiation: 1000\n\n\n\n# Exercise 2: Working with strings\nstr1 = 'Hello'\nstr2 = 'World'\n\n# String concatenation\nprint('Concatenation:', str1 + ' ' + str2)\n\n# String repetition\nprint('Repetition:', str1 * 3)\n\n# String slicing\nprint('Slicing:', str1[1:4])\n\n# String length\nprint('Length:', len(str1))\n\nConcatenation: Hello World\nRepetition: HelloHelloHello\nSlicing: ell\nLength: 5\n\n\n\n# Exercise 3: Working with lists\nlist1 = [1, 2, 3, 4, 5]\n\n# Accessing elements\nprint('First element:', list1[0])\nprint('Last element:', list1[-1])\n\n# List slicing\nprint('Slicing:', list1[1:4])\n\n# List length\nprint('Length:', len(list1))\n\n# Adding elements\nlist1.append(6)\nprint('After adding an element:', list1)\n\n# Removing elements\nlist1.remove(1)\nprint('After removing an element:', list1)\n\nFirst element: 1\nLast element: 5\nSlicing: [2, 3, 4]\nLength: 5\nAfter adding an element: [1, 2, 3, 4, 5, 6]\nAfter removing an element: [2, 3, 4, 5, 6]\n\n\n\n# Exercise 4: Working with dictionaries\ndict1 = {'name': 'John', 'age': 30, 'city': 'New York'}\n\n# Accessing elements\nprint('Name:', dict1['name'])\nprint('Age:', dict1['age'])\n\n# Adding elements\ndict1['job'] = 'Engineer'\nprint('After adding an element:', dict1)\n\n# Removing elements\ndel dict1['age']\nprint('After removing an element:', dict1)\n\nName: John\nAge: 30\nAfter adding an element: {'name': 'John', 'age': 30, 'city': 'New York', 'job': 'Engineer'}\nAfter removing an element: {'name': 'John', 'city': 'New York', 'job': 'Engineer'}"
  },
  {
    "objectID": "machine_learning_egei/class1.html#python-functions",
    "href": "machine_learning_egei/class1.html#python-functions",
    "title": "Class1: Different types of Data",
    "section": "Python Functions",
    "text": "Python Functions\nIn Python, a function is a block of reusable code that performs a specific task. Functions help break our program into smaller and modular chunks, making it organized and manageable. They also prevent repetition and make the code reusable.\nHere’s the basic syntax of a Python function:\ndef function_name(parameters):\n    \"\"\"docstring\"\"\"\n    statement(s)\n\nThe def keyword is a statement for defining a function in Python.\nfunction_name is a unique identifier for the function.\nparameters (arguments) through which we pass values to a function. These are optional.\nA colon (:) to mark the end of the function header.\nOptional documentation string (docstring) to describe what the function does.\nOne or more valid python statements that make up the function body. Statements must have the same indentation level (usually 4 spaces).\nAn optional return statement to return a value from the function.\n\n\n# Example of a Python function\n\ndef greet(name):\n    \"\"\"\n    This function greets the person passed in as a parameter\n    \"\"\"\n    print(f'Hello, {name}. Good morning!')\n\n# Calling the function\ngreet('John')\n\nHello, John. Good morning!\n\n\n\n# Example of a Python function with a return statement\n\ndef add_numbers(num1, num2):\n    \"\"\"\n    This function adds the two numbers passed in as parameters\n    \"\"\"\n    return num1 + num2\n\n# Calling the function and printing the result\nresult = add_numbers(5, 3)\nprint(f'The sum is {result}')\n\nThe sum is 8"
  },
  {
    "objectID": "machine_learning_egei/class1.html#exercises",
    "href": "machine_learning_egei/class1.html#exercises",
    "title": "Class1: Different types of Data",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 1: Data Types\nGiven the following variables, convert each one to a different data type:\nnum = '123'\nstr_num = 456\nConvert num to an integer and str_num to a string. Print the new variables and their types.\n\n\nExercise 2: Functions\nWrite a function named calculate_average that takes a list of numbers as a parameter and returns their average.\nThen, call your function with the list [1, 2, 3, 4, 5] and print the result.\n\n\nExercise 3: Lists and Dictionaries\nGiven the following list of dictionaries:\ndata = [\n    {'name': 'John', 'age': 30, 'job': 'Engineer'},\n    {'name': 'Anna', 'age': 27, 'job': 'Doctor'},\n    {'name': 'Mike', 'age': 35, 'job': 'Artist'}\n]\nWrite a function named get_ages that takes the list as a parameter and returns a list of all ages. Then, call your function with the data list and print the result."
  },
  {
    "objectID": "machine_learning_egei/class3.html",
    "href": "machine_learning_egei/class3.html",
    "title": "Machine Learning Models",
    "section": "",
    "text": "In the realm of data science and artificial intelligence, machine learning models play a pivotal role. They are algorithms that learn patterns from data and then make predictions or decisions without being explicitly programmed to do so. In the business context, machine learning models can be used for a variety of tasks such as customer segmentation, sales forecasting, and even fraud detection."
  },
  {
    "objectID": "machine_learning_egei/class3.html#what-is-a-ml-model",
    "href": "machine_learning_egei/class3.html#what-is-a-ml-model",
    "title": "Machine Learning Models",
    "section": "What is a ML Model?",
    "text": "What is a ML Model?\nA machine learning model is a mathematical representation of a real-world process. Think of it as a mathematical equation that has been trained on historical data to make predictions about future data. For businesses, these models can be used to gain insights, make informed decisions, and automate tasks."
  },
  {
    "objectID": "machine_learning_egei/class3.html#fitting-a-model",
    "href": "machine_learning_egei/class3.html#fitting-a-model",
    "title": "Machine Learning Models",
    "section": "Fitting a Model",
    "text": "Fitting a Model\nFitting a model, also known as training a model, involves providing a machine learning algorithm with data and allowing it to learn the patterns. This is done by feeding the model input data and the corresponding output. The model then adjusts its weights based on the error of its predictions. In a business scenario, this could mean using past sales data to predict future sales.\nIn the following sections, we will delve deeper into specific machine learning models and their applications in the business world."
  },
  {
    "objectID": "machine_learning_egei/class3.html#knn-k-nearest-neighbors",
    "href": "machine_learning_egei/class3.html#knn-k-nearest-neighbors",
    "title": "Machine Learning Models",
    "section": "KNN (K-Nearest Neighbors)",
    "text": "KNN (K-Nearest Neighbors)\nK-Nearest Neighbors (KNN) is a simple, yet powerful supervised machine learning algorithm used for classification and regression. It works by finding the ‘k’ training examples that are closest to a given input example and returning the most common output value among them.\n\nBusiness Application of KNN\nImagine a retail business that wants to segment its customers based on their purchase behavior. Using KNN, the business can classify a new customer into a particular segment by looking at the purchase behaviors of the ‘k’ most similar existing customers. This can help the business tailor its marketing strategies for different customer segments.\nLet’s see a simple example in Python where we use KNN to classify customers into two segments: ‘High Value’ and ‘Low Value’ based on their annual spending and frequency of purchase.\n\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nimport matplotlib.pyplot as plt\n\n# Sample data: Annual Spending (in $) and Frequency of Purchase (number of times in a year)\nX = np.array([[500, 2], [1500, 5], [3000, 8], [4000, 10], [750, 3], [400, 1], [4500, 12], [2000, 6]])\n# Labels: 0 for 'Low Value' and 1 for 'High Value'\ny = np.array([0, 0, 1, 1, 0, 0, 1, 1])\n\n# Splitting data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Using KNN for classification with k=3\nknn = KNeighborsClassifier(n_neighbors=3)\nknn.fit(X_train, y_train)\npredictions = knn.predict(X_test)\n\n# Plotting the data points\nplt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis')\nplt.xlabel('Annual Spending ($)')\nplt.ylabel('Frequency of Purchase')\nplt.title('Customer Segmentation using KNN')\nplt.show()\n\npredictions"
  },
  {
    "objectID": "machine_learning_egei/class3.html#polynomial-regression",
    "href": "machine_learning_egei/class3.html#polynomial-regression",
    "title": "Machine Learning Models",
    "section": "Polynomial Regression",
    "text": "Polynomial Regression\nPolynomial Regression is a type of regression analysis used to model the relationship between a dependent variable and one or more independent variables by fitting a polynomial equation to the observed data. Unlike linear regression which models the relationship using a straight line, polynomial regression models it using a curve.\n\nBusiness Application of Polynomial Regression\nConsider a business that wants to understand the relationship between advertising spend and sales. While initially, an increase in advertising might lead to a significant increase in sales, after a certain point, the effect of additional advertising might diminish. In such cases, a polynomial regression can capture the non-linear relationship between advertising spend and sales more effectively than a linear model.\nLet’s see a Python example where we use Polynomial Regression to model the relationship between advertising spend and sales.\n\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\n\n# Sample data: Advertising Spend (in $) and Sales (in $)\nX_ad = np.array([[50], [100], [150], [200], [250], [300], [350], [400]])\ny_sales = np.array([150, 220, 260, 275, 280, 285, 290, 295])\n\n# Transforming our data for Polynomial Regression\npoly = PolynomialFeatures(degree=2)\nX_poly = poly.fit_transform(X_ad)\n\n# Fitting the Polynomial Regression model\npoly_reg = LinearRegression()\npoly_reg.fit(X_poly, y_sales)\n\n# Predicting sales based on advertising spend\ny_pred = poly_reg.predict(X_poly)\n\n# Plotting the data points and the polynomial regression curve\nplt.scatter(X_ad, y_sales, color='blue', label='Actual Sales')\nplt.plot(X_ad, y_pred, color='red', label='Polynomial Regression')\nplt.xlabel('Advertising Spend ($)')\nplt.ylabel('Sales ($)')\nplt.title('Advertising Spend vs Sales using Polynomial Regression')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "machine_learning_egei/class3.html#advanced-knn-example-loan-default-prediction",
    "href": "machine_learning_egei/class3.html#advanced-knn-example-loan-default-prediction",
    "title": "Machine Learning Models",
    "section": "Advanced KNN Example: Loan Default Prediction",
    "text": "Advanced KNN Example: Loan Default Prediction\nIn this scenario, a bank wants to predict the risk associated with a loan based on a customer’s credit score and annual income. The bank classifies the risk into three categories: ‘Low Risk’, ‘Medium Risk’, and ‘High Risk’.\nLet’s use KNN to classify customers into these risk categories.\n\n# Sample data: Credit Score and Annual Income (in $)\nX_loan = np.array([[650, 40000], [700, 55000], [600, 30000], [720, 75000], [630, 25000], [680, 50000], [590, 20000], [750, 90000]])\n# Labels: 0 for 'Low Risk', 1 for 'Medium Risk', and 2 for 'High Risk'\ny_risk = np.array([0, 0, 1, 0, 2, 0, 2, 0])\n\n# Splitting data into training and testing sets\nX_loan_train, X_loan_test, y_risk_train, y_risk_test = train_test_split(X_loan, y_risk, test_size=0.2, random_state=42)\n\n# Using KNN for classification with k=3\nknn_loan = KNeighborsClassifier(n_neighbors=3)\nknn_loan.fit(X_loan_train, y_risk_train)\nrisk_predictions = knn_loan.predict(X_loan_test)\n\n# Plotting the data points\nplt.scatter(X_loan[:, 0], X_loan[:, 1], c=y_risk, cmap='viridis')\nplt.xlabel('Credit Score')\nplt.ylabel('Annual Income ($)')\nplt.title('Loan Default Risk Prediction using KNN')\nplt.colorbar().set_label('Risk Level')\nplt.show()\n\nrisk_predictions"
  },
  {
    "objectID": "machine_learning_egei/class3.html#advanced-polynomial-regression-example-salary-prediction",
    "href": "machine_learning_egei/class3.html#advanced-polynomial-regression-example-salary-prediction",
    "title": "Machine Learning Models",
    "section": "Advanced Polynomial Regression Example: Salary Prediction",
    "text": "Advanced Polynomial Regression Example: Salary Prediction\nIn this scenario, a company wants to understand the relationship between the years of experience and the salary of its employees. As employees gain more experience, the growth in their salary might not be linear, especially at higher experience levels where salary increments might start to plateau.\nLet’s use Polynomial Regression to model this non-linear relationship between years of experience and salary.\n\n# Sample data: Years of Experience and Salary (in $)\nX_exp = np.array([[1], [2], [3], [4], [5], [6], [7], [8], [9], [10]])\ny_sal = np.array([40000, 45000, 50000, 60000, 75000, 85000, 95000, 105000, 110000, 115000])\n\n# Transforming our data for Polynomial Regression of degree 2\npoly_exp = PolynomialFeatures(degree=2)\nX_exp_poly = poly_exp.fit_transform(X_exp)\n\n# Fitting the Polynomial Regression model\npoly_reg_exp = LinearRegression()\npoly_reg_exp.fit(X_exp_poly, y_sal)\n\n# Predicting salary based on years of experience\ny_sal_pred = poly_reg_exp.predict(X_exp_poly)\n\n# Plotting the data points and the polynomial regression curve\nplt.scatter(X_exp, y_sal, color='blue', label='Actual Salary')\nplt.plot(X_exp, y_sal_pred, color='red', label='Polynomial Regression')\nplt.xlabel('Years of Experience')\nplt.ylabel('Salary ($)')\nplt.title('Years of Experience vs Salary using Polynomial Regression')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "machine_learning_egei/class3.html#knn-exercise-customer-churn-prediction",
    "href": "machine_learning_egei/class3.html#knn-exercise-customer-churn-prediction",
    "title": "Machine Learning Models",
    "section": "KNN Exercise: Customer Churn Prediction",
    "text": "KNN Exercise: Customer Churn Prediction\nIn this exercise, you’ll be working with a fictional dataset representing a company’s customer data. Your task is to predict customer churn, i.e., the likelihood of a customer leaving the company’s services, based on their usage metrics and demographic information.\n\nDataset Description\n\nAge: Age of the customer (numeric)\nMonthlyCharge: Monthly charge for the services (numeric, in $)\nCustomerServiceCalls: Number of calls made to customer service (numeric)\nChurn: Whether the customer left the company within the last month (0 for No, 1 for Yes)\n\n\n\nObjective\nUsing the KNN algorithm, classify customers into ‘Churn’ or ‘No Churn’ based on the given features.\nLet’s get started!\n\n# Sample dataset\ndata = {\n    'Age': [25, 30, 35, 40, 45, 50, 55, 60, 65, 70],\n    'MonthlyCharge': [50, 55, 60, 65, 70, 75, 80, 85, 90, 95],\n    'CustomerServiceCalls': [1, 2, 1, 3, 2, 3, 1, 2, 3, 1],\n    'Churn': [0, 0, 0, 1, 0, 1, 0, 1, 1, 0]\n}\n\n# Convert the dictionary to a DataFrame for better visualization\nimport pandas as pd\ndf = pd.DataFrame(data)\ndf\n\n\n\nData Preparation\nBefore applying the KNN algorithm, we need to prepare our data. This involves:\n\nSplitting the data into features (X) and target label (y). In our case, ‘Age’, ‘MonthlyCharge’, and ‘CustomerServiceCalls’ are our features, and ‘Churn’ is our target label.\nSplitting the dataset into training and testing sets. This allows us to train our model on one subset and test its performance on another unseen subset.\n\nLet’s perform these steps.\n\nfrom sklearn.model_selection import train_test_split\n\n# Splitting the data into features (X) and target label (y)\nX = df[['Age', 'MonthlyCharge', 'CustomerServiceCalls']]\ny = df['Churn']\n\n# Splitting the dataset into training and testing sets (80% training, 20% testing)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nX_train.head(), y_train.head()\n\n\n\nApplying the KNN Algorithm\nNow that our data is prepared, we can apply the KNN algorithm. Here are the steps we’ll follow:\n\nInitialize the KNN Classifier: We’ll use the KNeighborsClassifier from sklearn to create our KNN model. The key parameter here is n_neighbors, which represents the number of neighbors to consider when making a prediction.\nTrain the Classifier: We’ll use the fit method to train our KNN classifier on the training data.\nMake Predictions: Once trained, we can use the predict method to make predictions on new, unseen data.\n\nLet’s go through each step with code and explanations.\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# Step 1: Initialize the KNN Classifier\n# We'll start with 3 neighbors for our classifier\nknn = KNeighborsClassifier(n_neighbors=3)\n\n# Step 2: Train the Classifier on the training data\nknn.fit(X_train, y_train)\n\n# Displaying the trained classifier\nknn\n\n\n\nMaking Predictions with the KNN Classifier\nNow that our KNN classifier is trained, we can use it to make predictions on new data. For this exercise, we’ll predict the churn for the test data and then compare these predictions to the actual churn values to evaluate the performance of our model.\nTo make predictions, we’ll use the predict method of our trained KNN classifier. This method takes in the features of the data we want to predict and returns the predicted labels.\nLet’s make predictions on our test data and display the results.\n\n# Step 3: Make Predictions on the test data\ny_pred = knn.predict(X_test)\n\n# Displaying the predicted churn values for the test data\ny_pred\n\n\nfrom sklearn.metrics import accuracy_score\n\n# Calculate the accuracy of the model\naccuracy = accuracy_score(y_test, y_pred)\n\n# Displaying the accuracy\naccuracy"
  },
  {
    "objectID": "machine_learning_egei/class3.html#overfitting-and-underfitting-bias-versus-variance",
    "href": "machine_learning_egei/class3.html#overfitting-and-underfitting-bias-versus-variance",
    "title": "Machine Learning Models",
    "section": "Overfitting and Underfitting: Bias versus Variance",
    "text": "Overfitting and Underfitting: Bias versus Variance\nIn machine learning, achieving the right balance between bias and variance is crucial for creating models that generalize well to new, unseen data. Let’s delve into these concepts:\n\nBias: Refers to the error introduced by approximating a real-world problem (which may be complex) by a too-simple model. High bias can cause the model to miss relevant relations between features and target outputs, leading to underfitting.\nVariance: Refers to the error introduced by using a model that’s too complex. High variance can cause the model to model the random noise in the training data, leading to overfitting.\n\n\nUnderfitting:\nOccurs when a model is too simple to capture the underlying structure of the data. Such a model has high bias and low variance.\n\n\nOverfitting:\nOccurs when a model is too complex and fits the training data too closely, including its noise and outliers. Such a model has low bias and high variance.\nThe goal in machine learning is to achieve a balance between bias and variance, ensuring that the model is flexible enough to model the data’s structure but not so flexible that it fits the noise in the data.\nLet’s visualize the concepts of overfitting and underfitting.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\n\n# Generate sample data\nnp.random.seed(0)\nX = np.sort(5 * np.random.rand(80, 1), axis=0)\ny = np.sin(X).ravel() + np.random.normal(0, 0.1, X.shape[0])\n\n# Create polynomial regression models of different degrees\ndegrees = [1, 4, 15]\nplt.figure(figsize=(14, 5))\nfor i in range(len(degrees)):\n    ax = plt.subplot(1, len(degrees), i + 1)\n    plt.setp(ax, xticks=(), yticks=())\n\n    polynomial_features = PolynomialFeatures(degree=degrees[i], include_bias=False)\n    linear_regression = LinearRegression()\n    pipeline = make_pipeline(polynomial_features, linear_regression)\n    pipeline.fit(X, y)\n\n    # Evaluate the models using cross-validation\n    X_test = np.linspace(0, 5, 100)\n    plt.plot(X_test, pipeline.predict(X_test[:, np.newaxis]), label=\"Model\")\n    plt.scatter(X, y, edgecolor='b', s=20, label=\"Samples\")\n    plt.xlabel(\"x\")\n    plt.ylabel(\"y\")\n    plt.xlim((0, 5))\n    plt.ylim((-2, 2))\n    plt.legend(loc=\"best\")\n    plt.title(\"Degree %d\" % degrees[i])\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "machine_learning_egei/class3.html#the-cost-function",
    "href": "machine_learning_egei/class3.html#the-cost-function",
    "title": "Machine Learning Models",
    "section": "The Cost Function",
    "text": "The Cost Function\nIn machine learning, the cost function (or loss function) quantifies how well the model’s predictions match the actual values. In other words, it measures the error of the model. The goal during training is to minimize this error.\nFor linear regression, a common cost function is the Mean Squared Error (MSE), which calculates the average of the squared differences between the predicted and actual values.\nThe formula for MSE is:\n\\[ MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 \\]\nWhere: - \\(y_i\\) is the actual value. - \\(\\hat{y}_i\\) is the predicted value. - $ n $ is the number of observations.\nLet’s implement the MSE in Python and calculate it for a sample set of actual and predicted values.\n\nfrom sklearn.metrics import mean_squared_error\n# Sample actual and predicted values\ny_true = np.array([3, 2.5, 4, 5.6])\ny_pred = np.array([2.8, 2.7, 3.8, 5.5])\n\n# Calculate MSE\nmse_value = mean_squared_error(y_true, y_pred)\nmse_value"
  },
  {
    "objectID": "machine_learning_egei/class3.html#the-training-error-and-the-test-error",
    "href": "machine_learning_egei/class3.html#the-training-error-and-the-test-error",
    "title": "Machine Learning Models",
    "section": "The Training Error and The Test Error",
    "text": "The Training Error and The Test Error\nIn machine learning, it’s essential to evaluate how well a model performs. Two common metrics used for this purpose are the training error and the test error:\n\nTraining Error: This is the error (typically the MSE) of the model on the same data it was trained on. A low training error might indicate that the model fits the training data well, but it doesn’t necessarily mean the model will perform well on new, unseen data.\nTest Error: This is the error of the model on a separate set of data that it hasn’t seen during training. It gives a better indication of how the model will perform in real-world scenarios. A model that performs well on the training data but poorly on the test data is likely overfitting.\n\nIn a business context, the training error can help in understanding how well the model fits historical data, while the test error can provide insights into how the model might perform on future data.\nLet’s calculate the training and test errors for a sample linear regression model using a business-related dataset.\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\n\n# Generate sample business data: Sales vs. Advertising Spend\nnp.random.seed(0)\nX = 2.5 * np.random.rand(100, 1)\ny = 5 + 3 * X + np.random.randn(100, 1)\n\n# Split the data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n\n# Train a linear regression model\nregressor = LinearRegression()\nregressor.fit(X_train, y_train)\n\n# Calculate the training error\ny_train_pred = regressor.predict(X_train)\ntraining_error = mean_squared_error(y_train, y_train_pred)\n\n# Calculate the test error\ny_test_pred = regressor.predict(X_test)\ntest_error = mean_squared_error(y_test, y_test_pred)\n\ntraining_error, test_error"
  },
  {
    "objectID": "machine_learning_egei/class3.html#the-cost-function-revisited",
    "href": "machine_learning_egei/class3.html#the-cost-function-revisited",
    "title": "Machine Learning Models",
    "section": "The Cost Function (Revisited)",
    "text": "The Cost Function (Revisited)\nThe cost function, as previously mentioned, quantifies the error between the model’s predictions and the actual values. For regression problems, the Mean Squared Error (MSE) is a commonly used cost function. It’s crucial to minimize this error during the training process to ensure the model makes accurate predictions.\nIn a business context, the cost function can be thought of as a measure of how far off our predictions are from the actual outcomes. For instance, if we’re predicting monthly sales, a high cost would indicate that our predictions are far from the actual sales figures, which could lead to incorrect business decisions.\nNow, let’s move on to the next topic."
  },
  {
    "objectID": "machine_learning_egei/class3.html#bias-versus-variance-revisited",
    "href": "machine_learning_egei/class3.html#bias-versus-variance-revisited",
    "title": "Machine Learning Models",
    "section": "Bias versus Variance (Revisited)",
    "text": "Bias versus Variance (Revisited)\nBias and variance are two fundamental concepts in understanding the performance of machine learning models. They represent two types of errors that can occur:\n\nBias: This is the error introduced by approximating a real-world problem, which might be complex, by a too-simple model. High bias can cause the model to miss the relevant relations between features and target outputs, leading to underfitting. In a business context, a high-bias model might consistently make the same type of error, such as consistently underestimating sales by a certain amount.\nVariance: This is the error introduced by using a model that’s too complex. High variance can cause the model to model the random noise in the training data, leading to overfitting. In a business context, a high-variance model might be very sensitive to small fluctuations in the training data, leading to erratic predictions.\n\nThe challenge in machine learning is to find the right trade-off between bias and variance. Ideally, we want a model with low bias and low variance, but in practice, there’s often a trade-off. Reducing bias might increase variance and vice versa.\nIn a business scenario, understanding bias and variance is crucial. A model with high bias might lead to consistent errors in decision-making, while a model with high variance might lead to unpredictable and erratic decisions.\nNow, let’s move on to the next topic.\n\n# Generate sample business data: Sales vs. Advertising Spend\nnp.random.seed(42)\nX = 2.5 * np.random.rand(100, 1)\ny = 5 + 3 * X + np.random.randn(100, 1)\n\n# Split the data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train a linear regression model\nregressor = LinearRegression()\nregressor.fit(X_train, y_train)\n\n# Calculate the training error\ny_train_pred = regressor.predict(X_train)\ntraining_error = mean_squared_error(y_train, y_train_pred)\n\n# Calculate the test error\ny_test_pred = regressor.predict(X_test)\ntest_error = mean_squared_error(y_test, y_test_pred)\n\ntraining_error, test_error"
  },
  {
    "objectID": "machine_learning_egei/class3.html#exercise-overfitting-and-underfitting-in-business-sales-prediction",
    "href": "machine_learning_egei/class3.html#exercise-overfitting-and-underfitting-in-business-sales-prediction",
    "title": "Machine Learning Models",
    "section": "Exercise: Overfitting and Underfitting in Business Sales Prediction",
    "text": "Exercise: Overfitting and Underfitting in Business Sales Prediction\nScenario: You are a data scientist at a retail company, and you are tasked with predicting monthly sales based on advertising spend. You decide to use polynomial regression. However, you want to ensure that your model neither overfits nor underfits the data.\nObjective: Fit polynomial regression models of varying degrees to the sales data and visualize the results to understand the concepts of overfitting and underfitting.\nLet’s start by visualizing the provided sales data.\n\n# Generate sample business data: Sales vs. Advertising Spend\nnp.random.seed(0)\nX = 2.5 * np.random.rand(100, 1)\ny = 5 + 3 * X + np.random.randn(100, 1)\n\n# Visualize the data\nplt.figure(figsize=(10, 6))\nplt.scatter(X, y, color='blue', s=30)\nplt.title('Sales vs. Advertising Spend')\nplt.xlabel('Advertising Spend (in thousands)')\nplt.ylabel('Sales (in thousands)')\nplt.grid(True)\nplt.show()\n\n\n# Split the data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n\n# Display the size of the training and test sets\nlen(X_train), len(X_test)\n\n\n# Fit polynomial regression models of varying degrees and visualize the results\ndegrees = [1, 4, 15]\nplt.figure(figsize=(14, 5))\nfor i in range(len(degrees)):\n    ax = plt.subplot(1, len(degrees), i + 1)\n    plt.setp(ax, xticks=(), yticks=())\n\n    polynomial_features = PolynomialFeatures(degree=degrees[i], include_bias=False)\n    linear_regression = LinearRegression()\n    pipeline = make_pipeline(polynomial_features, linear_regression)\n    pipeline.fit(X_train, y_train)\n\n    # Visualize the models\n    X_range = np.linspace(0, 2.5, 100)\n    plt.plot(X_range, pipeline.predict(X_range[:, np.newaxis]), label=\"Model\")\n    plt.scatter(X_train, y_train, edgecolor='b', s=20, label=\"Training Data\")\n    plt.scatter(X_test, y_test, edgecolor='r', s=20, label=\"Test Data\")\n    plt.xlabel(\"Advertising Spend (in thousands)\")\n    plt.ylabel(\"Sales (in thousands)\")\n    plt.xlim((0, 2.5))\n    plt.ylim((0, 15))\n    plt.legend(loc=\"best\")\n    plt.title(\"Degree %d\" % degrees[i])\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "machine_learning_egei/class3.html#exercise-understanding-the-cost-function-in-business",
    "href": "machine_learning_egei/class3.html#exercise-understanding-the-cost-function-in-business",
    "title": "Machine Learning Models",
    "section": "Exercise: Understanding the Cost Function in Business",
    "text": "Exercise: Understanding the Cost Function in Business\nScenario: You are working for a retail company and are tasked with predicting monthly sales based on advertising spend. You’ve chosen a linear regression model for this task. To evaluate the performance of your model, you decide to compute the Mean Squared Error (MSE) as your cost function.\nObjective: Calculate the MSE for your linear regression model using both the training and test data. Compare the results to understand the model’s performance.\nLet’s begin by calculating the MSE for the training data.\n\n# Calculate the MSE for the training data\ny_train_pred = regressor.predict(X_train)\nmse_train = mean_squared_error(y_train, y_train_pred)\nmse_train\n\n\n# Calculate the MSE for the test data\ny_test_pred = regressor.predict(X_test)\nmse_test = mean_squared_error(y_test, y_test_pred)\nmse_test"
  },
  {
    "objectID": "machine_learning_egei/class3.html#exercise-evaluating-model-performance-using-training-and-test-errors",
    "href": "machine_learning_egei/class3.html#exercise-evaluating-model-performance-using-training-and-test-errors",
    "title": "Machine Learning Models",
    "section": "Exercise: Evaluating Model Performance using Training and Test Errors",
    "text": "Exercise: Evaluating Model Performance using Training and Test Errors\nScenario: You are a data scientist at a retail company. The marketing team wants to understand the performance of the sales prediction model before launching a new advertising campaign. They are particularly interested in knowing how well the model performs on historical data (training data) and how it might perform on future data (test data).\nObjective: Calculate the training and test errors for the sales prediction model. Analyze the results to provide insights to the marketing team.\nLet’s begin by calculating the training and test errors.\n\n# Calculate the training error\ntraining_error = mean_squared_error(y_train, y_train_pred)\n\n# Calculate the test error\ntest_error = mean_squared_error(y_test, y_test_pred)\n\ntraining_error, test_error"
  },
  {
    "objectID": "machine_learning_egei/class2.html",
    "href": "machine_learning_egei/class2.html",
    "title": "DataFrames",
    "section": "",
    "text": "A DataFrame is a two-dimensional, size-mutable, and heterogeneous tabular data structure with labeled axes (rows and columns). It is one of the most commonly used data structures in data analysis and machine learning, especially when dealing with structured data."
  },
  {
    "objectID": "machine_learning_egei/class2.html#example-1-fictitious-data",
    "href": "machine_learning_egei/class2.html#example-1-fictitious-data",
    "title": "DataFrames",
    "section": "Example 1: Fictitious Data",
    "text": "Example 1: Fictitious Data\n\nCreating a Fictitious Data\n\nimport pandas as pd\nimport numpy as np\n\n# Creating a fictitious sales data\nnp.random.seed(42)\ndata = {\n    'Product ID': np.arange(1, 101),\n    'Product Name': ['Product_' + str(i) for i in range(1, 101)],\n    'Category': np.random.choice(['Electronics', 'Clothing', 'Groceries', 'Household'], 100),\n    'Price': np.random.uniform(10, 500, 100).round(2),\n    'Units Sold': np.random.randint(1, 100, 100),\n    'Date of Sale': pd.date_range(start='2022-01-01', periods=100, freq='D')\n}\n\n# Introducing some missing values\nfor _ in range(10):\n    data['Price'][np.random.randint(0, 100)] = np.nan\n    data['Units Sold'][np.random.randint(0, 100)] = np.nan\n\nsales_df = pd.DataFrame(data)\nsales_df.head()\n\n\n\nData Cleaning\n\n# Checking for missing values\nmissing_values = sales_df.isnull().sum()\nmissing_values\n\n\n# Handling missing values by filling with the mean of the column\nsales_df['Price'].fillna(sales_df['Price'].mean(), inplace=True)\nsales_df['Units Sold'].fillna(sales_df['Units Sold'].mean(), inplace=True)\n\n# Verifying if there are any missing values left\nsales_df.isnull().sum()\n\n\n\nData Exploration\n\n# Getting a summary of the data\nsales_summary = sales_df.describe()\nsales_summary\n\n\nimport matplotlib.pyplot as plt\n\n# Visualizing the distribution of 'Price'\nplt.figure(figsize=(10, 6))\nsales_df['Price'].hist(bins=30, color='skyblue', edgecolor='black')\nplt.title('Distribution of Product Prices')\nplt.xlabel('Price')\nplt.ylabel('Number of Products')\nplt.grid(False)\nplt.show()\n\n\n\nFeature Engineering\n\n# Creating a new feature 'Revenue' which is 'Price' multiplied by 'Units Sold'\nsales_df['Revenue'] = sales_df['Price'] * sales_df['Units Sold']\n\n# Creating a feature 'Month of Sale' extracted from 'Date of Sale'\nsales_df['Month of Sale'] = sales_df['Date of Sale'].dt.month\n\nsales_df[['Product Name', 'Price', 'Units Sold', 'Revenue', 'Month of Sale']].head()"
  },
  {
    "objectID": "machine_learning_egei/class2.html#example-2-fortune-data",
    "href": "machine_learning_egei/class2.html#example-2-fortune-data",
    "title": "DataFrames",
    "section": "Example 2: Fortune Data",
    "text": "Example 2: Fortune Data\n\nReading a CSV File\n\n# Importing necessary libraries\nimport pandas as pd\n\n# Reading a sample CSV file related to business and economy\n# For this example, I'll use a dataset about Fortune 500 companies\nurl = 'https://raw.githubusercontent.com/hizocar/datasets/main/fortune500.csv'\nfortune_df = pd.read_csv(url, sep=',', on_bad_lines='skip')\n\n# Displaying the first few rows of the dataset using .head()\nfortune_df.head()\n\nUnable to display output for mime type(s): application/vnd.dataresource+json\n\n\n\n\nDescripción de las Columnas\n\nYear: Esta columna representa el año en el que se registraron los datos de la empresa. Es de tipo int64, lo que significa que contiene valores enteros.\nName: Esta columna contiene el nombre de la empresa. Es de tipo object, lo que generalmente indica que contiene cadenas de texto.\nRevenue: Esta columna representa los ingresos de la empresa en millones. Es de tipo float64, lo que indica que contiene valores decimales.\n\nCada columna tiene un significado específico y es esencial para el análisis de las empresas Fortune 500 y su rendimiento a lo largo de los años.\n\n# Checking the data types of each column \ndata_types = fortune_df.dtypes\ndata_types\n\nUnable to display output for mime type(s): application/vnd.dataresource+json\n\n\n\n\nData Cleaning: Transforming the ‘Revenue’ Column\n\n# Replacing '-' with 0\nfortune_df['Revenue'] = fortune_df['Revenue'].replace('-', '0')\n\n# Removing any remaining non-numeric characters and converting to float\nfortune_df['Revenue'] = fortune_df['Revenue'].replace('[\\$,]', '', regex=True).astype(float)\n\n# Checking the data types of each column again\ndata_types_updated = fortune_df.dtypes\ndata_types_updated\n\nUnable to display output for mime type(s): application/vnd.dataresource+json\n\n\n\n# Standardizing column names to lowercase and replacing spaces with underscores\nfortune_df.columns = fortune_df.columns.str.lower().str.replace(' ', '_')\n\n# Displaying the updated column names\nfortune_df.columns\n\nIndex(['year', 'name', 'revenue', 'rank'], dtype='object')\n\n\n\n\nData Exploration\n\n# Checking for missing values in the dataframe\nmissing_values = fortune_df.isnull().sum()\nmissing_values\n\nUnable to display output for mime type(s): application/vnd.dataresource+json\n\n\nFrom the initial check for missing values, it appears that there are no missing values in any of the columns of the dataframe. This is great as it means we don’t have to perform any imputation or data filling for this dataset.\nNext, let’s use the .describe() method to get a summary of the distribution of the numerical data.\n\n# Using .describe() to get a summary of the numerical columns\ndata_summary = fortune_df.describe()\ndata_summary\n\nUnable to display output for mime type(s): application/vnd.dataresource+json\n\n\nThe .describe() method provides a summary of the numerical columns in the dataframe. Here are some insights from the summary:\n\nYear: The data spans from 1955 to 2021.\nRevenue: The average revenue of the companies listed is approximately 8,488.47 million. The minimum revenue is 0 million (which might indicate missing or unreported data for some companies), and the maximum revenue is a staggering 559,151 million. The 25th percentile is 480.45 million, the median (50th percentile) is 2,184.2 million, and the 75th percentile is 7,829.15 million.\nRank: The rank ranges from 1 to 500, which is expected for a list of the top 500 companies.\n\nNext, let’s check the shape of the dataframe to understand its dimensions and then explore the unique values in the non-numerical columns.\n\n# Checking the shape of the dataframe\ndata_shape = fortune_df.shape\n\n# Checking unique values in the 'name' column\nunique_companies = fortune_df['name'].nunique()\n\ndata_shape, unique_companies\n\n((33500, 4), 2273)\n\n\nHere are some additional insights from our exploration:\n\nThe dataframe has 33,500 rows and 4 columns. This means we have data for 33,500 entries across the 4 columns.\nThe name column, which represents the names of the companies, has 2,273 unique values. This indicates that many companies have appeared on the list multiple times over the years.\n\nGiven the size of the dataset and the number of unique companies, there’s a wealth of information to explore further, such as trends over time, the distribution of revenues among the top companies, and more.\n\n# Importing necessary libraries for data visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Setting the style for the plots\nsns.set_style('whitegrid')\n\n# Plotting the distribution of the 'year' column\nplt.figure(figsize=(14, 6))\nsns.countplot(x='year', data=fortune_df, palette='viridis')\nplt.title('Distribution of Companies by Year')\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n# Plotting the distribution of the 'revenue' column\nplt.figure(figsize=(14, 6))\nsns.histplot(fortune_df['revenue'], bins=50, color='blue', kde=True)\nplt.title('Distribution of Revenue')\nplt.xlabel('Revenue (in millions)')\nplt.ylabel('Frequency')\nplt.tight_layout()\nplt.show()\n\n\n\n\nNow, let’s explore the relationships between the numerical variables. One of the best ways to visualize relationships between numerical variables is by using a correlation heatmap. This will allow us to see if there are any strong linear relationships between the variables in our dataset.\n\n# Calculating the correlation matrix\ncorrelation_matrix = fortune_df.corr()\n\n# Plotting the correlation heatmap\nplt.figure(figsize=(10, 6))\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5, vmin=-1, vmax=1)\nplt.title('Correlation Heatmap')\nplt.tight_layout()\nplt.show()\n\n\n\n\nFrom the correlation heatmap, we can observe the following:\n\nYear and Rank: There’s a negative correlation between the year and the rank. This might indicate that over the years, the ranking criteria or the companies in the list have changed.\nRevenue and Rank: There’s also a negative correlation between revenue and rank. This is expected since a higher revenue would generally mean a higher rank (or a lower rank number) on the Fortune 500 list.\n\nIt’s important to note that correlation does not imply causation. While these variables might be correlated, it doesn’t mean that one causes the other. The heatmap provides a quick overview of potential relationships that might be worth exploring further.\n\n# Grouping the data by company and year\ngrouped_data = fortune_df.groupby(['name', 'year']).mean().reset_index()\n\n# Plotting the correlation heatmap for the grouped data\ncorrelation_matrix_grouped = grouped_data.corr()\nplt.figure(figsize=(10, 6))\nsns.heatmap(correlation_matrix_grouped, annot=True, cmap='coolwarm', linewidths=0.5, vmin=-1, vmax=1)\nplt.title('Correlation Heatmap (Grouped Data)')\nplt.tight_layout()\nplt.show()\n\n\n\n\nAfter grouping the data by company and year, the correlation heatmap for the grouped data shows similar insights as before. The negative correlation between year and rank and between revenue and rank remains consistent.\nNext, let’s visualize the revenue evolution over the years for three companies. We’ll choose ‘Walmart’, ‘Exxon Mobil’, and ‘Apple’ as our sample companies for this analysis.\n\n# Filtering data for the three chosen companies\ncompanies = ['Walmart', 'Exxon Mobil', 'Apple']\nfiltered_data = grouped_data[grouped_data['name'].isin(companies)]\n\n# Plotting the revenue evolution over the years for the three companies\nplt.figure(figsize=(14, 7))\nsns.lineplot(data=filtered_data, x='year', y='revenue', hue='name', palette='tab10', marker='o')\nplt.title('Revenue Evolution Over the Years')\nplt.xlabel('Year')\nplt.ylabel('Revenue (in millions)')\nplt.legend(title='Company')\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "machine_learning_egei/program.html",
    "href": "machine_learning_egei/program.html",
    "title": "Machine Learning – Business Applications",
    "section": "",
    "text": "Course title: Machine Learning – Business Applications\nECTS: 5\nSemester: 4\nLocation: Universidad Técnica Federico Santa María (Chile)\nCompulsory course: YES – Track GEME - Globalisation and Emerging Market Economies\nLecturer & Contact: Sebastián Azócar M. Master’s in Data Science\nEmail: hizocar@gmail.com\nPrerequisites: Applied Econometrics I and II\n\n\nThe general objective of the course is that students acquire the knowledge and practice necessary to Machine learning (ML). This is a branch of computer science that uses algorithms to mimic the way humans learn. In this course, we will analyze the different techniques and statistical methods used in ML to make predictions with Business Applications.\n\n\n\nThe methodology of the course is focused on learning by doing, so the individual work of each student is key (a study load of at least 3 hours per week is assumed), and each student is required to read the required material before each class.\n\n\n\n\n\n\n\nA. Data and Decision Making\n\nDifferent Types of Data\nData manipulation\n\n\n\n\n\n\nB.1 Machine Learning Models\n\nWhat is a ML Model?\nFitting a Model\nKNN\nPolynomial Regression\nOverfitting and Underfitting: Bias versus Variance\nThe Cost Function\nThe Training Error\nThe Test Error\n\nB.2 The Machine Learning Pipeline\n\nThe Bias-Variance Trade-Off\nCross-Validation\nApplying the machine learning pipeline\n\n\n\n\n\n\nC.1 Logistic Regression\n\nWhat is classification?\nTechnique and Methodology\nMeasuring the Model Performance\nThe ROC Curve\n\nC.2 Generative Models\n\nBasic Concepts\nThe Naïve Bayesian Classifier\nText Classification\nNLP Application: Measuring Text Sentiment\n\n\n\n\n\n\nD.1 Tree based Methods\n\nStructure of decision trees\nTypes of tree-based methods\nLoss functions\nTree pruning\nRegression Trees\nClassification Trees\n\nD.2 Ensemble Methods\n\nBasic Concepts\nTechniques and Methodology\nBagging\nRandom Forests\nBoosting\n\n\n\n\n\n\nE.1 Variable Selection\n\nApplications to Variable Selections\nTechniques and Methodology\nBest subset selection\nStepwise, Backward and Forward Selection\n\nE.2 Shrinkage Methods\n\nShrinkage versus Selection\nLASSO Regression\nRIDGE Regression\nElastic Net Regression\n\n\n\n\n\n\nF.1 Dimension Reduction\n\nUnlabeled data\nPrincipal Components\nApplication of PCA\n\nF.2 Clustering\n\nk-means clustering\nHierarchical clustering\nAdvantages and Limitations\nPractical Application\n\n\n\n\n\n\nG.1 Neural Networks\n\nBasic concepts\nArtificial neural networks (ANNs)\nThe simple perceptron\nStructure of the ANN\nMethods\n\nG.2 Neural Networks Implementation\n\nIntroduction\nAdvantages and Limitations\nTraining the Model\nModel Optimization\n\n\n\n\n\n\n\n\n\nBreiman, L. (2001). Statistical modeling: The two cultures (with comments and a rejoinder by the author). Statistical science, 16(3), 199-231.\nBurgess, M. (2018). This is how Netflix’s secret recommendation system works. Wired.\nCastañón, J. (10). Machine Learning Methods that Every Data Scientist Should Know, 2019.\nPant, A. (2019). Introduction to Machine Learning for Beginners. Preuzeto, 19, 2021.\nZhang (2018). Data Types From A Machine Learning Perspective With Examples.\n\n\n\n\n\nAgarwal, A. (2018). Polynomial Regression\nKoehrsen, W. (2018). Overfitting vs. Underfitting: A Complete Example\nGupta, P. (2017). Cross-Validation in Machine Learning\n\n\n\n\n\nISLR sections 6.1 (Subset selection), 6.2 (Shrinkage methods)\nLesson 4: Variable Selection\nLesson 5: Shrinkage Methods\nDeol, G. (2019)\n\n\n\n\n\nISLR sections 4.1, 4.2, 4.3, 4.6.2\nLesson 9.1: Logistic Regression\nAsiri, S.(2018)\n\n\n\n\n\nISLR Chapter 8\nLesson 11: Tree-based Methods\n[Analytics Vidha (2016). Tree Based Algorithms: A Complete Tutorial from Scratch (in R & Python)](https://www.analyticsvidhya.com/blog/2016/04/tree-based-algorithms-complete-tutorial-s"
  },
  {
    "objectID": "machine_learning_egei/program.html#course-syllabus",
    "href": "machine_learning_egei/program.html#course-syllabus",
    "title": "Machine Learning – Business Applications",
    "section": "",
    "text": "Course title: Machine Learning – Business Applications\nECTS: 5\nSemester: 4\nLocation: Universidad Técnica Federico Santa María (Chile)\nCompulsory course: YES – Track GEME - Globalisation and Emerging Market Economies\nLecturer & Contact: Sebastián Azócar M. Master’s in Data Science\nEmail: hizocar@gmail.com\nPrerequisites: Applied Econometrics I and II\n\n\nThe general objective of the course is that students acquire the knowledge and practice necessary to Machine learning (ML). This is a branch of computer science that uses algorithms to mimic the way humans learn. In this course, we will analyze the different techniques and statistical methods used in ML to make predictions with Business Applications.\n\n\n\nThe methodology of the course is focused on learning by doing, so the individual work of each student is key (a study load of at least 3 hours per week is assumed), and each student is required to read the required material before each class."
  },
  {
    "objectID": "machine_learning_egei/program.html#course-contents",
    "href": "machine_learning_egei/program.html#course-contents",
    "title": "Machine Learning – Business Applications",
    "section": "",
    "text": "A. Data and Decision Making\n\nDifferent Types of Data\nData manipulation\n\n\n\n\n\n\nB.1 Machine Learning Models\n\nWhat is a ML Model?\nFitting a Model\nKNN\nPolynomial Regression\nOverfitting and Underfitting: Bias versus Variance\nThe Cost Function\nThe Training Error\nThe Test Error\n\nB.2 The Machine Learning Pipeline\n\nThe Bias-Variance Trade-Off\nCross-Validation\nApplying the machine learning pipeline\n\n\n\n\n\n\nC.1 Logistic Regression\n\nWhat is classification?\nTechnique and Methodology\nMeasuring the Model Performance\nThe ROC Curve\n\nC.2 Generative Models\n\nBasic Concepts\nThe Naïve Bayesian Classifier\nText Classification\nNLP Application: Measuring Text Sentiment\n\n\n\n\n\n\nD.1 Tree based Methods\n\nStructure of decision trees\nTypes of tree-based methods\nLoss functions\nTree pruning\nRegression Trees\nClassification Trees\n\nD.2 Ensemble Methods\n\nBasic Concepts\nTechniques and Methodology\nBagging\nRandom Forests\nBoosting\n\n\n\n\n\n\nE.1 Variable Selection\n\nApplications to Variable Selections\nTechniques and Methodology\nBest subset selection\nStepwise, Backward and Forward Selection\n\nE.2 Shrinkage Methods\n\nShrinkage versus Selection\nLASSO Regression\nRIDGE Regression\nElastic Net Regression\n\n\n\n\n\n\nF.1 Dimension Reduction\n\nUnlabeled data\nPrincipal Components\nApplication of PCA\n\nF.2 Clustering\n\nk-means clustering\nHierarchical clustering\nAdvantages and Limitations\nPractical Application\n\n\n\n\n\n\nG.1 Neural Networks\n\nBasic concepts\nArtificial neural networks (ANNs)\nThe simple perceptron\nStructure of the ANN\nMethods\n\nG.2 Neural Networks Implementation\n\nIntroduction\nAdvantages and Limitations\nTraining the Model\nModel Optimization"
  },
  {
    "objectID": "machine_learning_egei/program.html#readings-literature",
    "href": "machine_learning_egei/program.html#readings-literature",
    "title": "Machine Learning – Business Applications",
    "section": "",
    "text": "Breiman, L. (2001). Statistical modeling: The two cultures (with comments and a rejoinder by the author). Statistical science, 16(3), 199-231.\nBurgess, M. (2018). This is how Netflix’s secret recommendation system works. Wired.\nCastañón, J. (10). Machine Learning Methods that Every Data Scientist Should Know, 2019.\nPant, A. (2019). Introduction to Machine Learning for Beginners. Preuzeto, 19, 2021.\nZhang (2018). Data Types From A Machine Learning Perspective With Examples.\n\n\n\n\n\nAgarwal, A. (2018). Polynomial Regression\nKoehrsen, W. (2018). Overfitting vs. Underfitting: A Complete Example\nGupta, P. (2017). Cross-Validation in Machine Learning\n\n\n\n\n\nISLR sections 6.1 (Subset selection), 6.2 (Shrinkage methods)\nLesson 4: Variable Selection\nLesson 5: Shrinkage Methods\nDeol, G. (2019)\n\n\n\n\n\nISLR sections 4.1, 4.2, 4.3, 4.6.2\nLesson 9.1: Logistic Regression\nAsiri, S.(2018)\n\n\n\n\n\nISLR Chapter 8\nLesson 11: Tree-based Methods\n[Analytics Vidha (2016). Tree Based Algorithms: A Complete Tutorial from Scratch (in R & Python)](https://www.analyticsvidhya.com/blog/2016/04/tree-based-algorithms-complete-tutorial-s"
  },
  {
    "objectID": "machine_learning_egei/homeworks/Python_Basics_Homework.html",
    "href": "machine_learning_egei/homeworks/Python_Basics_Homework.html",
    "title": "Python Basics Homework",
    "section": "",
    "text": "Welcome to your Python Basics Homework! This notebook is designed to help you practice and improve your Python programming skills. Please follow the instructions carefully and complete the tasks below."
  },
  {
    "objectID": "machine_learning_egei/homeworks/Python_Basics_Homework.html#instructions",
    "href": "machine_learning_egei/homeworks/Python_Basics_Homework.html#instructions",
    "title": "Python Basics Homework",
    "section": "Instructions",
    "text": "Instructions\n\nRead each problem statement carefully.\nWrite the Python code to solve each problem in the designated cell.\nDo not delete or modify any pre-existing code or comments unless instructed to do so.\nRun your code to ensure it works as expected and you have solved the problem correctly.\nSave your notebook once you have completed the homework."
  },
  {
    "objectID": "machine_learning_egei/homeworks/Python_Basics_Homework.html#problems",
    "href": "machine_learning_egei/homeworks/Python_Basics_Homework.html#problems",
    "title": "Python Basics Homework",
    "section": "Problems",
    "text": "Problems\nThis homework consists of problems that cover basic Python operations, functions, and libraries. You will practice using basic operations, creating functions, working with lists and strings, and using the random and pandas libraries.\nLet’s get started!"
  },
  {
    "objectID": "machine_learning_egei/homeworks/Python_Basics_Homework.html#problem-1-basic-operations-and-functions",
    "href": "machine_learning_egei/homeworks/Python_Basics_Homework.html#problem-1-basic-operations-and-functions",
    "title": "Python Basics Homework",
    "section": "Problem 1: Basic Operations and Functions",
    "text": "Problem 1: Basic Operations and Functions\n\nPart 1.1: Basic Operations\nPerform the following operations and print the results:\n\nAdd 15 and 23.\nSubtract 50 from 200.\nMultiply 25 by 4.\nDivide 100 by 3 (get the result as a float number).\n\n\n\nPart 1.2: Creating and Using Functions\nCreate a function named calculate_area that takes the base and height of a triangle as parameters and returns its area. The formula to calculate the area of a triangle is (base * height) / 2.\nThen, call this function with base=10 and height=5 and print the result."
  },
  {
    "objectID": "machine_learning_egei/homeworks/Python_Basics_Homework.html#problem-2-working-with-lists-and-strings",
    "href": "machine_learning_egei/homeworks/Python_Basics_Homework.html#problem-2-working-with-lists-and-strings",
    "title": "Python Basics Homework",
    "section": "Problem 2: Working with Lists and Strings",
    "text": "Problem 2: Working with Lists and Strings\n\nPart 2.1: Lists\nYou have a list of numbers: numbers = [10, 20, 30, 40, 50].\n\nAppend the number 60 to the list.\nInsert the number 25 between 20 and 30.\nRemove the number 40 from the list.\nPrint the modified list.\n\n\n\nPart 2.2: Strings\nYou have a string: sentence = 'Python is fun!'.\n\nConvert the string to uppercase and print it.\nCount and print the number of ‘n’ characters in the string.\nReplace ‘fun’ with ‘awesome’ in the string and print the result."
  },
  {
    "objectID": "machine_learning_egei/homeworks/Python_Basics_Homework.html#problem-3-using-libraries",
    "href": "machine_learning_egei/homeworks/Python_Basics_Homework.html#problem-3-using-libraries",
    "title": "Python Basics Homework",
    "section": "Problem 3: Using Libraries",
    "text": "Problem 3: Using Libraries\n\nPart 3.1: Importing Libraries and Using the random Library\n\nImport the random library.\nGenerate and print a random integer between 1 and 100 using the random.randint() function.\nGenerate and print a random float number between 0 and 1 using the random.random() function.\n\n\n\nPart 3.2: Introduction to the pandas Library\n\nImport the pandas library with the alias pd.\nCreate a dictionary with two keys: ‘Name’ and ‘Age’. The ‘Name’ key should have a list of names as values, and the ‘Age’ key should have a list of ages as values.\nCreate a pandas DataFrame from the dictionary and print it.\nPrint the average age from the ‘Age’ column in the DataFrame."
  },
  {
    "objectID": "machine_learning_egei/homeworks/Final Project.html",
    "href": "machine_learning_egei/homeworks/Final Project.html",
    "title": "Final Project",
    "section": "",
    "text": "The project is to be carried out by groups of 3-4 students.\nIt must be presented in person on December 15th.\nThis project accounts for 40% of your final grade.\nThe presentation should last no longer than 20 minutes."
  },
  {
    "objectID": "machine_learning_egei/homeworks/Final Project.html#instructions",
    "href": "machine_learning_egei/homeworks/Final Project.html#instructions",
    "title": "Final Project",
    "section": "",
    "text": "The project is to be carried out by groups of 3-4 students.\nIt must be presented in person on December 15th.\nThis project accounts for 40% of your final grade.\nThe presentation should last no longer than 20 minutes."
  },
  {
    "objectID": "machine_learning_egei/homeworks/Final Project.html#development",
    "href": "machine_learning_egei/homeworks/Final Project.html#development",
    "title": "Final Project",
    "section": "Development",
    "text": "Development\nChoose a relevant dataset that includes at least three categorical columns and three numerical columns. Select a target variable for your predictions.\nYou must clean the data, analyze it, and select the appropriate variables for your model.\nTest the Logistic, KNN, Decision Tree, and Random Forest models. Create a comparative table of the results obtained, considering all metrics, including the AUC ROC.\nPerform K-means clustering on your data and explain the characteristics of each cluster."
  },
  {
    "objectID": "machine_learning_egei/homeworks/Final Project.html#grading-rubric",
    "href": "machine_learning_egei/homeworks/Final Project.html#grading-rubric",
    "title": "Final Project",
    "section": "Grading Rubric",
    "text": "Grading Rubric\nThe 100 points for this project will be divided as follows, with each category evaluated at three levels: Achieved, Partially Achieved, and Not Achieved.\n\nData Selection and Cleaning (20 points)\n\nAchieved (20 points): Relevant dataset with appropriate cleaning and preprocessing.\nPartially Achieved (10-15 points): Dataset is relevant but requires more thorough cleaning or preprocessing.\nNot Achieved (0-9 points): Dataset is not relevant or poorly cleaned/preprocessed.\n\nData Analysis and Variable Selection (20 points)\n\nAchieved (20 points): Comprehensive analysis and appropriate variable selection for the model.\nPartially Achieved (10-15 points): Basic analysis conducted; variable selection could be improved.\nNot Achieved (0-9 points): Inadequate analysis and poor variable selection.\n\nModel Implementation and Testing (30 points)\n\nAchieved (30 points): Accurate implementation and testing of all required models with a well-constructed comparative table.\nPartially Achieved (15-25 points): Implementation and testing of most models; comparative table lacks detail.\nNot Achieved (0-14 points): Incomplete model implementation or testing; no comparative table.\n\nK-means Clustering Analysis (20 points)\n\nAchieved (20 points): Effective use of K-means clustering with clear explanations of each cluster.\nPartially Achieved (10-15 points): Basic use of K-means clustering; explanations of clusters are vague.\nNot Achieved (0-9 points): Poor or no use of K-means clustering; no explanations of clusters.\n\nOral Presentation (10 points)\n\nAchieved (10 points): Clear, concise, and well-organized presentation within the time limit.\nPartially Achieved (5-9 points): Presentation is understandable but could be more organized or better timed.\nNot Achieved (0-4 points): Unclear or disorganized presentation; significantly exceeds or falls short of the time limit."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "\nHi 👋, I’m Sebastián Azócar\n",
    "section": "",
    "text": "Hi 👋, I’m Sebastián Azócar"
  },
  {
    "objectID": "index.html#check-my-schedule",
    "href": "index.html#check-my-schedule",
    "title": "\nHi 👋, I’m Sebastián Azócar\n",
    "section": "Check my Schedule",
    "text": "Check my Schedule"
  },
  {
    "objectID": "data_science_ingemat/programa.html",
    "href": "data_science_ingemat/programa.html",
    "title": "Programa",
    "section": "",
    "text": "Objetivos Generales:\n\nDesarrollar habilidades avanzadas en ciencia de datos aplicadas en Python.\nFamiliarizar a los estudiantes con técnicas avanzadas de aprendizaje automático.\nCapacitar a los estudiantes en el uso de herramientas avanzadas para el análisis de datos en tiempo real.\nPromover la capacidad de los estudiantes para aplicar técnicas de análisis de datos en problemas del mundo real.\n\n\n\nMódulo 1: Sistemas de Recomendación\n\nIntroducción a los sistemas de recomendación.\nFamiliarización con las técnicas de filtrado colaborativo y basado en contenido.\nImplementación de sistemas de recomendación utilizando Python y la biblioteca de ciencia de datos de Pandas.\nEvaluación del rendimiento de los sistemas de recomendación.\n\n\n\nMódulo 2: Series de Tiempo con Prophet\n\nIntroducción a las series de tiempo y sus aplicaciones en la ciencia de datos.\nFamiliarización con la biblioteca Prophet de Facebook para el análisis de series de tiempo.\nImplementación de técnicas avanzadas de análisis de series de tiempo, como la descomposición y la modelización de tendencias y estacionalidad.\nEvaluación del rendimiento del modelo.\n\n\n\nMódulo 3: Teoría de Colas aplicado en Python\n\nIntroducción a la teoría de colas y sus aplicaciones en la ciencia de datos.\nFamiliarización con las técnicas avanzadas de análisis de colas, como la ley de Little y la teoría de colas de Jackson.\nImplementación de modelos de teoría de colas utilizando Python y la biblioteca de ciencia de datos de NumPy.\nEvaluación del rendimiento del modelo.\n\n\n\nMetodología y estructura del curso:\n\nEl curso tendrá una duración de un semestre.\nCada semana, se realizará un quiz corto para evaluar el progreso de los estudiantes.\nLos estudiantes trabajarán en un proyecto de ciencia de datos a lo largo del semestre.\nLa evaluación será con una proporción del 50% promedio de quiz y un 50% proyecto."
  },
  {
    "objectID": "data_science_ingemat/modulo2/prophet.html",
    "href": "data_science_ingemat/modulo2/prophet.html",
    "title": "Tema 3: Prophet",
    "section": "",
    "text": "Prophet es una biblioteca de Python desarrollada por Facebook que se utiliza para la predicción de series temporales. Prophet es especialmente útil para los casos de uso que tienen patrones estacionales fuertes y varias temporadas de datos históricos. Prophet también es robusto ante los datos faltantes y los cambios en la tendencia, y normalmente maneja bien los valores atípicos.\nLa biblioteca Prophet intenta capturar la tendencia y la estacionalidad al ajustar un modelo aditivo donde las observaciones no negativas se describen como la suma de los componentes. El objetivo de Prophet es hacer que las predicciones de series temporales sean escalables y automatizadas.\nA continuación, enumeraremos y describiremos brevemente algunas de las principales funciones que ofrece la biblioteca Prophet.\n\n\n\nProphet(): Esta es la clase principal de la biblioteca Prophet. Se utiliza para crear un objeto de Prophet que luego se puede ajustar a los datos de la serie temporal y se utiliza para hacer predicciones.\nfit(): Este método se utiliza para ajustar el modelo Prophet a los datos de la serie temporal. Se debe llamar antes de hacer predicciones.\nmake_future_dataframe(): Este método se utiliza para crear un DataFrame de Pandas de fechas futuras para las cuales hacer predicciones.\npredict(): Este método se utiliza para hacer predicciones. Se puede llamar después de ajustar el modelo Prophet a los datos de la serie temporal.\nplot() y plot_components(): Estos métodos se utilizan para visualizar las predicciones y los componentes del modelo Prophet.\nadd_seasonality(): Este método se utiliza para agregar estacionalidad personalizada al modelo Prophet.\nadd_country_holidays(): Este método se utiliza para agregar las vacaciones de un país específico al modelo Prophet.\n\n\n\n\nCrea una jupyter notebook con una data sintetica u otra a elección que simule una serie de tiempo. Usa cada función expuesta anteriormente y comenta los resultados"
  },
  {
    "objectID": "data_science_ingemat/modulo2/prophet.html#tarea-2",
    "href": "data_science_ingemat/modulo2/prophet.html#tarea-2",
    "title": "Tema 3: Prophet",
    "section": "",
    "text": "Crea una jupyter notebook con una data sintetica u otra a elección que simule una serie de tiempo. Usa cada función expuesta anteriormente y comenta los resultados"
  },
  {
    "objectID": "data_science_ingemat/modulo2/proyecto2.html",
    "href": "data_science_ingemat/modulo2/proyecto2.html",
    "title": "Proyecto",
    "section": "",
    "text": "En este proyecto, se espera que apliquen sus conocimientos de series de tiempo y el uso de la biblioteca Prophet para realizar pronósticos. Debe encontrar un conjunto de datos con más de tres años de historia para su análisis.\n\n\n\n\nRealizar un análisis exploratorio de los datos para entender las características de la serie de tiempo.\nPreprocesar los datos si es necesario para prepararlos para Prophet.\nEntrenar un modelo Prophet en los datos y realizar pronósticos.\nEvaluar la precisión de sus pronósticos.\nInterpretar los componentes de su modelo (tendencia, estacionalidad).\n\n\n\n\n\nAsegúrese de dividir sus datos en conjuntos de entrenamiento y prueba para evaluar la precisión de sus pronósticos.\nConsidere la posibilidad de ajustar los hiperparámetros de Prophet para mejorar su modelo.\nRecuerde que Prophet puede manejar datos faltantes, pero debe decidir si imputar estos datos es la mejor opción para su situación.\nLa entrega del proyecto debe realizarse a través de un repositorio de GitHub. Asegúrese de que su proyecto esté bien comentado, para esto use el archivo ReadMe.\nLa fecha límite para la entrega del proyecto es el lunes 31 de julio. Las presentaciones se llevarán a cabo hasta el 5 de agosto.\n\n\n\n\n\nCalidad del análisis exploratorio de datos y las conclusiones extraídas.\nCorrecta aplicación de Prophet y uso de sus funcionalidades.\nPrecisión de los pronósticos.\nInterpretación de los componentes del modelo.\nCalidad de la presentación de sus resultados.\n\n\n\n\n\nAjuste de la flexibilidad de la tendencia con el parámetro changepoint_prior_scale.\nAjuste de la estacionalidad con add_seasonality y seasonality_mode.\nUso de add_country_holidays para incluir efectos de vacaciones.\nUso de make_future_dataframe para generar fechas futuras para pronósticos.\nInterpretación de los componentes del modelo con plot_components."
  },
  {
    "objectID": "data_science_ingemat/modulo2/proyecto2.html#proyecto-de-series-de-tiempo-con-prophet",
    "href": "data_science_ingemat/modulo2/proyecto2.html#proyecto-de-series-de-tiempo-con-prophet",
    "title": "Proyecto",
    "section": "",
    "text": "En este proyecto, se espera que apliquen sus conocimientos de series de tiempo y el uso de la biblioteca Prophet para realizar pronósticos. Debe encontrar un conjunto de datos con más de tres años de historia para su análisis.\n\n\n\n\nRealizar un análisis exploratorio de los datos para entender las características de la serie de tiempo.\nPreprocesar los datos si es necesario para prepararlos para Prophet.\nEntrenar un modelo Prophet en los datos y realizar pronósticos.\nEvaluar la precisión de sus pronósticos.\nInterpretar los componentes de su modelo (tendencia, estacionalidad).\n\n\n\n\n\nAsegúrese de dividir sus datos en conjuntos de entrenamiento y prueba para evaluar la precisión de sus pronósticos.\nConsidere la posibilidad de ajustar los hiperparámetros de Prophet para mejorar su modelo.\nRecuerde que Prophet puede manejar datos faltantes, pero debe decidir si imputar estos datos es la mejor opción para su situación.\nLa entrega del proyecto debe realizarse a través de un repositorio de GitHub. Asegúrese de que su proyecto esté bien comentado, para esto use el archivo ReadMe.\nLa fecha límite para la entrega del proyecto es el lunes 31 de julio. Las presentaciones se llevarán a cabo hasta el 5 de agosto.\n\n\n\n\n\nCalidad del análisis exploratorio de datos y las conclusiones extraídas.\nCorrecta aplicación de Prophet y uso de sus funcionalidades.\nPrecisión de los pronósticos.\nInterpretación de los componentes del modelo.\nCalidad de la presentación de sus resultados.\n\n\n\n\n\nAjuste de la flexibilidad de la tendencia con el parámetro changepoint_prior_scale.\nAjuste de la estacionalidad con add_seasonality y seasonality_mode.\nUso de add_country_holidays para incluir efectos de vacaciones.\nUso de make_future_dataframe para generar fechas futuras para pronósticos.\nInterpretación de los componentes del modelo con plot_components."
  },
  {
    "objectID": "data_science_ingemat/modulo2/proyecto2.html#rúbrica-del-proyecto-de-series-de-tiempo-con-prophet",
    "href": "data_science_ingemat/modulo2/proyecto2.html#rúbrica-del-proyecto-de-series-de-tiempo-con-prophet",
    "title": "Proyecto",
    "section": "Rúbrica del Proyecto de Series de Tiempo con Prophet",
    "text": "Rúbrica del Proyecto de Series de Tiempo con Prophet\n\n\n\n\n\n\n\n\n\n\nPuntos de Evaluación\nNivel 1\nNivel 2\nNivel 3\nNivel 4\n\n\n\n\nCalidad del análisis exploratorio de datos y las conclusiones extraídas\nEl análisis exploratorio de datos es inexistente o muy limitado, sin conclusiones significativas.\nEl análisis exploratorio de datos es básico, con algunas conclusiones pero falta profundidad o comprensión completa de los datos.\nEl análisis exploratorio de datos es sólido, con conclusiones bien razonadas y una buena comprensión de los datos.\nEl análisis exploratorio de datos es excepcionalmente detallado y perspicaz, con conclusiones profundas y una comprensión completa de los datos.\n\n\nCorrecta aplicación de Prophet y uso de sus funcionalidades\nProphet no se utiliza correctamente o no se utiliza en absoluto.\nProphet se utiliza de manera básica, pero no se explotan todas sus funcionalidades.\nProphet se utiliza correctamente y se explotan la mayoría de sus funcionalidades.\nProphet se utiliza de manera experta, aprovechando todas sus funcionalidades para mejorar el modelo.\n\n\nPrecisión de los pronósticos\nLos pronósticos son inexactos y el modelo no se ajusta bien a los datos.\nLos pronósticos son moderadamente precisos, pero el modelo podría mejorarse.\nLos pronósticos son precisos y el modelo se ajusta bien a los datos.\nLos pronósticos son extremadamente precisos y el modelo se ajusta excepcionalmente bien a los datos.\n\n\nInterpretación de los componentes del modelo\nNo se realiza ninguna interpretación de los componentes del modelo.\nSe realiza una interpretación básica de algunos componentes del modelo.\nSe realiza una interpretación sólida de la mayoría de los componentes del modelo.\nSe realiza una interpretación detallada y perspicaz de todos los componentes del modelo.\n\n\nCalidad de la presentación de sus resultados\nLos resultados no se presentan de manera clara o comprensible.\nLos resultados se presentan de manera básica, pero podrían mejorarse para una mayor claridad o comprensión.\nLos resultados se presentan de manera clara y comprensible.\nLos resultados se presentan de manera excepcionalmente clara, detallada y comprensible."
  },
  {
    "objectID": "data_science_ingemat/modulo2/estacionaria.html",
    "href": "data_science_ingemat/modulo2/estacionaria.html",
    "title": "Tema 2: Estacionariedad",
    "section": "",
    "text": "Una serie de tiempo es considerada estacionaria si cumple con las siguientes propiedades estadísticas a lo largo del tiempo:\n\nTiene una media constante.\nTiene una varianza constante.\nLa covarianza entre los dos periodos (por ejemplo, t y t+m) depende solo de la diferencia m y no del tiempo t.\n\nFormalmente, una serie de tiempo {Xt} se considera estrictamente estacionaria si la distribución conjunta de (Xt1, Xt2, …, Xtk) es la misma que la de (Xt1+h, Xt2+h, …, Xtk+h) para cualquier elección de los tiempos t1, t2, …, tk y para cada desplazamiento h.\n\n\nExisten varias técnicas para determinar si una serie de tiempo es estacionaria. Algunas de las más populares incluyen:\n\nPrueba de Dickey-Fuller aumentada (ADF): Esta prueba hipotetiza que una serie de tiempo es no estacionaria (tiene alguna forma de raíz unitaria). Un resultado de prueba que rechaza esta hipótesis indica que la serie es estacionaria.\nPrueba de KPSS (Kwiatkowski-Phillips-Schmidt-Shin): A diferencia de la prueba ADF, la prueba KPSS hipotetiza que una serie de tiempo es estacionaria. Un resultado de prueba que rechaza esta hipótesis indica que la serie no es estacionaria.\n\n\n\n\nLa prueba de Dickey-Fuller Aumentada es una prueba de raíz unitaria en la presencia de estructura de serie autocorrelacionada. Para una serie de tiempo \\(y_t\\), la versión básica de la prueba de Dickey-Fuller considera la siguiente regresión de primer orden:\n\\(Δy_t = α + βt + γy_{t-1} + ε_t\\)\nLa hipótesis nula es que \\(γ = 0\\) (la serie tiene una raíz unitaria), mientras que la alternativa es \\(γ &lt; 0\\) (la serie es estacionaria). Para la versión aumentada de la prueba, se agregan términos de rezago de la serie diferenciada a la derecha de la ecuación de regresión para eliminar la autocorrelación en los errores (\\(ε_t\\)):\n\\(Δy_t = α + βt + γy_{t-1} + δ1Δy_{t-1} + … + δ_{p-1}Δy_{t-p+1} + ε_t\\)\n\n\n\nLa prueba KPSS es una prueba de hipótesis para probar la estacionariedad de una serie de tiempo (hipótesis nula) contra la presencia de una raíz unitaria (hipótesis alternativa).\nPara una serie de tiempo \\(y_t\\), la prueba KPSS considera la siguiente ecuación de regresión:\n\\(y_t = α + βt + St + ε_t\\)\ndonde \\(St\\) es una caminata aleatoria, que puede ser estocástica o determinística.\nLa hipótesis nula es que la serie es estacionaria (o trend-estacionaria), mientras que la alternativa es que la serie tiene una raíz unitaria.\n\n\n\nSerie Estacionaria: Las variaciones diarias de la temperatura (alrededor de la media) podrían ser consideradas como una serie estacionaria, ya que podríamos esperar que la variación media en la temperatura no cambie mucho de un día a otro.\nSerie No Estacionaria: El precio de una acción en el mercado es un ejemplo de una serie no estacionaria, ya que tiende a seguir una tendencia ascendente o descendente y no oscila alrededor de una constante.\n\nPara implementar pruebas de estacionariedad en Python, puedes usar la biblioteca statsmodels."
  },
  {
    "objectID": "data_science_ingemat/modulo2/estacionaria.html#series-de-tiempo-estacionarias",
    "href": "data_science_ingemat/modulo2/estacionaria.html#series-de-tiempo-estacionarias",
    "title": "Tema 2: Estacionariedad",
    "section": "",
    "text": "Una serie de tiempo es considerada estacionaria si cumple con las siguientes propiedades estadísticas a lo largo del tiempo:\n\nTiene una media constante.\nTiene una varianza constante.\nLa covarianza entre los dos periodos (por ejemplo, t y t+m) depende solo de la diferencia m y no del tiempo t.\n\nFormalmente, una serie de tiempo {Xt} se considera estrictamente estacionaria si la distribución conjunta de (Xt1, Xt2, …, Xtk) es la misma que la de (Xt1+h, Xt2+h, …, Xtk+h) para cualquier elección de los tiempos t1, t2, …, tk y para cada desplazamiento h.\n\n\nExisten varias técnicas para determinar si una serie de tiempo es estacionaria. Algunas de las más populares incluyen:\n\nPrueba de Dickey-Fuller aumentada (ADF): Esta prueba hipotetiza que una serie de tiempo es no estacionaria (tiene alguna forma de raíz unitaria). Un resultado de prueba que rechaza esta hipótesis indica que la serie es estacionaria.\nPrueba de KPSS (Kwiatkowski-Phillips-Schmidt-Shin): A diferencia de la prueba ADF, la prueba KPSS hipotetiza que una serie de tiempo es estacionaria. Un resultado de prueba que rechaza esta hipótesis indica que la serie no es estacionaria.\n\n\n\n\nLa prueba de Dickey-Fuller Aumentada es una prueba de raíz unitaria en la presencia de estructura de serie autocorrelacionada. Para una serie de tiempo \\(y_t\\), la versión básica de la prueba de Dickey-Fuller considera la siguiente regresión de primer orden:\n\\(Δy_t = α + βt + γy_{t-1} + ε_t\\)\nLa hipótesis nula es que \\(γ = 0\\) (la serie tiene una raíz unitaria), mientras que la alternativa es \\(γ &lt; 0\\) (la serie es estacionaria). Para la versión aumentada de la prueba, se agregan términos de rezago de la serie diferenciada a la derecha de la ecuación de regresión para eliminar la autocorrelación en los errores (\\(ε_t\\)):\n\\(Δy_t = α + βt + γy_{t-1} + δ1Δy_{t-1} + … + δ_{p-1}Δy_{t-p+1} + ε_t\\)\n\n\n\nLa prueba KPSS es una prueba de hipótesis para probar la estacionariedad de una serie de tiempo (hipótesis nula) contra la presencia de una raíz unitaria (hipótesis alternativa).\nPara una serie de tiempo \\(y_t\\), la prueba KPSS considera la siguiente ecuación de regresión:\n\\(y_t = α + βt + St + ε_t\\)\ndonde \\(St\\) es una caminata aleatoria, que puede ser estocástica o determinística.\nLa hipótesis nula es que la serie es estacionaria (o trend-estacionaria), mientras que la alternativa es que la serie tiene una raíz unitaria.\n\n\n\nSerie Estacionaria: Las variaciones diarias de la temperatura (alrededor de la media) podrían ser consideradas como una serie estacionaria, ya que podríamos esperar que la variación media en la temperatura no cambie mucho de un día a otro.\nSerie No Estacionaria: El precio de una acción en el mercado es un ejemplo de una serie no estacionaria, ya que tiende a seguir una tendencia ascendente o descendente y no oscila alrededor de una constante.\n\nPara implementar pruebas de estacionariedad en Python, puedes usar la biblioteca statsmodels."
  },
  {
    "objectID": "data_science_ingemat/modulo2/estacionaria.html#tarea-1-jupyter-notebook",
    "href": "data_science_ingemat/modulo2/estacionaria.html#tarea-1-jupyter-notebook",
    "title": "Tema 2: Estacionariedad",
    "section": "Tarea 1: Jupyter Notebook",
    "text": "Tarea 1: Jupyter Notebook\nAhora debes poner manos a la obra y completar el siguiente notebook: Notebook 1 de Series de Tiempo"
  }
]